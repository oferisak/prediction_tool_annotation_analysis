---
title: "Prediction tools analysis"
author: "Ofer Isakov"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: united
    toc: true
    toc_float: false
params:
  all_founder_vars_annotated_file: ''
  clinvar_am_annotated_file: ''
---

# Prompt
i have a dataset called tool_var_set of variants classified as either P/LP ir B/LB. the ratio between the variant types is different from the real ratio pop_ratio (approximately 0.5 P/LP instead of 0.04 P/LP). i also have nested subgroups of the dataset corresponding to different conditions, in each dataset the ratio between the variant types changes (and the expected pop_ratio ould also be different). i also have a classification model score for each variant (not necessarily between 0 and 1). using bayesian methods i want to calculate for each dataset and its corresponding prior ratio, and the real known population pop_ratio the probability of a variant being P/LP given the score. lets use r to do so. 

# Comments:
- went over the missing values for the different tools - they are really missing.. 
- tried different bw methods, SJ and ucv capture the density more accurately but the are not smooth enough and therefore much more noisy (when calculating the likelihood ratio), using nrd0 results in more consistent results - higher values have higher likelihoods

# TODO : 

- calculate the thresholds corresponding to the posterior likelihoods described in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9748256/table/tbl1/- added calibration tables to the calculation, consider reporting the results for the complete 2023 set and for each sub dataset (different thresholds per dataset)

- add features detailed in https://www.nature.com/articles/s41588-024-01821-8 and associate them with performance

## optional test sets
https://genomeinterpretation.org/cagi6-invitae.html - registered waiting for confirmation - checked 06/05/2024 - still no confirmation

## reviews
golden helix review - https://www.goldenhelix.com/blog/evaluating-deepminds-alphamissense-classifier/
review - https://www.frontiersin.org/articles/10.3389/fgene.2022.1010327/full
check the performance of each tool on benign vs pathogenic (maybe benign variants were used in training ans should be replaced)
benchmarking using deep mutational scans - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10407742/
becnchmarking analysis - phenotype based - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9313608/#humu24362-sec-0020title

https://www.biorxiv.org/content/10.1101/2024.06.12.598724v1.full
https://www.biorxiv.org/content/10.1101/2024.06.06.597828v1.full
https://nature.com/articles/s41467-022-31686-6#Sec11
https://www.biorxiv.org/content/10.1101/2024.07.08.602580v1.full


Papers to draw from:
https://jmg.bmj.com/content/58/8/547
https://pubmed.ncbi.nlm.nih.gov/25552646/
https://www.mdpi.com/1422-0067/23/14/7946
https://www.sciencedirect.com/science/article/pii/S0002929724002623#sec1

```{r load_project}
# if you want to load a preexisting project just define the folder name
analysis_folder_name<-'./output/prediction_tools_analysis.2024-09-25'
```

```{r markdown_setup,include=FALSE}
setwd('/media/SSD/Bioinformatics/Projects/prediction_tool_annotation_analysis/')

library(ProjectTemplate)
load.project()

data_folder_name<-'./data/preprocessed_data/2024-06-10'
if (exists('analysis_folder_name')){
  # grab the RData file and load it
  rdata_file<-list.files(glue('./{analysis_folder_name}/data'))
  message(glue('loading {rdata_file}..'))
  load(glue('{analysis_folder_name}/data/{rdata_file}'))
}else{
  analysis_folder_name<-glue('./output/prediction_tools_analysis.{Sys.Date()}')
  if (!dir.exists(analysis_folder_name)){
    dir.create(analysis_folder_name)
    dir.create(glue('{analysis_folder_name}/data'))
    }
  # Definitions ####
  # the year that will be used for all subsequent analysis
  year_to_analyze<-'at_or_after_2023'
  # the varity_r score is the best out of the varities, eve and mutpred,mpc,m.cap have a high rate of missingness. 
  # fitCons, lrt, mvp list.s2 tools have a different missingness pattern than the rest and removing them results in a larger set
  excluded_tools<-c('fathmm.xf_coding','esm1b','varity_r_loo','varity_er','varity_er_loo','eve','mutpred','m.cap','mpc','mutationassessor','lrt','mvp','list.s2','h1.hesc_fitcons','huvec_fitcons','gm12878_fitcons','integrated_fitcons') 
}

```

```{r save_load_data}
save.image(file = glue('{analysis_folder_name}/data/{basename(analysis_folder_name)}.RData'))
```

```{r load_preprocessed_data}
# The annotated table
proc_data_file<-grep('processed_data.prediction_tool_annotation',list.files(data_folder_name,full.names = T),value=T)
proc_data<-readr::read_delim(proc_data_file,delim='\t')
# The var sets to analyze
load(glue('{data_folder_name}/var_sets.RData'))
#excluded_tools<-c('esm1b')
tools_list<-grep('_score',colnames(proc_data),value = T)
tools_without_excluded<-setdiff(tools_list,paste0(excluded_tools,'_score.dbnsfp4.5a'))

# converted scores (in which lower score means higher prob of P)
converted_scores<-stringr::str_match(colnames(proc_data),'(.+)_converted')[,2]%>%unique()
converted_scores<-glue('{converted_scores[!is.na(converted_scores)]}_score.dbnsfp4.5a')
```

# Overall performance models 
collect overall performance for each model

```{r rocs}
# Definitions
tools_to_test<-tools_without_excluded
#time_var_set<-c('at_or_after_2023')
time_var_set<-c('at_or_after_2021','at_or_after_2022','at_or_after_2023','at_or_after_2024')
top_num<-10

analysis_var_sets<-generate_var_set_combinations(var_sets[time_var_set])
# overall performance for the complete var set - no missing data for any of the tools
overall_performance_complete_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,
                                                                 tools_without_excluded,complete = T,
                                                                 converted_scores = converted_scores,
                                                                 calculate_posterior_probs = FALSE)
# overall perforamnce for the full var set - the performance of each tool on all of it's annotated variants
overall_performance_full_analysis_res<-calculate_roc_metrics(proc_data,
                                                             analysis_var_sets,
                                                             tools_without_excluded,complete = F,
                                                             converted_scores = converted_scores,
                                                             calculate_posterior_probs = TRUE,bw_method = 'nrd0')

rocs_data<-c(overall_performance_complete_analysis_res$procs,overall_performance_full_analysis_res$procs)
overall_performance_table<-overall_performance_complete_analysis_res$roc_metrics_table%>%
  bind_rows(overall_performance_full_analysis_res$roc_metrics_table)%>%
  filter(tool %in% tools_to_test)%>%
  mutate(complete=ifelse(complete,'complete','full'))
write.table(overall_performance_table,file=glue('{analysis_folder_name}/overall_performance.{Sys.Date()}.csv'),row.names = F,sep='\t')

# Collect the top tools rank
top_tools_rank<-overall_performance_table%>%
  group_by(filter1,complete)%>%
  slice_max(order_by = AUC50.,n=top_num)%>%
  mutate(rank=rank(-AUC50.))%>%
  select(filter1,complete,tool,rank)%>%
  pivot_wider(id_cols = c(tool,complete),names_from = filter1,values_from = rank)

# Collect the top tools auc
top_tools_auc<-overall_performance_table%>%
  group_by(filter1,complete)%>%
  slice_max(order_by = AUC50.,n=top_num)%>%
  select(filter1,complete,tool,AUC50.)%>%
  pivot_wider(id_cols = c(tool,complete),names_from = filter1,values_from = AUC50.)

top_tools<-overall_performance_table%>%group_by(complete)%>%filter(filter1==year_to_analyze)%>%
  slice_max(order_by = AUC50.,n=top_num)%>%select(tool)%>%mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())

# generate the rocs table
raw_roc_table<-NULL
for (var_set in names(rocs_data)){
  split_var_set<-stringr::str_split(var_set,'\\|')%>%unlist()
  tool=split_var_set[2]
  if (!tool %in% tools_to_test){next}
  raw_roc_table<-raw_roc_table%>%bind_rows(
    data.frame(var_set=var_set,
               year=split_var_set[1],
               tool=split_var_set[2],
               complete=split_var_set[3],
               sensitivity=rocs_data[[var_set]]$sensitivities,
               specificity=rocs_data[[var_set]]$specificities)
  )
}

roc_table<-raw_roc_table%>%
    #filter(tool %in% tools_to_test)%>%
    filter(year==year_to_analyze)%>%
    left_join(overall_performance_table%>%
                rename(year=filter1))%>%
    mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  left_join(top_tools%>%mutate(is_top=T))%>%
  mutate(tool_name=ifelse(is.na(is_top),'Other',tool),
         tool_name=forcats::fct_relevel(tool_name,'Other'),
         complete=ifelse(complete=='complete',glue('Complete (Non-missing) set'),glue('Full set')))
colors <- setNames(colorRampPalette(c("darkred", "darkcyan", "darkorange",'darkmagenta'))(length(unique(roc_table$tool_name))), unique(roc_table$tool_name))
colors["Other"] <- "lightgray" 
print(colors)
# Plot the ROC for all the tools and emphasize the top ones
top_models_roc_plot<-
  roc_table%>%filter(tool_name=='Other')%>%
  ggplot(aes(x=1-specificity,y=sensitivity,color=tool_name,group=tool))+
  facet_wrap(complete~.)+
  geom_line(linewidth=1)+
  geom_line(data=roc_table%>%filter(tool_name!='Other'),linewidth=1)+
  geom_abline(slope = 1,intercept = 0,linetype=2,alpha=0.4)+
  scale_color_manual(values=colors)+
  theme_minimal()+
  theme(legend.position = 'top')+
  labs(color=NULL)
print(top_models_roc_plot)
ggsave(filename = glue('{analysis_folder_name}/top_models_roc_plot.{year_to_analyze}.{Sys.Date()}.png'),
       plot=top_models_roc_plot,
       device = 'png',
       height = 7,
       width = 10,
       dpi = 300,bg='white')
# collect the top tools in BOTH complete and full sets
top_performing_tools<-top_tools_rank%>%
  #filter(complete=='complete')%>%
  #slice_min(!!sym(year_to_analyze),n=10)%>%
  pull(tool)%>%stringr::str_replace('_score.+','')%>%toupper()%>%
  unique()

top_performing_tools_original_names<-top_tools_rank%>%pull(tool)%>%unique()

# info on top performing tools for the manuscript
for_text<-overall_performance_table%>%filter(filter1=='at_or_after_2023',grepl('bayes|metarnn|clinpred',tool),complete=='complete')%>%select(contains(c('tool','AUC','precision90')))

# Extract necessary data
metaRNN <- for_text %>% filter(tool == "metarnn_score.dbnsfp4.5a")
bayesDel <- for_text %>% filter(tool == "bayesdel_addaf_score.dbnsfp4.5a")
clinPred <- for_text %>% filter(tool == "clinpred_score.dbnsfp4.5a")

# Create the paragraph
paragraph <- sprintf(
  "MetaRNN, ClinPred and BayesDel outperformed all other tools across different years and for both the complete and full sets (Figure XXX) with a ROCAUC in the complete set of variants created at or after 2023 of %.3f [%.3f,%.3f], %.3f [%.3f,%.3f] and %.3f [%.3f,%.3f], respectively. Setting a threshold corresponding to a precision level of 0.9 (90%% probability that a positive variant is pathogenic), all three tools demonstrated high recall values, identifying %.1f%%, %.1f%% and %.1f%% of pathogenic variants, respectively.",
  metaRNN$AUC50, metaRNN$AUC2.5, metaRNN$AUC97.5,
  clinPred$AUC50, clinPred$AUC2.5, clinPred$AUC97.5,
  bayesDel$AUC50, bayesDel$AUC2.5, bayesDel$AUC97.5,
  metaRNN$recall_precision90 * 100,
  clinPred$recall_precision90 * 100,
  bayesDel$recall_precision90 * 100
)

# Print the paragraph
cat(paragraph)

# score thresholds calculations ####
# - should be performed using the full dataset (per tool)
overall_score_thresholds<-
overall_performance_full_analysis_res$posterior_prob%>%
  filter(filter1==year_to_analyze)%>%
  filter(tool%in%top_performing_tools_original_names)%>%
  filter(sensitivity>0.005,total.1>100)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  select(tool,score,criteria)%>%
  pivot_wider(names_from = criteria,values_from = score)
write.table(overall_score_thresholds,file=glue('{analysis_folder_name}/overall_performance.posterior_prob.{Sys.Date()}.csv'),row.names = F,sep='\t')

library(ggrepel)
overall_score_thresholds_plot<-
proc_data%>%filter(var_sets[[year_to_analyze]])%>%
  select(clinvar_class,all_of(top_performing_tools_original_names))%>%
  pivot_longer(-clinvar_class)%>%
  left_join(overall_score_thresholds%>%rename(name=tool))%>%
  group_by(name) %>%
  mutate(
    row_to_keep = row_number() == 1,  # Select only the first row for each tool
    P = ifelse(row_to_keep, P, NA),
    M = ifelse(row_to_keep, M, NA),
    S = ifelse(row_to_keep, S, NA),
 #   VS = ifelse(row_to_keep, VS, NA)
  ) %>%
  ungroup() %>%
  mutate(name=toupper(stringr::str_replace(name,'_score.dbnsfp4.5a','')))%>%
  mutate(M=ifelse(name%in%c('CADD_PHRED','CLINPRED'),NA,M))%>%
  filter(!is.na(value))%>%
  ggplot(aes(x=value,fill=clinvar_class))+
  geom_histogram(bins=50,position='dodge',alpha=0.5)+
  scale_fill_manual(values=c('gray','black'))+
  geom_vline(aes(xintercept = P,color="P"),linetype=2,linewidth=0.75)+
  geom_vline(aes(xintercept = M,color="M"),linetype=2,linewidth=0.75)+
  geom_vline(aes(xintercept = S,color="S"),linetype=2,linewidth=0.75)+
 # geom_vline(aes(xintercept = VS,color="VS"),linetype=2,linewidth=0.75)+
  # Text annotations for vertical lines
  geom_text(aes(x = P, y = Inf, label = round(P,2)), vjust = -0.2,hjust=1.2, color = prettycols(palette = 'Lively', n = 4)[1],angle = 90) +
  geom_text(aes(x = M, y = Inf, label = round(M,2)), vjust = -0.2,hjust=1.2, color = prettycols(palette = 'Lively', n = 4)[2],angle = 90) +
  geom_text(aes(x = S, y = Inf, label = round(S,2)), vjust = -0.2,hjust=1.2, color = prettycols(palette = 'Lively', n = 4)[3],angle = 90) +
  #geom_text(aes(x = VS, y = Inf, label = round(VS,2)), vjust = -0.2,hjust=1.2, color = prettycols(palette = 'Lively', n = 4)[4],angle = 90) +
  facet_wrap(name~.,scales='free')+
  # Add color scale for vlines to generate the legend
  scale_color_manual(name = "Criteria", values = c(
    "P" = prettycols(palette = 'Lively', n = 4)[1],
    "M" = prettycols(palette = 'Lively', n = 4)[2],
    "S" = prettycols(palette = 'Lively', n = 4)[3]
  #  "VS" = prettycols(palette = 'Lively', n = 4)[4]
  ),breaks = c("P", "M", "S")
  )+
  labs(fill='Class',x='Score',y='Number of variants')+
  theme_minimal()+theme(legend.position = 'top')
overall_score_thresholds_plot
ggsave(filename = glue('{analysis_folder_name}/overall_score_thresholds_plot.{year_to_analyze}.{Sys.Date()}.png'),
       plot=overall_score_thresholds_plot,
       device = 'png',
       height = 10,
       width = 18,
       dpi = 300,bg='white')

```

# Missingness
```{r missingness}

library(naniar)
naniar::gg_miss_upset(proc_data%>%select(tools_without_excluded),nsets=20) 

missing_count<-sapply(tools_without_excluded,function(tool) {proc_data%>%filter(is.na(!!sym(tool)))%>%nrow()})
missing_count<-missing_count%>%as.data.frame()%>%rename(n_missing='.')%>%mutate(missing_rate=n_missing/nrow(proc_data))
missing_count
# to get metrics on missingness
skimr::skim(missing_count$missing_rate)

missing_rate_plot<-
missing_count%>%
  mutate(tool=stringr::str_replace(rownames(missing_count),'_score.+','')%>%toupper())%>%
  ggplot(aes(x=forcats::fct_reorder(tool,missing_rate),y=missing_rate,label=glue('{round(missing_rate,2)*100}%')))+
  geom_col(alpha=0.5)+
  geom_text(nudge_y = 0.03,size=3)+
  coord_flip()+
  scale_y_continuous(labels = scales::percent)+
  theme_minimal()+
  labs(x=NULL,y='Missing rate')
missing_rate_plot
ggsave(filename = glue('{analysis_folder_name}/01_missing_rate_plot.{Sys.Date()}.png'),
       plot=missing_rate_plot,
       device = 'png',
       height = 10,
       width = 8,
       dpi = 300,bg='white')

# missing per year
missing_count_per_year<-NULL
#for (year in unique(year(proc_data$date_created))){
for (year in as.character(2013:2024)){
  #year_data<-proc_data%>%filter(year(date_created)==year)
  year_data<-proc_data%>%filter(var_sets[[year]])
  year_missing_count<-sapply(tools_without_excluded,function(tool) {year_data%>%filter(is.na(!!sym(tool)))%>%nrow()})
  missing_count_per_year<-missing_count_per_year%>%
    bind_rows(year_missing_count %>%
                as.data.frame() %>%
                rename(n_missing = '.') %>%
                mutate(tool=tools_without_excluded,
                       year = year, 
                       total_vars = nrow(year_data),
                       missing_rate = n_missing / nrow(year_data)))
}

missing_count_per_year%>%
  ggplot(aes(x=year,y=missing_rate))+
  geom_col()+
  facet_wrap(tool~.)

# check association between date created and missingness
# analyzing effect over time - all tools together
missing_by_year_model <- lme(missing_rate ~ as.numeric(year),random=~1 | tool, data = missing_count_per_year)
summary(missing_by_year_model)
```

# Variant assertion by year added to clinvar
metrics to test options  - AUC , Accuracy, Balanced accuracy
since we are intereseted in the intra-tool variance, then we should use the entire dataset for each tool

```{r perfo_by_time}
library(ggtext)
library(viridis)
library(fuzzyjoin)

# the clinvar_training_date is derived from the date the clinvar dataset was downloaded or if not available - the publication date
tools_with_pub_year<-readxl::read_xlsx('./data/accessory_data/tools_list.xlsx')

# Set the metric you wish to plot
analysis_var_sets<-generate_var_set_combinations(var_sets['all'],var_sets[c(as.character(2013:2024))])
rocs_analysis_results<-calculate_roc_metrics(proc_data,
                                             analysis_var_sets,
                                             tools_without_excluded,
                                             complete=F,
                                             calculate_posterior_probs = F,
                                             save_rocs = F)

if (length(excluded_tools)>0){
  tools_to_test<-grep(paste0('^',excluded_tools,'_score',collapse='|'),unique(rocs_analysis_results$roc_metrics_table$tool),value = T,invert = T)
}else{tools_to_test<-tools_list}
perf_by_year_table<-
  rocs_analysis_results$roc_metrics_table%>%
  rename(year=filter2)%>%
  filter(tool %in% tools_to_test)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())
# add publication year
perf_by_year_table<-perf_by_year_table%>%
  left_join(
    tools_with_pub_year%>%
      rename(pub_year="Publication Year")%>%mutate(pub_year=as.character(pub_year)))%>%
  mutate(pub_year=as.character(pub_year),
         training_year=as.character(clinvar_training_date))
write.table(perf_by_year_table,file=glue('{analysis_folder_name}/perf_by_year_table.{Sys.Date()}.csv'),row.names = F,sep='\t')

# plot for all tools
perf_metric<-'AUC50.'
perf_by_year_all_plot<-
  perf_by_year_table%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  ggplot(aes_string(x='year',y=perf_metric,group='tool'))+
  geom_point(size=1)+geom_line()+
  facet_wrap(tool~.,scales='free')+
  labs(x='ClinVar Year',y=perf_metric)+
  #scale_color_viridis_d(option = 'turbo')+
  geom_vline(aes(xintercept=training_year),data=perf_by_year_table,linetype=2,color='darkred')+
  theme_minimal()+
  theme(legend.position = 'top')+
  labs(color=NULL)+theme(axis.text.x = element_text(angle = 90))
perf_by_year_all_plot

ggsave(filename = glue('{analysis_folder_name}/all_tools_{perf_metric}_by_year_plot.{Sys.Date()}.png'),
       plot=perf_by_year_all_plot,
       device = 'png',
       height = 8,
       width = 12,
       dpi = 300,bg='white')

# plot for top tools
perf_by_year_top_plot<-
  perf_by_year_table%>%
  filter(tool %in% top_performing_tools)%>%
  ggplot(aes_string(x='year',y=perf_metric,group='tool'))+
  geom_point(size=1)+geom_line()+
  facet_wrap(tool~.,nrow=3)+
  labs(x='ClinVar Year',y=perf_metric)+
  #scale_color_viridis_d(option = 'turbo')+
  geom_vline(aes(xintercept=training_year),data=perf_by_year_table%>%
               filter(tool %in% top_performing_tools),linetype=2,color='darkred')+
  theme_minimal()+
  theme(legend.position = 'top')+
  labs(color=NULL)+theme(axis.text.x = element_text(angle = 90))
perf_by_year_top_plot

ggsave(filename = glue('{analysis_folder_name}/top_tools_{perf_metric}_by_year_plot.{Sys.Date()}.png'),
       plot=perf_by_year_top_plot,
       device = 'png',
       height = 6,
       width = 12,
       dpi = 300,bg='white')


# analyze results per tool to identify tools with decreasing values
year_analysis_table <- perf_by_year_table %>%mutate(year=as.numeric(year),
                                      before_training=ifelse(year<=as.numeric(training_year),
                                                             'before_training','after_training'))%>%
  mutate(ifelse(year<=2013,NA,year))%>%
  mutate(before_training=forcats::fct_relevel(before_training,'before_training'))
  
no_clinvar_training_tools<-year_analysis_table%>%group_by(tool)%>%
  summarize(before_and_after=length(unique(before_training)))%>%
  filter(before_and_after<2)%>%pull(tool)

# analyzing effect over time - for every tool 
year_effect_model_by_tool<-NULL
for (tool2analyze in unique(year_analysis_table$tool)){
  print(tool2analyze)
  tool_analysis<-tidy(lm(AUC50.~(year>=2020),data=year_analysis_table%>%filter(tool==tool2analyze)))
  year_effect_model_by_tool<-year_effect_model_by_tool%>%
    bind_rows(data.frame(tool=tool2analyze,
                         tool_analysis%>%filter(grepl('year',term))))
}

year_effect_model_by_tool%>%filter(p.value<0.05)

# analyzing effect over time - all tools together
year_effect_model <- lme(AUC50. ~ (year>=2020),random=~1 | tool, data = year_analysis_table%>%filter(tool%in%top_performing_tools))
summary(year_effect_model)

year_effect_model_by_tool
# analyze effect before and after training
lm_by_training_year<-
  year_analysis_table%>%
  filter(!tool %in% no_clinvar_training_tools)%>%
  group_by(tool) %>%
  do(tidy(lm(AUC50. ~ before_training, data = .)))%>%
  filter(term=='before_trainingafter_training')
lm_by_training_year

overall_with_by_year.combined_plot<-(top_models_roc_plot + labs(title='A')+theme(legend.position = 'bottom')) + (perf_by_year_all_plot+labs(title='B')) + plot_layout(widths = c(2, 3))
overall_with_by_year.combined_plot
ggsave(filename = glue('{analysis_folder_name}/overall_with_by_year.combined_plot.{perf_metric}.{Sys.Date()}.png'),
       plot=overall_with_by_year.combined_plot,
       device = 'png',
       height = 8,
       width = 18,
       dpi = 300,bg='white')
```

# Affect of conservation

## Overall 
```{r conservation_effec_overall}
conservation_var_sets<-grep('conserv',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[year_to_analyze],var_sets[conservation_var_sets])
var_sets$all_conservation <- rowSums(sapply(conservation_var_sets, function(v) !is.na(var_sets[[v]]))) > 0

# get the overall performance for variants with conservation scores
all_conservation_analysis_res<-calculate_roc_metrics(proc_data,
                                                     data.frame(filter1=year_to_analyze,filter2='all_conservation'),
                                                     tools_to_test,
                                                     complete = F,
                                                     save_rocs=F,
                                                     calculate_posterior_probs = TRUE,
                                                     converted_scores = converted_scores)
conservation_analysis_res<-calculate_roc_metrics(proc_data,
                                                     analysis_var_sets,
                                                     tools_to_test,
                                                     complete = F,
                                                     save_rocs=F,
                                                     calculate_posterior_probs = TRUE,
                                                     converted_scores = converted_scores)


performance_by_conservation_table<-
  conservation_analysis_res$roc_metrics_table%>%
  filter(tool %in% tools_to_test)%>%
  rename(conservation_level=filter2)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.),
         conservation_level=stringr::str_replace(conservation_level,'_conservation','')%>%stringr::str_to_title(),
         conservation_level=factor(conservation_level,levels = c('Low','Intermediate','High')))

# add overall performance
performance_by_conservation_table<-performance_by_conservation_table%>%left_join(
  all_conservation_analysis_res$roc_metrics_table%>%
    mutate(complete=ifelse(complete,'complete','full'))%>%filter(filter1==year_to_analyze,complete=='full')%>%
    mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
    select(filter1,tool,overall_total=total,overall_AUC50=AUC50.,overall_sens=sens,overall_spec=spec)
)

write.table(performance_by_conservation_table,file=glue('{analysis_folder_name}/performance_by_conservation_table.{Sys.Date()}.csv'),row.names = F,sep='\t')

# add min-max difference for the plot
performance_by_conservation_table<-performance_by_conservation_table%>%
  left_join(performance_by_conservation_table%>%group_by(tool)%>%mutate(min_auc=min(AUC50.),max_auc=max(AUC50.),min_max_diff=max_auc-min_auc)%>%select(tool,conservation_level,min_max_diff))
# order to the tools according to the min-max difference 
performance_by_conservation_table<-performance_by_conservation_table%>%mutate(tool=forcats::fct_reorder(tool,min_max_diff))

performance_by_conservation_table%>%
  select(conservation_level,tool,total,AUC50.)%>%
  pivot_wider(id_cols = tool,names_from = conservation_level,values_from = AUC50.)

# analyze results using mixed effects model 
# discriminatory performance using all the tools
cons_auc_model <- lme(AUC50. ~ conservation_level,random=~1 | tool, data = performance_by_conservation_table)
summary(cons_auc_model)
# discriminatory performance using the top tools
cons_auc_model_top_tools <- lme(AUC50. ~ conservation_level,random=~1 | tool, data = performance_by_conservation_table%>%filter(tool %in% top_performing_tools))
summary(cons_auc_model_top_tools)

# sensitivity and specificity using tools that have predictions
cons_sens_model<-lme(sens ~ conservation_level,random=~1 | tool, data = performance_by_conservation_table%>%filter(!is.na(sens)))
summary(cons_sens_model)
cons_spec_model<-lme(spec ~ conservation_level,random=~1 | tool, data = performance_by_conservation_table%>%filter(!is.na(spec)))
summary(cons_spec_model)

# plot the results
conservation_to_plot<-
performance_by_conservation_table%>%
  filter(!is.na(sens))%>%
  pivot_longer(-c(tool,filter1,conservation_level,total))%>%
  mutate(name=forcats::fct_recode(name,AUC='AUC50.',Sensitivity='sens',Specificity='spec'))%>%
  filter(name%in%c('AUC','Sensitivity','Specificity'))%>%
  #filter(name%in%c('AUC'))%>%
  left_join(
    performance_by_conservation_table%>%
    filter(!is.na(sens))%>%
    pivot_longer(-c(tool,filter1,conservation_level,total))%>%
    mutate(name=forcats::fct_recode(name,AUC='AUC50.',Sensitivity='sens',Specificity='spec'))%>%
    filter(name%in%c('AUC','Sensitivity','Specificity'))%>%
    #filter(name%in%c('AUC'))%>%
    group_by(tool,name)%>%
      summarize(min_val=min(value),max_val=max(value))%>%ungroup()
  )

cons_overall_performance<-
  performance_by_conservation_table%>%
  filter(!is.na(sens))%>%
  pivot_longer(-c(tool,filter1,conservation_level,total))%>%
  mutate(name=forcats::fct_recode(name,AUC='overall_AUC50',Sensitivity='overall_sens',Specificity='overall_spec'))%>%
  filter(name%in%c('AUC','Sensitivity','Specificity'))%>%select(-c(total,conservation_level))%>%distinct()

performance_by_conservation_plot<-
  conservation_to_plot%>%
  ggplot(aes(color=conservation_level,y=value,x=tool,ymin=min_val,ymax=max_val))+
  geom_errorbar(color='gray',width=0)+
  geom_point(size=2)+
  #geom_point(data = cons_overall_performance, aes(y = value, x = tool, shape = "Overall performance"), fill = "black", size = 2, inherit.aes = FALSE) +
  facet_grid(.~name)+
  coord_flip()+
  theme_minimal()+
  PrettyCols::scale_color_pretty_d(palette = 'Autumn',scale_name='Autumn')+
  #scale_color_manual(values=c('darkcyan','darkorange','darkred'))+
  #scale_shape_manual(values = c("Overall performance" = 3)) +
  labs(color='Conservation',x=NULL,shape=NULL,y=NULL)+
  theme(legend.position = 'top',strip.text = element_text(face='bold'))
performance_by_conservation_plot

ggsave(filename = glue('{analysis_folder_name}/performance_by_conservation_plot.{year_to_analyze}.{Sys.Date()}.png'),
       plot=performance_by_conservation_plot,
       device = 'png',
       height = 7,
       width = 10,
       dpi = 300,bg='white')

# posterior probabilities ####
conservation_score_thresholds<-conservation_analysis_res$posterior_prob%>%
  filter(tool%in%top_performing_tools_original_names)%>%
  filter(sensitivity>0.005,total.1>100)

conservation_score_thresholds_plot<-
conservation_score_thresholds%>%
  mutate(filter2=stringr::str_replace(filter2,'_conservation',''))%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  ggplot(aes(x=factor(filter2, levels = rev(levels(factor(filter2)))),
             color=criteria,y=score,size=sensitivity))+
  geom_point(shape=3,stroke=1.5)+
  PrettyCols::scale_color_pretty_d(palette = 'Lively',scale_name='Lively')+
  facet_wrap(tool~.,scales='free_y')+
  theme_minimal()+theme(legend.position = 'top')+
  labs(x='Conservation',y='Score',color='Criteria',size='TPR')
conservation_score_thresholds_plot
ggsave(filename = glue('{analysis_folder_name}/conservation_score_thresholds_plot.{year_to_analyze}.{Sys.Date()}.png'),
       plot=conservation_score_thresholds_plot,
       device = 'png',
       height = 10,
       width = 10,
       dpi = 300,bg='white')

conservation.combined_plot<-(performance_by_conservation_plot+labs(title='A')+theme(legend.position = 'bottom'))+
  (conservation_score_thresholds_plot+labs(title='B')+theme(legend.position = 'bottom')) + 
  plot_layout(widths = c(2,2))
conservation.combined_plot
ggsave(filename = glue('{analysis_folder_name}/conservation.combined_plot.{year_to_analyze}.{Sys.Date()}.png'),
       plot=conservation.combined_plot,
       device = 'png',
       height = 8,
       width = 20,
       dpi = 300,bg='white')

conservation_score_thresholds_table<-
conservation_analysis_res$posterior_prob%>%
  filter(filter1==year_to_analyze)%>%
  filter(tool%in%top_performing_tools_original_names)%>%
  filter(sensitivity>0.005,total.1>150)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  select(tool,score,criteria,conservation=filter2)%>%
  pivot_wider(names_from = criteria,values_from = score)
write.table(conservation_score_thresholds_table,file=glue('{analysis_folder_name}/conservation_performance.posterior_prob.{Sys.Date()}.csv'),row.names = F,sep='\t')

# score distributions ####
revel_score_distribution_example<-
proc_data%>%
  mutate(conservation=case_when(
    var_sets$high_conservation~'high_conservation',
    var_sets$intermediate_conservation~'intermediate_conservation',
    var_sets$low_conservation~'low_conservation',
    .default=NA
  ))%>%
  filter(!is.na(conservation))%>%
  select(clinvar_class,conservation,revel_score.dbnsfp4.5a)%>%
  pivot_longer(-c(clinvar_class,conservation))%>%
  left_join(conservation_score_thresholds_table%>%rename(name=tool))%>%
  mutate(conservation=str_to_title(stringr::str_replace(conservation,'_',' ')))%>%
  #mutate(M=ifelse(name%in%c('CADD_PHRED','CLINPRED'),NA,M))%>%
  filter(!is.na(value))

a<-revel_score_distribution_example%>%
  mutate(clingen_cutoffs=cut(value,breaks = c(0.644,0.773,0.932,1)))%>%
  group_by(conservation)%>%
  count(clingen_cutoffs,clinvar_class)%>%
  group_by(conservation,clingen_cutoffs)%>%
  mutate(rate=n/sum(n))

revel_score_distribution_example_plot<-
  revel_score_distribution_example%>%
  group_by(name,conservation) %>%
  mutate(
    row_to_keep = row_number() == 1,  # Select only the first row for each tool
    P = ifelse(row_to_keep, P, NA),
    M = ifelse(row_to_keep, M, NA),
    S = ifelse(row_to_keep, S, NA),
    VS = ifelse(row_to_keep, VS, NA)
  ) %>%
  ungroup() %>%
  mutate(name=toupper(stringr::str_replace(name,'_score.dbnsfp4.5a','')))%>%
  ggplot(aes(x=value,fill=clinvar_class))+
  #geom_histogram(bins=50,position='dodge',alpha=0.5)+
  geom_density(alpha=0.3)+
  scale_fill_manual(values=c('gray','black'))+
  geom_vline(aes(xintercept = P,color="P"),linetype=2,linewidth=0.75)+
  geom_vline(aes(xintercept = M,color="M"),linetype=2,linewidth=0.75)+
  geom_vline(aes(xintercept = S,color="S"),linetype=2,linewidth=0.75)+
  geom_vline(aes(xintercept = VS,color="VS"),linetype=2,linewidth=0.75)+
  # Text annotations for vertical lines
  geom_text(aes(x = P, y = Inf, label = round(P,2)), vjust = -0.2,hjust=1.2, color = prettycols(palette = 'Lively', n = 4)[1],angle = 90) +
  geom_text(aes(x = M, y = Inf, label = round(M,2)), vjust = -0.2,hjust=1.2, color = prettycols(palette = 'Lively', n = 4)[2],angle = 90) +
  geom_text(aes(x = S, y = Inf, label = round(S,2)), vjust = -0.2,hjust=1.2, color = prettycols(palette = 'Lively', n = 4)[3],angle = 90) +
  geom_text(aes(x = VS, y = Inf, label = round(VS,2)), vjust = -0.2,hjust=1.2, color = prettycols(palette = 'Lively', n = 4)[4],angle = 90) +
  facet_wrap(conservation~.,scales='free')+
  # Add color scale for vlines to generate the legend
  scale_color_manual(name = "Criteria", values = c(
    "P" = prettycols(palette = 'Lively', n = 4)[1],
    "M" = prettycols(palette = 'Lively', n = 4)[2],
    "S" = prettycols(palette = 'Lively', n = 4)[3],
    "VS" = prettycols(palette = 'Lively', n = 4)[4]
  ),breaks = c("P", "M", "S", "VS")
  )+
  labs(fill='Class',x='Score',y='Variant density')+
  theme_minimal()+theme(legend.position = 'top')
revel_score_distribution_example_plot
ggsave(filename = glue('{analysis_folder_name}/revel_score_distribution_example_plot.{year_to_analyze}.{Sys.Date()}.png'),
       plot=revel_score_distribution_example_plot,
       device = 'png',
       height = 7,
       width = 10,
       dpi = 300,bg='white')
```

## Comparions
```{r conservation_effec_comparison}
conservation_var_sets<-grep('conserv',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[year_to_analyze],var_sets[conservation_var_sets])

# for inter tool comparison
conservation_analysis_res_complete<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_to_test,complete = T,save_rocs = F,calculate_posterior_probs = F)

performance_by_conservation_table_complete<-
  conservation_analysis_res_complete$roc_metrics_table%>%
  filter(tool %in% tools_to_test)%>%
  filter(filter2!='all_conservation')%>%
  rename(conservation_level=filter2)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.),
         conservation_level=stringr::str_replace(conservation_level,'_conservation','')%>%stringr::str_to_title(),
         conservation_level=factor(conservation_level,levels = c('Low','Intermediate','High')))


top_tools_conservation <- performance_by_conservation_table_complete %>%
  filter(!tool%in%c('BAYESDEL_NOAF'))%>%
  group_by(conservation_level) %>%
  top_n(10, AUC50.) %>%
  arrange(conservation_level, desc(AUC50.)) %>%
  mutate(rank = row_number(),  # Assign ranks within each conservation level
         conservation_level = factor(conservation_level, levels = c("Low", "Intermediate", "High")),
         tool_label=glue('{tool} ({round(AUC50.,3)})'))%>%
  ungroup() 

# Create the bump plot
library(ggbump)
library(PrettyCols)
performance_by_conservation_bump_plot<-
ggplot(top_tools_conservation, aes(x = conservation_level, y = rank, group = tool, label = tool_label,color = tool)) +
  geom_bump( linewidth = 1.2, alpha = 0.8) +
  geom_text(aes(y = rank), nudge_y = 0.3, check_overlap = TRUE, size = 3.5, color = "black") +
  scale_y_reverse(breaks = 1:10) +  # Reverse scale so top tools are at the top
  labs(x = "Conservation Level", y = "AUROC Rank") +
  theme_minimal() +
  PrettyCols::scale_color_pretty_d('Autumn',scale_name='Autumn')+
  theme(legend.position = "none")
performance_by_conservation_bump_plot

ggsave(filename = glue('{analysis_folder_name}/performance_by_conservation_bump_plot.{year_to_analyze}.{Sys.Date()}.png'),
       plot=performance_by_conservation_bump_plot,
       device = 'png',
       height = 6,
       width = 9,
       dpi = 300,bg='white')
```

# MOI analysis

## Overall comparison
```{r moi}
var_sets[['XL']]<-grepl('X-linked',proc_data$moi_gencc)
proc_data%>%
  mutate(moi_gencc=case_when(
           moi_gencc=='Autosomal dominant'~'AD',
           moi_gencc=='Autosomal recessive'~'AR',
           moi_gencc=='X-linked'~'XL',
           .default='Unknown'))%>%
  count(moi_gencc)%>%
  mutate(rate=n/sum(n))%>%
  arrange(desc(rate))

tools_to_test<-tools_without_excluded
#tools_to_test<-top_tools_2023
moi_var_sets<-grep('AD|AR|XL|prec|pli',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[year_to_analyze],var_sets[moi_var_sets])
moi_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_to_test,complete=F,
                                        save_rocs = F,converted_scores = converted_scores,
                                        calculate_posterior_probs = T)

performance_by_moi_table<-
  moi_analysis_res$roc_metrics_table%>%
  rename(moi=filter2)%>%
  #filter(tool %in% tools_to_test)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.),
         complete=ifelse(complete,'complete','full'))
write.table(performance_by_moi_table,file=glue('{analysis_folder_name}/performance_by_moi_{year_to_analyze}_table.{Sys.Date()}.csv'),row.names = F,sep='\t')

# plot the difference between AD and AR
moi_to_plot<-performance_by_moi_table%>%
  filter(moi%in%c('AD','AR'))%>%select(moi,tool,AUC50.)%>%
  pivot_wider(id_cols = tool,names_from = moi,values_from = AUC50.)%>%
  left_join(
    performance_by_moi_table%>%
      filter(moi%in%c('prec_high','pli_high'))%>%select(moi,tool,AUC50.)%>%
      pivot_wider(id_cols = tool,names_from = moi,values_from = AUC50.)
  )%>%
  mutate('AD vs AR'=AD-AR,
         'pLI vs pRec'=pli_high-prec_high,
         tool=forcats::fct_reorder(tool,desc(`AD vs AR`)))%>%
  select(tool,`AD vs AR`,`pLI vs pRec`)%>%
  pivot_longer(-tool)
performance_by_moi_plot<-
moi_to_plot%>%
  ggplot(aes(y=value,x=tool,color=value>0,fill=value>0))+
  geom_point(size=3)+
  geom_col(linewidth=0.1,width = 0.1)+
  geom_hline(yintercept = 0,linetype=2,color='gray')+
  facet_grid(.~name)+
  scale_fill_manual(values=c('darkred','darkgreen'),labels=c('Better perofrmance in AR/pRec','Better performance in AD/pLI'))+
  scale_color_manual(values=c('darkred','darkgreen'))+
  coord_flip()+
  labs(y='AUC difference',fill=NULL,x=NULL)+guides(color='none')+
  theme_minimal()+theme(strip.text = element_text(face='bold'),legend.position = 'bottom',
                        axis.text.y = element_text(face=ifelse(levels(moi_to_plot$tool)%in%top_performing_tools,"bold","italic")))
performance_by_moi_plot

ggsave(filename = glue('{analysis_folder_name}/performance_by_moi_plot.{Sys.Date()}.png'),
       plot=performance_by_moi_plot,
       device = 'png',
       height = 8,
       width = 9,
       dpi = 300,bg='white')


moi_to_plot<-
performance_by_moi_table%>%
  filter(!is.na(sens))%>%filter(tool %in% top_performing_tools)%>%filter(moi%in%c('AD','AR','XL'))%>%
  pivot_longer(-c(tool,filter1,moi,total,complete))%>%
  mutate(name=forcats::fct_recode(name,AUC='AUC50.',Sensitivity='sens',Specificity='spec'))%>%
  filter(name%in%c('AUC','Sensitivity','Specificity'))%>%
  #filter(name%in%c('AUC'))%>%
  left_join(
    performance_by_moi_table%>%
    filter(!is.na(sens))%>%filter(tool %in% top_performing_tools)%>%filter(moi%in%c('AD','AR','XL'))%>%
    pivot_longer(-c(tool,filter1,moi,total,complete))%>%
    mutate(name=forcats::fct_recode(name,AUC='AUC50.',Sensitivity='sens',Specificity='spec'))%>%
    filter(name%in%c('AUC','Sensitivity','Specificity'))%>%
    #filter(name%in%c('AUC'))%>%
    group_by(tool,name)%>%
      summarize(min_val=min(value),max_val=max(value))%>%ungroup()
  )

performance_by_moi_plot<-
  moi_to_plot%>%
  ggplot(aes(color=moi,y=value,x=tool,ymin=min_val,ymax=max_val))+
  geom_errorbar(color='gray',width=0)+
  geom_point(size=3)+
  facet_grid(.~name,scales='free_x')+
  coord_flip()+
  theme_minimal()+
  PrettyCols::scale_color_pretty_d(palette = 'Autumn',scale_name='Autumn')+
  labs(color='MOI',x=NULL,shape=NULL,y=NULL)+
  theme(legend.position = 'top',strip.text = element_text(face='bold'))
performance_by_moi_plot
ggsave(filename = glue('{analysis_folder_name}/performance_by_moi_plot.{Sys.Date()}.png'),
       plot=performance_by_moi_plot,
       device = 'png',
       height = 10,
       width = 9,
       dpi = 300,bg='white')

library(nlme)
# discriminatory performance using all the tools AD vs AR
moi_auc_model_ad_vs_ar <- lme(AUC50. ~ moi,random=~1 | tool, data = performance_by_moi_table%>%filter(moi%in%c('AD','AR')))
summary(moi_auc_model_ad_vs_ar)
# discriminatory performance using all the tools pLI vs pREC
moi_auc_model_pLI_vs_pRec <- lme(AUC50. ~ moi,random=~1 | tool, data = performance_by_moi_table%>%filter(moi%in%c('pli_high','prec_high')))
summary(moi_auc_model_pLI_vs_pRec)

# sensitivity and specificity using tools that have predictions
moi_sens_model<-lme(sens ~ moi,random=~1 | tool, data = performance_by_moi_table%>%filter(moi%in%c('AD','AR'))%>%filter(!is.na(sens)))
summary(moi_sens_model)
moi_spec_model<-lme(spec ~ moi,random=~1 | tool, data = performance_by_moi_table%>%filter(moi%in%c('AD','AR'))%>%filter(!is.na(spec)))
summary(moi_spec_model)

# posterior probabilities ####
moi_score_thresholds<-moi_analysis_res$posterior_prob%>%
  filter(tool%in%top_performing_tools_original_names)%>%
  filter(sensitivity>0.005,total.1>100)

moi_score_thresholds_plot<-
moi_score_thresholds%>%
  filter(filter2%in%c('AD','AR','XL'))%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  ggplot(aes(x=factor(filter2, levels = rev(levels(factor(filter2)))),
             color=criteria,y=score,size=sensitivity))+
  geom_point(shape=3,stroke=1.5)+
  PrettyCols::scale_color_pretty_d(palette = 'Lively',scale_name='Lively')+
  facet_wrap(tool~.,scales='free_y')+
  theme_minimal()+theme(legend.position = 'top')+
  labs(x='Mode of Inheritance',y='Score',color='Criteria',size='TPR')
moi_score_thresholds_plot
ggsave(filename = glue('{analysis_folder_name}/moi_score_thresholds_plot.{year_to_analyze}.{Sys.Date()}.png'),
       plot=moi_score_thresholds_plot,
       device = 'png',
       height = 10,
       width = 10,
       dpi = 300,bg='white')

moi_score_thresholds_table<-
moi_analysis_res$posterior_prob%>%
  filter(filter1==year_to_analyze)%>%
  filter(tool%in%top_performing_tools_original_names)%>%
  filter(sensitivity>0.005,total.1>100)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  select(tool,score,criteria,moi=filter2)%>%
  pivot_wider(names_from = criteria,values_from = score)
write.table(moi_score_thresholds_table,file=glue('{analysis_folder_name}/moi_performance.posterior_prob.{Sys.Date()}.csv'),row.names = F,sep='\t')

moi.combined_plot<-(performance_by_moi_plot+labs(title='A')+theme(legend.position = 'bottom'))+
  (moi_score_thresholds_plot+labs(title='B')+theme(legend.position = 'bottom'))
ggsave(filename = glue('{analysis_folder_name}/moi.combined_plot.{year_to_analyze}.{Sys.Date()}.png'),
       plot=moi.combined_plot,
       device = 'png',
       height = 9,
       width = 15,
       dpi = 300,bg='white')
# plot with CI
# performance_by_moi_table%>%
#   filter(moi%in%c('AD','AR'))%>%
#   pivot_wider(id_cols = tool,names_from = moi,values_from = c(AUC50.,AUC2.5,AUC97.5))%>%
#   mutate(SE_AD = (AUC97.5_AD - AUC2.5_AD) / (2 * 1.96),
#          SE_AR = (AUC97.5_AR - AUC2.5_AR) / (2 * 1.96)) %>%
#   summarize(
#     mean_diff = AUC50._AD - AUC50._AR,
#     se_diff = sqrt(SE_AD^2 + SE_AR^2),
#     ci_lower = mean_diff - 1.96 * se_diff,
#     ci_upper = mean_diff + 1.96 * se_diff,
#     tool=forcats::fct_reorder(tool,mean_diff)
#   )%>%
#   ggplot(aes(x=tool,y=mean_diff,ymin=ci_lower,ymax=ci_upper))+
#   geom_point()+geom_errorbar()+
#   geom_hline(yintercept = 0,linetype=2)+
#   coord_flip()+
#   theme_minimal()

# ran the comparison with the complete set, no real insights
```

# Misense constraint 

```{r missense_constraint}
time_var_set<-'at_or_after_2023'
mismatchoe_vars<-c(grep('mismatch_min_oe_ratio:',names(var_sets),value = T),
                       'regeneron_constrained','regeneron_not_constrained')
#af_var_sets<-grep('regeneron',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[time_var_set],var_sets[mismatchoe_vars])
mismatchoe_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_to_test,
                                               complete = F,save_rocs = F,converted_scores = converted_scores,
                                               calculate_posterior_probs = T)

performance_by_mismatchoe_table<-
  mismatchoe_analysis_res$roc_metrics_table%>%
  rename(mismatch_oe=filter2)%>%
  filter(tool %in% tools_to_test)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.),
         mismatch_oe=stringr::str_replace(mismatch_oe,'mismatch_min_oe_ratio:','')
         )
write.table(performance_by_moi_table,file=glue('{analysis_folder_name}/performance_by_moi_{time_var_set}_table.{Sys.Date()}.csv'),row.names = F,sep='\t')

performance_by_mismatchoe_table%>%
  filter(!(grepl('regeneron',mismatch_oe)))%>%
  filter(tool %in% top_performing_tools)%>%
  ggplot(aes(x=mismatch_oe,y=AUC50.,group=tool,ymin=AUC2.5,ymax=AUC97.5))+
  geom_line()+
  geom_point()+
  geom_errorbar(width=0)+
  facet_wrap(tool~.)+
  theme_minimal()

performance_by_mismatchoe_plot<-
performance_by_mismatchoe_table%>%
  filter(!(grepl('regeneron',mismatch_oe)))%>%mutate(mismatch_oe=as.numeric(mismatch_oe))%>%
  filter(tool %in% top_performing_tools)%>%
  ggplot(aes(x=mismatch_oe,y=AUC50.,group=tool,ymin=AUC2.5,ymax=AUC97.5,group=tool))+
  geom_line(alpha=0.5)+
  geom_point(alpha=0.5)+
  geom_smooth(inherit.aes = F,aes(x=mismatch_oe,y=AUC50.),color='darkred')+
  #facet_wrap(tool~.)+
  labs(x='Mismatch O/E',y='AUC')+
  theme_minimal()
performance_by_mismatchoe_plot

ggsave(filename = glue('{analysis_folder_name}/performance_by_mismatch_oe_plot.{Sys.Date()}.png'),
       plot=performance_by_mismatchoe_plot,
       device = 'png',
       height = 8,
       width = 8,
       dpi = 300,bg='white')

# analyze results using mixed effects model
library(nlme)
model <- lme(AUC50. ~ mismatch_oe,random=~1 | tool, data = performance_by_mismatchoe_table%>%filter(!(grepl('regeneron',mismatch_oe)))%>%mutate(mismatch_oe=as.numeric(mismatch_oe)))
summary(model)
plot(model)

# find the tools with the highest difference in performance
performance_difference_mismatchoe<-performance_by_mismatchoe_table%>%
  filter(tool %in% top_performing_tools)%>%
  group_by(tool)%>%
  summarize(mean_AUC50=mean(AUC50.),
            min_auc50=min(AUC50.),
            max_auc50=max(AUC50.),
            worst_perf=min_auc50-mean_AUC50,
            worst_perf_constraint=mismatch_oe[which(AUC50.==min_auc50)])


# posterior probabilities
mismatchoe_score_thresholds<-mismatchoe_analysis_res$posterior_prob%>%
  filter(tool%in%top_performing_tools_original_names)%>%
  filter(sensitivity>0.005,total.1>100)

mismatchoe_score_thresholds_plot<-
mismatchoe_score_thresholds%>%
  filter(grepl('mismatch_min',filter2))%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper(),
         filter2=as.numeric(stringr::str_replace(filter2,'mismatch_min_oe_ratio:','')))%>%
  ggplot(aes(x=filter2,
             color=criteria,y=score,group=criteria))+
  geom_line()+
  #geom_point(shape=3,size=2.5,stroke=1.5)+
  PrettyCols::scale_color_pretty_d(palette = 'Lively',scale_name='Lively')+
  facet_wrap(tool~.,scales='free_y')+
  theme_minimal()+theme(legend.position = 'top')+
  labs(x='Mismatch O/E ratio',y='Score',color='Criteria')
mismatchoe_score_thresholds_plot
ggsave(filename = glue('{analysis_folder_name}/mismatchoe_score_thresholds_plot.{year_to_analyze}.{Sys.Date()}.png'),
       plot=mismatchoe_score_thresholds_plot,
       device = 'png',
       height = 10,
       width = 10,
       dpi = 300,bg='white')

bump_mismatchoe.combind_plot<-(performance_by_conservation_bump_plot+labs(y=NULL,title='C')) + (performance_by_mismatchoe_plot+labs(title='D'))

conservation_mismatchoe.combined_plot<-
  conservation.combined_plot / 
  bump_mismatchoe.combind_plot + plot_layout(heights = c(4,3))
ggsave(filename = glue('{analysis_folder_name}/conservation_mismatchoe.combined_plot.{year_to_analyze}.{Sys.Date()}.png'),
       plot=conservation_mismatchoe.combined_plot,
       device = 'png',
       height = 12,
       width = 21,
       dpi = 300,bg='white')
```

# Allele frequency
seems there is no clear trend not sure need to add this to the analysis
```{r allele frequency}
time_var_set<-'at_or_after_2023'
af_col<-'allele.frequency.gnomad.joint.variant.frequencies.4.0.v2..broad'
af_cats<-cut(proc_data%>%pull(af_col),breaks=c(0,10^-6,10^-5,0.0001,1))%>%as.character()
af_cats[is.na(af_cats)]<-'0'
af_cats[proc_data%>%pull(af_col)==0]<-'0'
af_cats<-factor(af_cats)
for (af_cat in levels(af_cats)){
  var_sets[[glue('afs_{af_cat}')]]<-af_cats==af_cat
}

af_var_sets<-c(grep('afs_',names(var_sets),value = T))

#af_var_sets<-grep('regeneron',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[time_var_set],var_sets[af_var_sets])
af_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_to_test,complete = F,save_rocs = F,calculate_posterior_probs = TRUE,bw_method = 'nrd0',converted_scores = converted_scores)

af_vals<-proc_data%>%
  mutate(af=af_cats)%>%select(af,all_of(c(tools_to_test,af_col)))%>%
  pivot_longer(-c(af,af_col))%>%filter(!is.na(value))%>%
  group_by(af,name)%>%
  summarize(mean_af=mean(!!sym(af_col)),
            median_af=median(!!sym(af_col)))%>%ungroup()%>%
  mutate(mean_af=ifelse(af=='0',0,mean_af),
         median_af=ifelse(af=='0',0,median_af))%>%rename(tool=name)

performance_by_af_table<-
  af_analysis_res$roc_metrics_table%>%
  rename(af=filter2)%>%
  left_join(af_vals%>%mutate(af=paste0('afs_',af)))%>%
  mutate(af=forcats::fct_recode(af,
                                'AF=0'='afs_0',
                                '0<AF<1e-06'='afs_(0,1e-06]',
                                '1e-06≤AF<1e-05'='afs_(1e-06,1e-05]',
                                '1e-05≤AF<1e-04'='afs_(1e-05,0.0001]',
                                '1e-04≤AF'='afs_(0.0001,1]'
                                ))%>%
  mutate(af=forcats::fct_relevel(af,'AF=0','0<AF<1e-06','1e-06≤AF<1e-05','1e-05≤AF<1e-04','1e-04≤AF'))%>%
  filter(tool %in% tools_to_test)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.))
write.table(performance_by_moi_table,file=glue('{analysis_folder_name}/performance_by_af_{time_var_set}_table.{Sys.Date()}.csv'),row.names = F,sep='\t')

# all tools
af_model<-lme(AUC50. ~ af,random=~1 | tool, data = performance_by_af_table)
summary(af_model)
# top performing tools
af_model<-lme(AUC50. ~ af,random=~1 | tool, data = performance_by_af_table%>%filter(tool%in%top_performing_tools))
summary(af_model)

af_effect_by_model<-NULL

for (tool2analyze in performance_by_af_table$tool%>%unique()){
  print(tool2analyze)
  tool_analysis<-tidy(lm(AUC50.~median_af,data=performance_by_af_table%>%filter(tool==tool2analyze)))
  af_effect_by_model<-af_effect_by_model%>%
    bind_rows(data.frame(tool=tool2analyze,
                         tool_analysis%>%filter(grepl('af',term))))
}
af_effect_by_model

performance_by_af_plot<-
performance_by_af_table%>%
  #filter(!af%in%c('af_cat_(0.001,1]','af_cat_unknown'))%>%
  filter(tool %in% top_performing_tools)%>%
  mutate(tool_strip=as.character(tool))%>%
  ggplot(aes(x=af,y=AUC50.,group=tool,ymin=AUC2.5,ymax=AUC97.5))+
  geom_line(data=performance_by_af_table%>%filter(tool %in% top_performing_tools),color='gray')+
  geom_line()+
  geom_point()+
 # geom_errorbar(width=0)+
  facet_wrap(tool_strip~.)+
  theme_minimal()+theme(axis.text.x=element_text(angle=-45,hjust=0))+
  labs(x=NULL,y='AUC')
performance_by_af_plot

ggsave(filename = glue('{analysis_folder_name}/performance_by_af_plot.{year_to_analyze}.{Sys.Date()}.png'),
       plot=performance_by_af_plot,
       device = 'png',
       height = 10,
       width = 14,
       dpi = 300,bg='white')


# posterior probabilities
af_score_thresholds<-af_analysis_res$posterior_prob%>%
  filter(tool%in%top_performing_tools_original_names)%>%
  filter(sensitivity>0.005,total.1>100)%>%
  rename(af=filter2)%>%
  mutate(af=forcats::fct_recode(af,
                                'AF=0'='afs_0',
                                '0<AF<1e-06'='afs_(0,1e-06]',
                                '1e-06≤AF<1e-05'='afs_(1e-06,1e-05]',
                                '1e-05≤AF<1e-04'='afs_(1e-05,0.0001]',
                                '1e-04≤AF'='afs_(0.0001,1]'
                                ))%>%
  mutate(af=forcats::fct_relevel(af,'AF=0','0<AF<1e-06','1e-06≤AF<1e-05','1e-05≤AF<1e-04','1e-04≤AF'))

af_score_thresholds_plot<-
af_score_thresholds%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  ggplot(aes(x=af,
             color=criteria,y=score,size=sensitivity))+
  geom_point(shape=3,stroke=1.5)+
  PrettyCols::scale_color_pretty_d(palette = 'Lively',scale_name='Lively')+
  facet_wrap(tool~.,scales='free_y')+
  theme_minimal()+theme(legend.position = 'top',axis.text.x = element_text(angle=-60,hjust=0))+
  labs(x='Conservation',y='Score',color='Criteria',size='TPR')
af_score_thresholds_plot

ggsave(filename = glue('{analysis_folder_name}/af_score_thresholds_plot.{year_to_analyze}.{Sys.Date()}.png'),
       plot=af_score_thresholds_plot,
       device = 'png',
       height = 7,
       width = 15,
       dpi = 300,bg='white')

af.combined_plot<-(performance_by_af_plot+labs(title='A')) +
  (af_score_thresholds_plot+labs(title='B')+theme(legend.position = 'bottom'))
ggsave(filename = glue('{analysis_folder_name}/af.combined_plot.{year_to_analyze}.{Sys.Date()}.png'),
       plot=af.combined_plot,
       device = 'png',
       height = 9,
       width = 15,
       dpi = 300,bg='white')

af_score_thresholds_table<-
af_analysis_res$posterior_prob%>%
  filter(filter1==year_to_analyze)%>%
  filter(tool%in%top_performing_tools_original_names)%>%
  filter(sensitivity>0.005,total.1>100)%>%
  rename(af=filter2)%>%
  mutate(af=forcats::fct_recode(af,
                                'AF=0'='afs_0',
                                '0<AF<1e-06'='afs_(0,1e-06]',
                                '1e-06≤AF<1e-05'='afs_(1e-06,1e-05]',
                                '1e-05≤AF<1e-04'='afs_(1e-05,0.0001]',
                                '1e-04≤AF'='afs_(0.0001,1]'
                                ))%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  select(tool,score,criteria,af)%>%
  pivot_wider(names_from = criteria,values_from = score)
write.table(af_score_thresholds_table,file=glue('{analysis_folder_name}/af_performance.posterior_prob.{Sys.Date()}.csv'),row.names = F,sep='\t')
```

# PanelApp

```{r panelapp}
panelapp_cols<-grep('panelapp:',colnames(proc_data),value = T)
# convert all NAs in panelapp columns to FALSE (variant not in the panelapp gene)
proc_data <- proc_data %>%
  mutate(across(all_of(panelapp_cols), ~ ifelse(is.na(.), FALSE, .)))

min_count_thresh<-200
panelapp_counts<-data.frame()
for (pa_col in panelapp_cols){
  print(pa_col)
  for (tool in tools_to_test){
    min_count<-proc_data%>%filter(var_sets[[year_to_analyze]],!is.na(!!sym(tool)))%>%count(!!sym(pa_col),clinvar_class)%>%filter(!is.na(!!sym(pa_col)))%>%pull(n)%>%min()
    panelapp_counts<-panelapp_counts%>%bind_rows(data.frame(pa_col,tool,min_count=min_count))
  }
}

panelapp_to_test<-panelapp_counts%>%
  group_by(pa_col)%>%
  summarize(all_tools_above_thresh=all(min_count>min_count_thresh))%>%
  filter(all_tools_above_thresh)%>%pull(pa_col)
# add var_sets
for (pa_to_test in panelapp_to_test){
  var_sets[[pa_to_test]]<-proc_data%>%pull(pa_to_test)
}

#mismatchoe_var_sets<-grep('regeneron',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[year_to_analyze],var_sets[panelapp_to_test])
panelapp_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_to_test,complete = F,save_rocs=F,
                                             converted_scores = converted_scores,calculate_posterior_probs = T)
# calculate performance for all the variants that have a panelapp 
var_sets[['all_panelapp']]<-apply(proc_data[, panelapp_to_test], 1, function(row) any(row == TRUE))
all_panelapp_analysis_res<-calculate_roc_metrics(proc_data,data.frame(filter1=year_to_analyze,filter2='all_panelapp'),tools_to_test,complete = F,save_rocs=F,converted_scores = converted_scores,calculate_posterior_probs = T)

performance_by_panelapp_table<-
  panelapp_analysis_res$roc_metrics_table%>%
  rename(panelapp=filter2)%>%
  filter(tool %in% tools_to_test)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.),
         panelapp=stringr::str_replace(panelapp,'panelapp:','')
         )


#library(nlme)
# pa_for_nle<-pa_comp_table%>%bind_rows(pa_comp_table%>%select(AUC50.=all_other_panels,tool)%>%mutate(panelapp='all_other_panels')%>%distinct())%>%select(-all_other_panels)%>%
#   mutate(panelapp=relevel(factor(panelapp), ref = "all_other_panels"))
# model <- lme(AUC50. ~ panelapp,random=~1 | tool, data = pa_for_nle)
# pa_panel_vs_performance<-as.data.frame(summary(model)$tTable)
# pa_panel_vs_performance$panelapp=rownames(pa_panel_vs_performance)
# rownames(pa_panel_vs_performance)<-NULL
# pa_panel_vs_performance<-pa_panel_vs_performance%>%
#   filter(panelapp!='(Intercept)')%>%
#   rename(pval='p-value')%>%
#   mutate(panelapp=stringr::str_replace(panelapp,'^panelapp',''))
# pa_panel_vs_performance$padj=p.adjust(pa_panel_vs_performance$pval,method='fdr')

# panelapp performance per panel - check whether there are panels with better/worse overall performance across tools
pa_comp_table<-performance_by_panelapp_table%>%
  filter(tool%in%top_performing_tools)%>%
  select(tool,AUC50.,panelapp)%>%
  group_by(tool)%>%
  mutate(all_other_panels = purrr::map_dbl(panelapp, ~ mean(`AUC50.`[panelapp != .x]))) %>% # calculate the mean AUC in all other panels
  ungroup()

# run a paired t-test 
pa_comp_res<-pa_comp_table%>%
  left_join(
    pa_comp_table%>%
    group_by(panelapp) %>%
    summarize(
      t_test_result = list(t.test(AUC50., all_other_panels, paired = TRUE))
    ) %>%
    mutate(tidy_result = purrr::map(t_test_result, tidy)) %>%
    unnest(tidy_result)%>%
    select(-t_test_result)%>%ungroup()
  )%>%
  mutate(auc_diff=AUC50.-all_other_panels)%>%
  mutate(panelapp=stringr::str_replace_all(panelapp,'\\.',' '))


pa_comp_res<-
  pa_comp_res%>%left_join(
    pa_comp_res%>%group_by(panelapp)%>%summarize(fill_value=median(auc_diff)))%>%
  mutate(fill_color=ifelse(p.value<0.05 & abs(estimate)>0.01,fill_value,0),
         text_color = ifelse(p.value < 0.05 & fill_color>0, "royalblue4",
                             ifelse(p.value < 0.05 & fill_color<0, 'saddlebrown',"black")))

pa_comp_res%>%select(panelapp,text_color)%>%distinct()%>%count(text_color)%>%mutate(rate=n/sum(n))

panelapp_performance_plot_per_panel<-
pa_comp_res%>%
  ggplot(aes(x = reorder(panelapp, auc_diff, FUN = median), y = auc_diff))+
  geom_boxplot(outlier.size = 0.5,aes(fill=fill_color),alpha=0.5)+
  geom_hline(yintercept = 0,linetype=2)+
  coord_flip()+
  theme_minimal()+
  PrettyCols::scale_fill_pretty_c(palette = 'TangerineBlues')+
  labs(x=NULL,y='AUCROC difference')+guides(fill='none')+
  annotate("segment", y = 0.002, yend = 0.01, x = -Inf, xend = -Inf, 
           arrow = arrow(type = "closed", length = unit(0.3, "cm")), color = "black") +
  annotate("text", y = 0.025, x = -1.5, label = "Better performance", color = "black", hjust = 0.5) +
  annotate("segment", y = -0.002, yend = -0.01, x = -Inf, xend = -Inf, 
           arrow = arrow(type = "closed", length = unit(0.3, "cm")), color = "black") +
  annotate("text", y = -0.025, x = -1.5, label = "Worse performance", color = "black", hjust = 0.5)+
  scale_x_discrete(expand = expansion(mult = c(0.05, 0.0)))+
  theme(axis.text.y = element_text(color=pa_comp_res %>% distinct(panelapp,text_color,.keep_all = T)%>%arrange(fill_value) %>% pull(text_color)))
panelapp_performance_plot_per_panel


ggsave(filename = glue('{analysis_folder_name}/panelapp_performance_plot_per_panel.{year_to_analyze}.{Sys.Date()}.png'),
       plot=panelapp_performance_plot_per_panel,
       device = 'png',
       height = 10,
       width = 14,
       dpi = 300,bg='white')

# library(ggrepel)
# pa_all_comps<-pa_panel_vs_performance%>%mutate(panelapp_original=panelapp,
#                                     panelapp=stringr::str_replace_all(panelapp,'[\\.]+',' '))
# panelapp_overall_performance_diff_plot<-
#   pa_all_comps%>%
#   ggplot(aes(x=Value,y=-log(`p-value`),label=panelapp))+
#   geom_point()+
#  # geom_point(data=pa_all_comps%>%filter(p.value<0.01),color='blue')+
#   geom_vline(xintercept = 0,linetype=2)+
#   geom_point(data=pa_all_comps%>%filter(`p-value`<0.05),color='darkorange')+
#   geom_point(data=pa_all_comps%>%filter(padj<0.05),color='red')+
#   geom_label_repel(data=pa_all_comps%>%filter(`p-value`<0.01))+
#   annotate("segment", x = 0.002, xend = 0.01, y = -Inf, yend = -Inf, 
#            arrow = arrow(type = "closed", length = unit(0.3, "cm")), color = "black") +
#   annotate("text", x = 0.005, y = -0.2, label = "Better performance", color = "black", hjust = 0.5) +
#   annotate("segment", x = -0.002, xend = -0.01, y = -Inf, yend = -Inf, 
#            arrow = arrow(type = "closed", length = unit(0.3, "cm")), color = "black") +
#   annotate("text", x = -0.005, y = -0.2, label = "Worse performance", color = "black", hjust = 0.5) +
#   theme_minimal()+labs(x='AUC difference')
# 
# panelapp_overall_performance_diff_plot
# ggsave(filename = glue('{analysis_folder_name}/panelapp_overall_performance_diff_plot.{year_to_analyze}.{Sys.Date()}.png'),
#        plot=panelapp_overall_performance_diff_plot,
#        device = 'png',
#        height = 8,
#        width = 12,
#        dpi = 300,bg='white')

# panelapp performance per panel per tool - check whether there are panels with better/worse overall performance in specific tools
feature_comp<-
pa_comp_res%>%select(panelapp,p.value,estimate)%>%distinct()%>%
  mutate(sig_better=ifelse(p.value<0.05 & estimate>0.01,1,0),
            sig_worse=ifelse(p.value<0.05 & estimate<(-0.01),1,0))%>%
  left_join(panelapp_analysis_res$roc_metrics_table%>%select(panelapp_original=filter2)%>%distinct()%>%
              mutate(panelapp=stringr::str_replace_all(panelapp_original,'panelapp:',''))%>%
              mutate(panelapp=stringr::str_replace_all(panelapp,'\\.',' ')))
better_panels<-feature_comp%>%filter(sig_better==1)%>%pull(panelapp_original)
worse_panels<-feature_comp%>%filter(sig_worse==1)%>%pull(panelapp_original)


library(gtsummary)

pa_attributes<-proc_data%>%
  mutate(in_panelapp = rowSums(select(., contains('panelapp')) == TRUE) > 0)%>%
  mutate(better_panel = as.integer(Reduce(`|`, var_sets[names(var_sets) %in% better_panels])),
         worse_panel = as.integer(Reduce(`|`, var_sets[names(var_sets) %in% worse_panels])))%>%
  mutate(perf=case_when(
    better_panel==1 & worse_panel==1~'Undertermined',
    better_panel==1~'Better AUROC',
    worse_panel==1~'Worse AUROC',
    .default = 'Same'
  ))%>%
  mutate(perf=ifelse(in_panelapp,perf,NA))%>%
  mutate(MOI=case_when(
    var_sets$AR~'AR',
    var_sets$AD~'AD',
    var_sets$XL~'XL',
    .default='undetermined'
  ))%>%
  mutate(Conservation=case_when(
    var_sets$high_conservation~'High',
    var_sets$intermediate_conservation~'Intermediate',
    var_sets$low_conservation~'Low',
    .default='undetermined'
  ))%>%
  mutate(AF=case_when(
    !is.na(!!sym(af_col))& !!sym(af_col)<10e-5 & !!sym(af_col)>10e-7~'Rare',
    is.na(!!sym(af_col)) | !!sym(af_col)<10e-7~'Very rare',
    .default='Common'
  ))%>%select(AF,Conservation,MOI,perf,missense_min_oe_ratio,clinvar_class)

#need to show that in worse performing panels, there are more P/LP variants in low conservation regions..
pa_attributes%>%count(perf,clinvar_class,Conservation)%>%group_by(perf,clinvar_class)%>%mutate(rate=n/sum(n))

pa_attributes%>%filter(Conservation=='Low',perf%in%c('Better AUROC','Same','Worse AUROC'))%>%
  count(perf,clinvar_class)%>%
  group_by(perf)%>%mutate(rate=n/sum(n))

pa_attributes%>%
  filter(perf%in%c('Better AUROC','Same','Worse AUROC'))%>%
  select(perf,clinvar_class,Conservation)%>%
  pivot_longer(-c(perf,clinvar_class))%>%
  ggplot(aes(x=perf,fill=clinvar_class))+
  geom_bar(position='fill')+
  facet_wrap(name~value,scales='free')+
  theme_minimal()

pa_attributes%>%
  filter(perf%in%c('Better AUROC','Same','Worse AUROC'))%>%
  select(perf,clinvar_class,MOI)%>%
  #pivot_longer(-c(perf,clinvar_class))%>%
  ggplot(aes(x=perf,fill=MOI))+
  geom_bar(position='fill')+
  #facet_wrap(name~value,scales='free')+
  theme_minimal()

pa_attributes%>%filter(MOI=='XL',perf%in%c('Better AUROC','Same','Worse AUROC'))%>%
  count(perf,clinvar_class)%>%
  group_by(perf)%>%mutate(rate=n/sum(n))

pa_attributes%>%filter(Conservation=='Low',perf%in%c('Better AUROC','Worse AUROC'))%>%
  infer::chisq_test(perf~clinvar_class)

pa_attributes%>%filter(perf%in%c('Better AUROC','Worse AUROC'))%>%
  mutate(is_XL=ifelse(MOI=='XL','XL','Not XL'))%>%
  infer::chisq_test(perf~is_XL)

library(gtsummary)
pa_comp_output<-
pa_attributes%>%
  filter(perf%in%c('Better AUROC','Worse AUROC'))%>%
  select(perf,clinvar_class,MOI,Conservation,`Missense O/E ratio`=missense_min_oe_ratio,AF)%>%
  #filter(avg_diff_cat!='AUC same')%>%
  tbl_summary(by = perf) %>%
  add_p()
pa_comp_output
pa_comp_output%>%
as_tibble()%>%
  write.table(file = glue('{analysis_folder_name}/panelapp_better_worse_comp_{year_to_analyze}.{Sys.Date()}.csv'),row.names = F,sep = '\t')
```

```{r depracated_panelapp}
# DEPRACATED

# all panels
pa_all_comps_for_glm<-pa_all_comps%>%filter(p.value<0.01)

pa_all_comps_for_glm<-pa_all_comps_for_glm%>%left_join(pa_attributes_table)
pa_all_comps_for_glm<-pa_all_comps_for_glm%>%mutate(
  better=ifelse(Value>0,1,0)
)
res<-tidy(lm('Value~AD+AR+XL+high_conservation+low_conservation',data=pa_all_comps_for_glm))
res



# per panel
#panel_for_table<-'Structural eye disease'
panel_for_table<-'Bleeding and platelet disorders'
selected_panel<-glue('panelapp:{stringr::str_replace_all(panel_for_table," ",".")}')
panel_table<-proc_data%>%select(selected_panel,missense_min_oe_ratio,moi_gencc)%>%
  mutate(conservation=ifelse(var_sets$high_conservation,'High',
                             ifelse(var_sets$low_conservation,'Low','Intermediate')),
         moi_gencc=case_when(
           moi_gencc=='Autosomal dominant'~'AD',
           moi_gencc=='Autosomal recessive'~'AR',
           moi_gencc=='X-linked'~'XL',
           .default='Unknown'))
library(gtsummary)
panel_table%>%
  tbl_summary(by = selected_panel) %>%
  add_p()



# this analysis goes over each tool and tests what are the attributes of the variants in the panels that contributed to the performance. so for example if you see a significant positive value in AR it means that having a higher rate of AR genes in the panel results in better performance of the tool
pa_per_tool_attribute_effect<-performance_by_panelapp_table%>%left_join(pa_attributes_table,by=c('panelapp'='panelapp_original'))
pa_per_tool_attribute_effect<-pa_per_tool_attribute_effect%>%filter(tool %in% top_performing_tools)

# all tools
all_tools_all_panels_attribute_lme<- lme(AUC50.~AD+AR+XL+missense_min_oe_ratio,random=~1 | tool, data = pa_per_tool_attribute_effect)
all_tools_all_panels_attribute_lme_res<-data.frame(summary(all_tools_all_panels_attribute_lme)$tTable)
all_tools_all_panels_attribute_lme_res

write.table(all_tools_all_panels_attribute_lme_res,file=glue('{analysis_folder_name}/all_tools_all_panels_attribute_lm_res.{Sys.Date()}.csv'),row.names =T,sep='\t')

pa_per_tool_attribute_effect%>%select(panelapp,tool,AUC50.,AD,AR,XL,high_conservation,low_conservation)%>%
  pivot_longer(-c(panelapp,tool,AUC50.))%>%
  mutate(name=forcats::fct_relevel(factor(name),'AD','AR','XL'))%>%
  group_by(name)%>%
  mutate(value_cat=cut(value,5))%>%
  ggplot(aes(x=value_cat,y=AUC50.))+
  geom_jitter(width=0.2,alpha=0.1)+
  geom_boxplot(fill=NA,outlier.color = NA)+
  facet_wrap(name~.,scales = 'free')+
  theme_minimal()

# DEPRACATED

# per tool
pa_per_tool_attribute_effect_res<-NULL
for (selected_tool in pa_per_tool_attribute_effect%>%pull(tool)%>%unique()){
  print(selected_tool)
  pa_per_tool_attribute_effect_res<-
    pa_per_tool_attribute_effect_res%>%bind_rows(
      data.frame(selected_tool,tidy(lm('AUC50.~AD+AR+XL+high_conservation+low_conservation',
                              data = pa_per_tool_attribute_effect%>%filter(tool==selected_tool)))))
}



# now for each tool, compare each panel against all the rest
pa_per_tool_comp<-NULL
for (pa in performance_by_panelapp_table%>%pull(panelapp)%>%unique()){
  print(pa)
  panel_table<-proc_data%>%
    mutate(AR=var_sets$AR,AD=var_sets$AD,XL=var_sets$XL,
           high_conservation=var_sets$high_conservation,low_conservation=var_sets$low_conservation)%>%
    group_by(!!sym(selected_panel))%>%
    summarize(n=n(),
              AR=mean(AR,na.rm = T),AD=mean(AD,na.rm = T),XL=mean(XL,na.rm=T),
              high_conservation=mean(high_conservation,na.rm = T),low_conservation=mean(low_conservation,na.rm = T),
              missense_min_oe_ratio=mean(missense_min_oe_ratio,na.rm = T))%>%
    filter(!!sym(selected_panel))
}


library(ggrepel)
to_plot<-pa_per_tool_comp%>%
  #filter(tool %in% top_performing_tools)%>%
  filter(panelapp %in% (pa_all_comps%>%filter(p.value<0.05)%>%pull(panelapp_original)))
to_plot$p.adjust<-p.adjust(to_plot$p.value,method='fdr')
to_plot%>%
  ggplot(aes(x=estimate,y=-log(p.value),label=panelapp))+
  geom_point()+
  geom_point(data=to_plot%>%filter(p.adjust<0.05,estimate>0),color='lightgreen')+
  geom_point(data=to_plot%>%filter(p.adjust<0.05,estimate<(-0)),color='lightpink')+
  geom_label_repel(data=to_plot%>%filter(p.adjust<0.05,estimate>0),fill='lightgreen')+
  geom_label_repel(data=to_plot%>%filter(p.adjust<0.05,estimate<(-0)),fill='lightpink')+
  facet_wrap(tool~.,scales='free')+
  theme_minimal()
```


# Prepare application data
```{r app_data}
all_score_thresholds<-overall_score_thresholds%>%
  mutate(subset='overall')%>%
  bind_rows(
    moi_score_thresholds_table%>%rename(subset=moi)
  )%>%bind_rows(
    conservation_score_thresholds_table%>%rename(subset=conservation)
  )

write.table(all_score_thresholds,file=glue('{analysis_folder_name}/all_score_thresholds_{year_to_analyze}.{Sys.Date()}.csv'),row.names = F,sep = '\t')
```

# GO functional / bio process

```{r go_process_analysis}
go_cols<-grep('go_process:',colnames(proc_data),value = T)
min_count_thresh<-200
go_process_counts<-data.frame()
for (go_col in go_cols){
  print(go_col)
  for (tool in tools_to_test){
    min_count<-proc_data%>%filter(var_sets[[year_to_analyze]],!is.na(!!sym(tool)))%>%count(!!sym(go_col),clinvar_class)%>%filter(!is.na(!!sym(go_col)))%>%pull(n)%>%min()
    go_process_counts<-go_process_counts%>%bind_rows(data.frame(go_col,tool,min_count=min_count))
  }
}
go_process_to_test<-go_process_counts%>%group_by(go_col)%>%summarize(all_tools_above_thresh=all(min_count>min_count_thresh))%>%filter(all_tools_above_thresh)%>%pull(go_col)
# add var_sets
for (go_to_test in go_process_to_test){
  var_sets[[go_to_test]]<-proc_data%>%pull(go_to_test)
}

#mismatchoe_var_sets<-grep('regeneron',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[year_to_analyze],var_sets[go_process_to_test])
go_process_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_to_test,complete = F,save_rocs = F)

performance_by_go_process_table<-
  go_process_analysis_res$roc_metrics_table%>%
  rename(go_process=filter2)%>%
  filter(tool %in% tools_to_test)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.),
         go_process=stringr::str_replace(go_process,'go_process:','')
         )

performance_by_go_process_table<-
performance_by_go_process_table%>%left_join(
  overall_performance_table%>%filter(complete=='full',filter1==year_to_analyze)%>%
    select(tool,overall_performance=AUC50.)%>%mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())
)%>%
  mutate(tool=forcats::fct_reorder(tool,overall_performance),)
performance_by_go_process_table%>%
  filter(tool%in%(top_tools%>%filter(complete=='complete')%>%pull(tool)))%>%
  mutate(color=case_when(
    AUC50.<overall_performance & AUC97.5<overall_performance~'decrease',
    AUC50.>overall_performance & AUC2.5>overall_performance~'increase',
    .default='no_change'))%>%
  ggplot(aes(x=tool,y=AUC50.,ymin=AUC2.5,ymax=AUC97.5,color=color))+
  geom_point()+
  geom_errorbar(width=0.5)+
  geom_point(aes(x=tool,y=overall_performance),inherit.aes = F,shape=3)+
  coord_flip()+
  scale_color_manual(values=c('increase'='darkblue','no_change'='black','decrease'='darkred'))+
  facet_wrap(go_process~.,scales='free_x')+
  guides(color='none')+
  theme_minimal()

library(nlme)
go_me_analysis_table<-
performance_by_go_process_table%>%bind_rows(
  overall_performance_table%>%filter(complete=='full',filter1==year_to_analyze)%>%
    select(tool,AUC50.)%>%mutate(go_process='overall_performance',tool=stringr::str_replace(tool,'_score.+','')%>%toupper())
  )%>%
  mutate(go_process=relevel(factor(go_process), ref = "overall_performance"))

model <- lme(AUC50. ~ go_process,random=~1 | tool, data = go_me_analysis_table)
summary(model)
plot(model)

# which go process has the best overall performance?
performance_by_go_process_table%>%
  ggplot(aes(x=forcats::fct_reorder(go_process,AUC50.),y=AUC50.))+
  geom_boxplot()+
  geom_point(size=1)+
  coord_flip()+
  theme_minimal()

```

```{r go_function_analysis}
go_func_cols<-grep('go_function:',colnames(proc_data),value = T)
min_count_thresh<-200
go_func_counts<-data.frame()
for (go_col in go_func_cols){
  print(go_col)
  for (tool in tools_to_test){
    min_count<-proc_data%>%filter(var_sets[[year_to_analyze]],!is.na(!!sym(tool)))%>%count(!!sym(go_col),clinvar_class)%>%filter(!is.na(!!sym(go_col)))%>%pull(n)%>%min()
    go_func_counts<-go_func_counts%>%bind_rows(data.frame(go_col,tool,min_count=min_count))
  }
}
go_func_to_test<-go_func_counts%>%group_by(go_col)%>%summarize(all_tools_above_thresh=all(min_count>min_count_thresh))%>%filter(all_tools_above_thresh)%>%pull(go_col)
# add var_sets
for (go_to_test in go_func_to_test){
  var_sets[[go_to_test]]<-proc_data%>%pull(go_to_test)
}

#mismatchoe_var_sets<-grep('regeneron',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[year_to_analyze],var_sets[go_func_to_test])
go_func_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_to_test,complete = F)

performance_by_go_func_table<-
  go_func_analysis_res$roc_metrics_table%>%
  rename(go_function=filter2)%>%
  filter(tool %in% tools_to_test)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.),
         go_function=stringr::str_replace(go_function,'go_function:','')
         )

performance_by_go_func_table<-
performance_by_go_func_table%>%left_join(
  overall_performance_table%>%filter(complete=='full',filter1==year_to_analyze)%>%
    select(tool,overall_performance=AUC50.)%>%mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())
)%>%
  mutate(tool=forcats::fct_reorder(tool,overall_performance),)
performance_by_go_func_table%>%
  filter(tool%in%(top_tools%>%filter(complete=='complete')%>%pull(tool)))%>%
  mutate(color=case_when(
    AUC50.<overall_performance & AUC97.5<overall_performance~'decrease',
    AUC50.>overall_performance & AUC2.5>overall_performance~'increase',
    .default='no_change'))%>%
  ggplot(aes(x=tool,y=AUC50.,ymin=AUC2.5,ymax=AUC97.5,color=color))+
  geom_point()+
  geom_errorbar(width=0.5)+
  geom_point(aes(x=tool,y=overall_performance),inherit.aes = F,shape=3)+
  coord_flip()+
  scale_color_manual(values=c('increase'='darkblue','no_change'='black','decrease'='darkred'))+
  facet_wrap(go_function~.)+
  guides(color='none')+
  theme_minimal()

library(nlme)
go_func_me_analysis_table<-
performance_by_go_func_table%>%bind_rows(
  overall_performance_table%>%filter(complete=='full',filter1==year_to_analyze)%>%
    select(tool,AUC50.)%>%mutate(go_function='overall_performance',tool=stringr::str_replace(tool,'_score.+','')%>%toupper())
  )%>%
  mutate(go_function=relevel(factor(go_function), ref = "overall_performance"))
model <- lme(AUC50. ~ go_function,random=~1 | tool, data = go_func_me_analysis_table)
summary(model)

# which go function has the best overall performance?
performance_by_go_func_table%>%
  ggplot(aes(x=forcats::fct_reorder(go_function,AUC50.),y=AUC50.))+
  geom_boxplot()+
  coord_flip()+
  theme_minimal()


```




# Calibration

```{r calibration}
# calibration plots
text_size<-10
scale_min_max <- function(x) {
  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}

# data for balance
balanced_sample_size_per_tool<-
  proc_data %>%
  select(tools_to_test, clinvar_class, cons_cat) %>%
  pivot_longer(-c(clinvar_class,cons_cat))%>%
  filter(!(is.na(value)|is.na(cons_cat))) %>%
  group_by(name,cons_cat, clinvar_class)%>%
  count()%>%ungroup()%>%group_by(name,cons_cat)%>%summarize(balanced_n=min(n))

proc_data_conservation <- proc_data%>%
    mutate(across(all_of(intersect(converted_scores,tools_to_test)), ~ .x * -1))%>%
    select(tools_to_test,clinvar_class,cons_cat)%>%
    mutate(across(all_of(tools_to_test), scale_min_max))%>%
    filter(!is.na(cons_cat))%>%
    pivot_longer(cols=-c(clinvar_class,cons_cat))

balanced_data<-NULL
for (tool in tools_to_test){
  print(tool)
  for (conservation in unique(proc_data_conservation$cons_cat)){
    balanced_data<-balanced_data%>%bind_rows(
      proc_data_conservation%>%filter(name==tool)%>%
      filter(cons_cat==conservation)%>%
      slice_sample(n=balanced_sample_size_per_tool%>%filter(name==tool,cons_cat==conservation)%>%pull(balanced_n)))
  }
}

# Calibration by conservation, it looks like mos tools overestimate the pathogenicity of low conservation variants - or putting it differently, for a given score, the probability that a variant in a low conservation region is pathogenic is lower compared to higher conservation levels

calibration_by_conservation_plot<-
proc_data_conservation%>%
    #mutate(value_cat=cut(value,breaks=c(seq(0,1,0.05)),labels = seq(0.025,0.975,0.05)))%>%
    mutate(value_cat=cut(value,breaks=c(seq(0,1,0.1)),labels = seq(0.05,0.95,0.1)),
           name=stringr::str_replace(name,'_score.+','')%>%toupper())%>%
    group_by(name,value_cat,cons_cat)%>%
    count(clinvar_class)%>%
    mutate(value_cat=as.numeric(as.character(value_cat)),
           total=sum(n),
           rate=n/sum(n))%>%
    filter(clinvar_class=='P/LP')%>%
    mutate(broom::tidy(binom.test(n,total)))%>%
    ggplot(aes(x=value_cat,y=estimate,color=cons_cat,ymin=conf.low,ymax=conf.high))+
    geom_line()+
    geom_point()+
    facet_wrap(name~.,nrow = 3)+
    geom_errorbar(width=0.01)+
    ggsci::scale_color_cosmic()+
    ylim(0,1)+xlim(0,1)+
    labs(color=NULL,x=NULL,y=NULL)+
    geom_abline(intercept = 0,slope = 1,linetype=2,alpha=0.5)+
    theme_minimal()+
    theme(legend.position='top',text = element_text(size=text_size))
calibration_by_conservation_plot
ggsave(filename = glue('{analysis_folder_name}/calibration_by_conservation_plot.{Sys.Date()}.png'),
       plot=calibration_by_conservation_plot,
       device = 'png',
       height = 7,
       width = 14,
       dpi = 300,bg='white')


```

```{r }
z<-
  roc_metrics_table%>%filter(sub_var_set=='all',missing_option=='no_missing')%>%
  select(tool,main_var_set,bal_accuracy)%>%pivot_wider(names_from = main_var_set,values_from = bal_accuracy)%>%
  rowwise()%>%
  mutate(sd=sd(c(confident_all,confident_2020,confident_2022)),
         diff=confident_all-confident_2020)

proc_data%>%filter(main_var_sets[['confident_2020']],sub_var_sets[['all']])%>%
  select(clinvar_class,contains('_score'))%>%
  pivot_longer(-clinvar_class)%>%
  ggplot(aes(x=value,fill=clinvar_class))+
  geom_histogram(alpha=0.5,position='dodge')+
  facet_wrap(name~.,scales = 'free')+
  scale_fill_manual(values=c('darkcyan','darkred'))+
  theme_minimal()+theme(legend.position = 'top')



# GOF vs LOF
roc_metrics_table%>%
  filter(n>200)%>%
  filter(grepl('AD|AR',sub_var_set))%>%filter(class_balance=='balanced_class')%>%filter(main_var_set=='confident_new')%>%
  ggplot(aes(x=tool,y=`X50.`,color=sub_var_set))+
  geom_point()+
  coord_flip()+
  theme_minimal()


# AD vs AR
roc_metrics_table%>%
  filter(n>200)%>%
  filter(grepl('AD|AR',sub_var_set))%>%filter(class_balance=='balanced_class')%>%filter(main_var_set=='confident_new')%>%
  ggplot(aes(x=tool,y=`X50.`,color=sub_var_set))+
  geom_point()+
  coord_flip()+
  theme_minimal()

# allele freq
roc_metrics_table%>%
  filter(n>50)%>%
  filter(grepl('af_cat',sub_var_set))%>%filter(class_balance=='balanced_class')%>%filter(main_var_set=='confident_new')%>%
  mutate(sub_var_set=forcats::fct_relevel(sub_var_set,"af_cat_NA","af_cat_[0.00e+00,8.24e-06)","af_cat_[8.24e-06,4.91e-05)","af_cat_[4.91e-05,2.60e-04)","[2.60e-04,3.18e-03)","af_cat_[3.18e-03,1.00e+00]"))%>%
  ggplot(aes(x=sub_var_set,y=`X50.`,group=tool))+
  geom_point()+
  geom_line()+
  #coord_flip()+
  theme_minimal()

roc_metrics_table%>%
  filter(n>50)%>%
  filter(grepl('af_cat',sub_var_set))%>%filter(class_balance=='balanced_class')%>%filter(main_var_set=='confident_new')%>%
  ggplot(aes(x=tool,y=`X50.`,color=sub_var_set))+
  geom_point()+
  coord_flip()+
  theme_minimal()


# Grab top performing tools for each category
top_3<-
  roc_metrics_table%>%
  filter(n>1000)%>%
  group_by(main_var_set,sub_var_set,class_balance)%>%
  slice_max(n=3,order_by = `X50.`,with_ties = F)%>%
  mutate(rank=rank(-`X50.`))%>%
  pivot_wider(id_cols = c(main_var_set,sub_var_set,class_balance),names_from = rank,values_from = tool)
  

rocs_table%>%
  ggplot(aes(x=1-specificities,y=sensitivities,color=tool))+
  geom_line(size=1,alpha=0.55)+
  geom_abline(linetype=2,alpha=0.5)+
  #ggsci::scale_color_d3()+
  theme_minimal()+
  labs(color=NULL,x='Sensitivity',y='Specificity',linetype=NULL)+
  theme(legend.position = 'top')
roc_plot

```


