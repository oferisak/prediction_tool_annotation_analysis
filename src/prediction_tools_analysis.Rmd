---
title: "Prediction tools analysis"
author: "Ofer Isakov"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: united
    toc: true
    toc_float: false
params:
  all_founder_vars_annotated_file: ''
  clinvar_am_annotated_file: ''
---

# Comments:
- went over the missing values for the different tools - they are really missing.. 

# TODO : 
- in the panelapp analysis, check the date created per panel (perhaps better performing panels are more represented)
- check the gene content in the significantly under/overperforming subsets
- add features detailed in https://www.nature.com/articles/s41588-024-01821-8 and associate them with performance

## optional test sets
https://genomeinterpretation.org/cagi6-invitae.html - registered waiting for confirmation - checked 06/05/2024 - still no confirmation

## reviews
golden helix review - https://www.goldenhelix.com/blog/evaluating-deepminds-alphamissense-classifier/
review - https://www.frontiersin.org/articles/10.3389/fgene.2022.1010327/full
check the performance of each tool on benign vs pathogenic (maybe benign variants were used in training ans should be replaced)
benchmarking using deep mutational scans - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10407742/
becnchmarking analysis - phenotype based - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9313608/#humu24362-sec-0020title

# possible comparisons
- completeness (how many variants have a score)
- conservation (high vs low)
- in domain (interpro, yes vs no)
- allele frequencies (common vs rare)
- mode of inheritance (AD vs AR vs X linked)
- haploinsufficient / triploinsufficient genes
- pLI  , pRec , under missense constraint, LOF constraint
- known cancer pathogenic? (downloaded data from oncovar. consider uploading as a new project and annotating it)
- disease panels from panelapp

```{r load_project}
# if you want to load a preexisting project just define the folder name
analysis_folder_name<-'./output/prediction_tools_analysis.2024-06-25'
```

```{r markdown_setup,include=FALSE}
setwd('/media/SSD/Bioinformatics/Projects/prediction_tool_annotation_analysis/')

library(ProjectTemplate)
load.project()

data_folder_name<-'./data/preprocessed_data/2024-06-10'
if (exists('analysis_folder_name')){
  # grab the RData file and load it
  rdata_file<-list.files(glue('./{analysis_folder_name}/data'))
  message(glue('loading {rdata_file}..'))
  load(glue('{analysis_folder_name}/data/{rdata_file}'))
}else{
  analysis_folder_name<-glue('./output/prediction_tools_analysis.{Sys.Date()}')
  if (!dir.exists(analysis_folder_name)){
    dir.create(analysis_folder_name)
    dir.create(glue('{analysis_folder_name}/data'))
    }
  # Definitions ####
  # the year that will be used for all subsequent analysis
  year_to_analyze<-'at_or_after_2023'
  # the varity_r score is the best out of the varities, eve and mutpred,mpc,m.cap have a high rate of missingness. 
  # fitCons, lrt, mvp list.s2 tools have a different missingness pattern than the rest and removing them results in a larger set
  excluded_tools<-c('fathmm.xf_coding','esm1b','varity_r_loo','varity_er','varity_er_loo','eve','mutpred','m.cap','mpc','mutationassessor','lrt','mvp','list.s2','h1.hesc_fitcons','huvec_fitcons','gm12878_fitcons','integrated_fitcons') 
}



```

```{r save_load_data}
save.image(file = glue('{analysis_folder_name}/data/{basename(analysis_folder_name)}.RData'))
```

```{r load_preprocessed_data}
# The annotated table
proc_data_file<-grep('processed_data.prediction_tool_annotation',list.files(data_folder_name,full.names = T),value=T)
proc_data<-readr::read_delim(proc_data_file,delim='\t')
# The var sets to analyze
load(glue('{data_folder_name}/var_sets.RData'))
#excluded_tools<-c('esm1b')
tools_list<-grep('_score',colnames(proc_data),value = T)
tools_without_excluded<-setdiff(tools_list,paste0(excluded_tools,'_score.dbnsfp4.5a'))

# converted scores (in which lower score means higher prob of P)
converted_scores<-stringr::str_match(colnames(proc_data),'(.+)_converted')[,2]%>%unique()
converted_scores<-glue('{converted_scores[!is.na(converted_scores)]}_score.dbnsfp4.5a')
```

# Overall performance models 
collect overall performance for each model

```{r rocs}
# Definitions
tools_to_test<-tools_without_excluded

time_var_set<-c('at_or_after_2021','at_or_after_2022','at_or_after_2023','at_or_after_2024')
top_num<-10

analysis_var_sets<-generate_var_set_combinations(var_sets[time_var_set])
# overall performance for the complete var set - no missing data for any of the tools
overall_performance_complete_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_without_excluded,complete = T)
# overall perforamnce for the full var set - the performance of each tool on all of it's annotated variants
overall_performance_full_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_without_excluded,complete = F)

rocs_data<-c(overall_performance_complete_analysis_res$procs,overall_performance_full_analysis_res$procs)
overall_performance_table<-overall_performance_complete_analysis_res$roc_metrics_table%>%
  bind_rows(overall_performance_full_analysis_res$roc_metrics_table)%>%
  filter(tool %in% tools_to_test)%>%
  mutate(complete=ifelse(complete,'complete','full'))
write.table(overall_performance_table,file=glue('{analysis_folder_name}/overall_performance.{Sys.Date()}.csv'),row.names = F,sep='\t')

# Collect the top tools rank
top_tools_rank<-overall_performance_table%>%
  group_by(filter1,complete)%>%
  slice_max(order_by = AUC50.,n=top_num)%>%
  mutate(rank=rank(-AUC50.))%>%
  select(filter1,complete,tool,rank)%>%
  pivot_wider(id_cols = c(tool,complete),names_from = filter1,values_from = rank)

# Collect the top tools auc
top_tools_auc<-overall_performance_table%>%
  group_by(filter1,complete)%>%
  slice_max(order_by = AUC50.,n=top_num)%>%
  select(filter1,complete,tool,AUC50.)%>%
  pivot_wider(id_cols = c(tool,complete),names_from = filter1,values_from = AUC50.)

top_tools<-overall_performance_table%>%group_by(complete)%>%filter(filter1==year_to_analyze)%>%
  slice_max(order_by = AUC50.,n=top_num)%>%select(tool)%>%mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())

# generate the rocs table
raw_roc_table<-NULL
for (var_set in names(rocs_data)){
  split_var_set<-stringr::str_split(var_set,'\\|')%>%unlist()
  tool=split_var_set[2]
  if (!tool %in% tools_to_test){next}
  raw_roc_table<-raw_roc_table%>%bind_rows(
    data.frame(var_set=var_set,
               year=split_var_set[1],
               tool=split_var_set[2],
               complete=split_var_set[3],
               sensitivity=rocs_data[[var_set]]$sensitivities,
               specificity=rocs_data[[var_set]]$specificities)
  )
}

roc_table<-raw_roc_table%>%
    #filter(tool %in% tools_to_test)%>%
    filter(year==year_to_analyze)%>%
    left_join(overall_performance_table%>%
                rename(year=filter1))%>%
    mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  left_join(top_tools%>%mutate(is_top=T))%>%
  mutate(tool_name=ifelse(is.na(is_top),'Other',tool),
         tool_name=forcats::fct_relevel(tool_name,'Other'),
         complete=ifelse(complete=='complete',glue('Complete (Non-missing) set'),glue('Full set')))
colors <- setNames(colorRampPalette(c("darkred", "darkcyan", "darkorange",'darkmagenta'))(length(unique(roc_table$tool_name))), unique(roc_table$tool_name))
colors["Other"] <- "lightgray" 
print(colors)
# Plot the ROC for all the tools and emphasize the top ones
top_models_roc_plot<-
  roc_table%>%filter(tool_name=='Other')%>%
  ggplot(aes(x=1-specificity,y=sensitivity,color=tool_name,group=tool))+
  facet_wrap(complete~.)+
  geom_line(linewidth=1)+
  geom_line(data=roc_table%>%filter(tool_name!='Other'),linewidth=1)+
  geom_abline(slope = 1,intercept = 0,linetype=2,alpha=0.4)+
  scale_color_manual(values=colors)+
  theme_minimal()+
  theme(legend.position = 'top')+
  labs(color=NULL)
print(top_models_roc_plot)
ggsave(filename = glue('{analysis_folder_name}/top_models_roc_plot.{year_to_analyze}.{Sys.Date()}.png'),
       plot=top_models_roc_plot,
       device = 'png',
       height = 7,
       width = 10,
       dpi = 300,bg='white')
# collect the top tools in BOTH complete and full sets
top_performing_tools<-top_tools_rank%>%
  #filter(complete=='complete')%>%
  #slice_min(!!sym(year_to_analyze),n=10)%>%
  pull(tool)%>%stringr::str_replace('_score.+','')%>%toupper()%>%
  unique()

top_performing_tools_original_names<-top_tools_rank%>%pull(tool)%>%unique()

# info on top performing tools for the manuscript
for_text<-overall_performance_table%>%filter(filter1=='at_or_after_2023',grepl('bayes|metarnn|clinpred',tool),complete=='complete')%>%select(contains(c('tool','AUC','precision90')))


# Extract necessary data
metaRNN <- for_text %>% filter(tool == "metarnn_score.dbnsfp4.5a")
bayesDel <- for_text %>% filter(tool == "bayesdel_addaf_score.dbnsfp4.5a")
clinPred <- for_text %>% filter(tool == "clinpred_score.dbnsfp4.5a")

# Create the paragraph
paragraph <- sprintf(
  "MetaRNN, ClinPred and BayesDel outperformed all other tools across different years and for both the complete and full sets (Figure XXX) with a ROCAUC in the complete set of variants created at or after 2023 of %.3f [%.3f,%.3f], %.3f [%.3f,%.3f] and %.3f [%.3f,%.3f], respectively. Setting a threshold corresponding to a precision level of 0.9 (90%% probability that a positive variant is pathogenic), all three tools demonstrated high recall values, identifying %.1f%%, %.1f%% and %.1f%% of pathogenic variants, respectively.",
  metaRNN$AUC50, metaRNN$AUC2.5, metaRNN$AUC97.5,
  clinPred$AUC50, clinPred$AUC2.5, clinPred$AUC97.5,
  bayesDel$AUC50, bayesDel$AUC2.5, bayesDel$AUC97.5,
  metaRNN$recall_precision90 * 100,
  clinPred$recall_precision90 * 100,
  bayesDel$recall_precision90 * 100
)

# Print the paragraph
cat(paragraph)
```

# Missingness
```{r missingness}

library(naniar)
naniar::gg_miss_upset(proc_data%>%select(tools_without_excluded),nsets=20) 

missing_count<-sapply(tools_without_excluded,function(tool) {proc_data%>%filter(is.na(!!sym(tool)))%>%nrow()})
missing_count<-missing_count%>%as.data.frame()%>%rename(n_missing='.')%>%mutate(missing_rate=n_missing/nrow(proc_data))
missing_count
# to get metrics on missingness
skimr::skim(missing_count$missing_rate)

missing_rate_plot<-
missing_count%>%
  mutate(tool=stringr::str_replace(rownames(missing_count),'_score.+','')%>%toupper())%>%
  ggplot(aes(x=forcats::fct_reorder(tool,missing_rate),y=missing_rate,label=glue('{round(missing_rate,2)*100}%')))+
  geom_col(alpha=0.5)+
  geom_text(nudge_y = 0.03,size=3)+
  coord_flip()+
  scale_y_continuous(labels = scales::percent)+
  theme_minimal()+
  labs(x=NULL,y='Missing rate')
missing_rate_plot
ggsave(filename = glue('{analysis_folder_name}/01_missing_rate_plot.{Sys.Date()}.png'),
       plot=missing_rate_plot,
       device = 'png',
       height = 10,
       width = 8,
       dpi = 300,bg='white')

# missing per year
missing_count_per_year<-NULL
#for (year in unique(year(proc_data$date_created))){
for (year in as.character(2013:2024)){
  #year_data<-proc_data%>%filter(year(date_created)==year)
  year_data<-proc_data%>%filter(var_sets[[year]])
  year_missing_count<-sapply(tools_without_excluded,function(tool) {year_data%>%filter(is.na(!!sym(tool)))%>%nrow()})
  missing_count_per_year<-missing_count_per_year%>%
    bind_rows(year_missing_count %>%
                as.data.frame() %>%
                rename(n_missing = '.') %>%
                mutate(tool=tools_without_excluded,
                       year = year, 
                       total_vars = nrow(year_data),
                       missing_rate = n_missing / nrow(year_data)))
}

missing_count_per_year%>%
  ggplot(aes(x=year,y=missing_rate))+
  geom_col()+
  facet_wrap(tool~.)

# check association between date created and missingness
# analyzing effect over time - all tools together
missing_by_year_model <- lme(missing_rate ~ as.numeric(year),random=~1 | tool, data = missing_count_per_year)
summary(missing_by_year_model)
```

# Variant assertion by year added to clinvar
metrics to test options  - AUC , Accuracy, Balanced accuracy
since we are intereseted in the intra-tool variance, then we should use the entire dataset for each tool

```{r perfo_by_time}
library(ggtext)
library(viridis)
library(fuzzyjoin)

# the clinvar_training_date is derived from the date the clinvar dataset was downloaded or if not available - the publication date
tools_with_pub_year<-readxl::read_xlsx('./data/accessory_data/tools_list.xlsx')

# Set the metric you wish to plot
analysis_var_sets<-generate_var_set_combinations(var_sets['all'],var_sets[c(as.character(2013:2024))])
rocs_analysis_results<-calculate_roc_metrics(proc_data,
                                             analysis_var_sets,
                                             tools_without_exclusion,
                                             complete=F)

if (length(excluded_tools)>0){
  tools_to_test<-grep(paste0('^',excluded_tools,'_score',collapse='|'),unique(rocs_analysis_results$roc_metrics_table$tool),value = T,invert = T)
}else{tools_to_test<-tools_list}
perf_by_year_table<-
  rocs_analysis_results$roc_metrics_table%>%
  rename(year=filter2)%>%
  filter(tool %in% tools_to_test)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())
# add publication year
perf_by_year_table<-perf_by_year_table%>%
  left_join(
    tools_with_pub_year%>%
      rename(pub_year="Publication Year")%>%mutate(pub_year=as.character(pub_year)))%>%
  mutate(pub_year=as.character(pub_year),
         training_year=as.character(clinvar_training_date))
write.table(perf_by_year_table,file=glue('{analysis_folder_name}/perf_by_year_table.{Sys.Date()}.csv'),row.names = F,sep='\t')

# plot for all tools
perf_metric<-'AUC50.'
perf_by_year_all_plot<-
  perf_by_year_table%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  ggplot(aes_string(x='year',y=perf_metric,group='tool'))+
  geom_point(size=1)+geom_line()+
  facet_wrap(tool~.,scales='free')+
  labs(x='ClinVar Year',y=perf_metric)+
  #scale_color_viridis_d(option = 'turbo')+
  geom_vline(aes(xintercept=training_year),data=perf_by_year_table,linetype=2,color='darkred')+
  theme_minimal()+
  theme(legend.position = 'top')+
  labs(color=NULL)+theme(axis.text.x = element_text(angle = 90))
perf_by_year_all_plot

ggsave(filename = glue('{analysis_folder_name}/all_tools_{perf_metric}_by_year_plot.{Sys.Date()}.png'),
       plot=perf_by_year_all_plot,
       device = 'png',
       height = 8,
       width = 12,
       dpi = 300,bg='white')

# plot for top tools
perf_by_year_top_plot<-
  perf_by_year_table%>%
  filter(tool %in% top_performing_tools)%>%
  ggplot(aes_string(x='year',y=perf_metric,group='tool'))+
  geom_point(size=1)+geom_line()+
  facet_wrap(tool~.,nrow=3)+
  labs(x='ClinVar Year',y=perf_metric)+
  #scale_color_viridis_d(option = 'turbo')+
  geom_vline(aes(xintercept=training_year),data=perf_by_year_table%>%
               filter(tool %in% top_performing_tools),linetype=2,color='darkred')+
  theme_minimal()+
  theme(legend.position = 'top')+
  labs(color=NULL)+theme(axis.text.x = element_text(angle = 90))
perf_by_year_top_plot

ggsave(filename = glue('{analysis_folder_name}/top_tools_{perf_metric}_by_year_plot.{Sys.Date()}.png'),
       plot=perf_by_year_top_plot,
       device = 'png',
       height = 6,
       width = 12,
       dpi = 300,bg='white')


# analyze results per tool to identify tools with decreasing values
year_analysis_table <- perf_by_year_table %>%mutate(year=as.numeric(year),
                                      before_training=ifelse(year<=as.numeric(training_year),
                                                             'before_training','after_training'))%>%
  mutate(ifelse(year<=2013,NA,year))%>%
  mutate(before_training=forcats::fct_relevel(before_training,'before_training'))
  
no_clinvar_training_tools<-year_analysis_table%>%group_by(tool)%>%
  summarize(before_and_after=length(unique(before_training)))%>%
  filter(before_and_after<2)%>%pull(tool)

# analyzing effect over time - for every tool 
year_effect_model_by_tool<-NULL
for (tool2analyze in unique(year_analysis_table$tool)){
  print(tool2analyze)
  tool_analysis<-tidy(lm(AUC50.~(year>=2020),data=year_analysis_table%>%filter(tool==tool2analyze)))
  year_effect_model_by_tool<-year_effect_model_by_tool%>%
    bind_rows(data.frame(tool=tool2analyze,
                         tool_analysis%>%filter(grepl('year',term))))
}

year_effect_model_by_tool%>%filter(p.value<0.05)

# analyzing effect over time - all tools together
year_effect_model <- lme(AUC50. ~ (year>=2020),random=~1 | tool, data = year_analysis_table%>%filter(tool%in%top_performing_tools))
summary(year_effect_model)

year_effect_model_by_tool
# analyze effect before and after training
lm_by_training_year<-
  year_analysis_table%>%
  filter(!tool %in% no_clinvar_training_tools)%>%
  group_by(tool) %>%
  do(tidy(lm(AUC50. ~ before_training, data = .)))%>%
  filter(term=='before_trainingafter_training')


```

# Affect of conservation

## Overall 
```{r conservation_effec_overall}
conservation_var_sets<-grep('conserv',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[year_to_analyze],var_sets[conservation_var_sets])

# get the overall performance for variants with conservation scores
var_sets[['all_conservation']]<-!is.na()
all_panelapp_analysis_res<-calculate_roc_metrics(proc_data,data.frame(filter1=year_to_analyze,filter2='all_panelapp'),tools_to_test,complete = F,save_rocs=F)

performance_by_conservation_table<-
  conservation_analysis_res$roc_metrics_table%>%
  filter(tool %in% tools_to_test)%>%
  rename(conservation_level=filter2)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.),
         conservation_level=stringr::str_replace(conservation_level,'_conservation','')%>%stringr::str_to_title(),
         conservation_level=factor(conservation_level,levels = c('Low','Intermediate','High')))

# add overall performance
performance_by_conservation_table<-performance_by_conservation_table%>%left_join(
  overall_performance_table%>%filter(filter1==year_to_analyze,complete=='full')%>%
    mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
    select(filter1,tool,overall_total=total,overall_AUC50=AUC50.,overall_sens=sens,overall_spec=spec)
)

write.table(performance_by_conservation_table,file=glue('{analysis_folder_name}/performance_by_conservation_table.{Sys.Date()}.csv'),row.names = F,sep='\t')

# add min-max difference for the plot
performance_by_conservation_table<-performance_by_conservation_table%>%
  left_join(performance_by_conservation_table%>%group_by(tool)%>%mutate(min_auc=min(AUC50.),max_auc=max(AUC50.),min_max_diff=max_auc-min_auc)%>%select(tool,conservation_level,min_max_diff))
# order to the tools according to the min-max difference 
performance_by_conservation_table<-performance_by_conservation_table%>%mutate(tool=forcats::fct_reorder(tool,min_max_diff))

performance_by_conservation_table%>%
  select(conservation_level,tool,total,AUC50.)%>%
  pivot_wider(id_cols = tool,names_from = conservation_level,values_from = AUC50.)

# analyze results using mixed effects model 
# discriminatory performance using all the tools
cons_auc_model <- lme(AUC50. ~ conservation_level,random=~1 | tool, data = performance_by_conservation_table)
summary(cons_auc_model)
# discriminatory performance using the top tools
cons_auc_model_top_tools <- lme(AUC50. ~ conservation_level,random=~1 | tool, data = performance_by_conservation_table%>%filter(tool %in% top_performing_tools))
summary(cons_auc_model_top_tools)

# sensitivity and specificity using tools that have predictions
cons_sens_model<-lme(sens ~ conservation_level,random=~1 | tool, data = performance_by_conservation_table%>%filter(!is.na(sens)))
summary(cons_sens_model)
cons_spec_model<-lme(spec ~ conservation_level,random=~1 | tool, data = performance_by_conservation_table%>%filter(!is.na(spec)))
summary(cons_spec_model)

# plot the results
conservation_to_plot<-
performance_by_conservation_table%>%
  filter(!is.na(sens))%>%
  pivot_longer(-c(tool,filter1,conservation_level,total))%>%
  mutate(name=forcats::fct_recode(name,AUC='AUC50.',Sensitivity='sens',Specificity='spec'))%>%
  filter(name%in%c('AUC','Sensitivity','Specificity'))%>%
  #filter(name%in%c('AUC'))%>%
  left_join(
    performance_by_conservation_table%>%
    filter(!is.na(sens))%>%
    pivot_longer(-c(tool,filter1,conservation_level,total))%>%
    mutate(name=forcats::fct_recode(name,AUC='AUC50.',Sensitivity='sens',Specificity='spec'))%>%
    filter(name%in%c('AUC','Sensitivity','Specificity'))%>%
    #filter(name%in%c('AUC'))%>%
    group_by(tool,name)%>%
      summarize(min_val=min(value),max_val=max(value))%>%ungroup()
  )

cons_overall_performance<-
  performance_by_conservation_table%>%
  filter(!is.na(sens))%>%
  pivot_longer(-c(tool,filter1,conservation_level,total))%>%
  mutate(name=forcats::fct_recode(name,AUC='overall_AUC50',Sensitivity='overall_sens',Specificity='overall_spec'))%>%
  filter(name%in%c('AUC','Sensitivity','Specificity'))%>%select(-c(total,conservation_level))%>%distinct()

performance_by_conservation_plot<-
  conservation_to_plot%>%
  ggplot(aes(color=conservation_level,y=value,x=tool,ymin=min_val,ymax=max_val))+
  geom_errorbar(color='gray',width=0)+
  geom_point(size=2)+
  geom_point(data = cons_overall_performance, aes(y = value, x = tool, shape = "Overall performance"), fill = "black", size = 2, inherit.aes = FALSE) +
  facet_grid(.~name)+
  coord_flip()+
  theme_minimal()+
  scale_color_manual(values=c('darkcyan','darkorange','darkred'))+
  scale_shape_manual(values = c("Overall performance" = 3)) +
  labs(color='Conservation',x=NULL,shape=NULL,y=NULL)+
  theme(legend.position = 'top',strip.text = element_text(face='bold'))
performance_by_conservation_plot

ggsave(filename = glue('{analysis_folder_name}/performance_by_conservation_plot.{year_to_analyze}.{Sys.Date()}.png'),
       plot=performance_by_conservation_plot,
       device = 'png',
       height = 7,
       width = 10,
       dpi = 300,bg='white')


# a second plot option.. 
g<-conservation_to_plot%>%
  ggplot(aes(color=conservation_level,y=value,x=name,ymin=min_val,ymax=max_val))+
  geom_errorbar(color='gray',width=0)+
  geom_point(size=2)+
  coord_flip()+
  facet_grid(tool~.,space='free',scales='free',switch = 'both',labeller = label_wrap_gen(width = 6))+
  theme_minimal()+
  scale_color_manual(values=c('darkcyan','darkorange','darkred'))+
  theme(legend.position = 'bottom',strip.placement = 'outside',strip.text.y.left = element_text(angle=0,face = 'bold'))+
  geom_vline(xintercept = 0.4,alpha=0.75)+theme(panel.spacing = unit(0, "lines"))
g

```

## Comparions
```{r conservation_effec_comparison}
conservation_var_sets<-grep('conserv',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[year_to_analyze],var_sets[conservation_var_sets])

# for inter tool comparison
conservation_analysis_res_complete<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_to_test,complete = T,save_rocs = F)

performance_by_conservation_table_complete<-
  conservation_analysis_res_complete$roc_metrics_table%>%
  filter(tool %in% tools_to_test)%>%
  rename(conservation_level=filter2)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.),
         conservation_level=stringr::str_replace(conservation_level,'_conservation','')%>%stringr::str_to_title(),
         conservation_level=factor(conservation_level,levels = c('Low','Intermediate','High')))


top_tools_conservation <- performance_by_conservation_table_complete %>%
  filter(!tool%in%c('BAYESDEL_NOAF'))%>%
  group_by(conservation_level) %>%
  top_n(10, AUC50.) %>%
  arrange(conservation_level, desc(AUC50.)) %>%
  mutate(rank = row_number(),  # Assign ranks within each conservation level
         conservation_level = factor(conservation_level, levels = c("Low", "Intermediate", "High")),
         tool_label=glue('{tool} ({round(AUC50.,3)})'))%>%
  ungroup() 

# Create the bump plot
library(ggbump)
library(PrettyCols)
performance_by_conservation_bump_plot<-
ggplot(top_tools_conservation, aes(x = conservation_level, y = rank, group = tool, label = tool_label,color = tool)) +
  geom_bump( linewidth = 1.2, alpha = 0.8) +
  geom_text(aes(y = rank), nudge_y = 0.3, check_overlap = TRUE, size = 3.5, color = "black") +
  scale_y_reverse(breaks = 1:10) +  # Reverse scale so top tools are at the top
  labs(x = "Conservation Level", y = "AUROC Rank") +
  theme_minimal() +
  PrettyCols::scale_color_pretty_d('Autumn',scale_name='Autumn')+
  theme(legend.position = "none")
performance_by_conservation_bump_plot

ggsave(filename = glue('{analysis_folder_name}/performance_by_conservation_bump_plot.{year_to_analyze}.{Sys.Date()}.png'),
       plot=performance_by_conservation_bump_plot,
       device = 'png',
       height = 6,
       width = 9,
       dpi = 300,bg='white')
```

# MOI analysis

## Overall comparison
```{r moi}
tools_to_test<-tools_without_excluded
#tools_to_test<-top_tools_2023
moi_var_sets<-grep('AD|AR|prec|pli',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[year_to_analyze],var_sets[moi_var_sets])
moi_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_to_test,complete=F,save_rocs = F)

performance_by_moi_table<-
  moi_analysis_res$roc_metrics_table%>%
  rename(moi=filter2)%>%
  #filter(tool %in% tools_to_test)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.),
         complete=ifelse(complete,'complete','full'))
write.table(performance_by_moi_table,file=glue('{analysis_folder_name}/performance_by_moi_{year_to_analyze}_table.{Sys.Date()}.csv'),row.names = F,sep='\t')

# plot the difference between AD and AR
moi_to_plot<-performance_by_moi_table%>%
  filter(moi%in%c('AD','AR'))%>%select(moi,tool,AUC50.)%>%
  pivot_wider(id_cols = tool,names_from = moi,values_from = AUC50.)%>%
  left_join(
    performance_by_moi_table%>%
      filter(moi%in%c('prec_high','pli_high'))%>%select(moi,tool,AUC50.)%>%
      pivot_wider(id_cols = tool,names_from = moi,values_from = AUC50.)
  )%>%
  mutate('AD vs AR'=AD-AR,
         'pLI vs pRec'=pli_high-prec_high,
         tool=forcats::fct_reorder(tool,desc(`AD vs AR`)))%>%
  select(tool,`AD vs AR`,`pLI vs pRec`)%>%
  pivot_longer(-tool)
performance_by_moi_plot<-
moi_to_plot%>%
  ggplot(aes(y=value,x=tool,color=value>0,fill=value>0))+
  geom_point(size=3)+
  geom_col(linewidth=0.1,width = 0.1)+
  geom_hline(yintercept = 0,linetype=2,color='gray')+
  facet_grid(.~name)+
  scale_fill_manual(values=c('darkred','darkgreen'),labels=c('Better perofrmance in AR/pRec','Better performance in AD/pLI'))+
  scale_color_manual(values=c('darkred','darkgreen'))+
  coord_flip()+
  labs(y='AUC difference',fill=NULL,x=NULL)+guides(color='none')+
  theme_minimal()+theme(strip.text = element_text(face='bold'),legend.position = 'bottom',
                        axis.text.y = element_text(face=ifelse(levels(moi_to_plot$tool)%in%top_performing_tools,"bold","italic")))
performance_by_moi_plot

ggsave(filename = glue('{analysis_folder_name}/performance_by_moi_plot.{Sys.Date()}.png'),
       plot=performance_by_moi_plot,
       device = 'png',
       height = 8,
       width = 9,
       dpi = 300,bg='white')

library(nlme)
# discriminatory performance using all the tools AD vs AR
moi_auc_model_ad_vs_ar <- lme(AUC50. ~ moi,random=~1 | tool, data = performance_by_moi_table%>%filter(moi%in%c('AD','AR')))
summary(moi_auc_model_ad_vs_ar)
# discriminatory performance using all the tools pLI vs pREC
moi_auc_model_pLI_vs_pRec <- lme(AUC50. ~ moi,random=~1 | tool, data = performance_by_moi_table%>%filter(moi%in%c('pli_high','prec_high')))
summary(moi_auc_model_pLI_vs_pRec)

# sensitivity and specificity using tools that have predictions
moi_sens_model<-lme(sens ~ moi,random=~1 | tool, data = performance_by_moi_table%>%filter(moi%in%c('AD','AR'))%>%filter(!is.na(sens)))
summary(moi_sens_model)
moi_spec_model<-lme(spec ~ moi,random=~1 | tool, data = performance_by_moi_table%>%filter(moi%in%c('AD','AR'))%>%filter(!is.na(spec)))
summary(moi_spec_model)

# plot with CI
# performance_by_moi_table%>%
#   filter(moi%in%c('AD','AR'))%>%
#   pivot_wider(id_cols = tool,names_from = moi,values_from = c(AUC50.,AUC2.5,AUC97.5))%>%
#   mutate(SE_AD = (AUC97.5_AD - AUC2.5_AD) / (2 * 1.96),
#          SE_AR = (AUC97.5_AR - AUC2.5_AR) / (2 * 1.96)) %>%
#   summarize(
#     mean_diff = AUC50._AD - AUC50._AR,
#     se_diff = sqrt(SE_AD^2 + SE_AR^2),
#     ci_lower = mean_diff - 1.96 * se_diff,
#     ci_upper = mean_diff + 1.96 * se_diff,
#     tool=forcats::fct_reorder(tool,mean_diff)
#   )%>%
#   ggplot(aes(x=tool,y=mean_diff,ymin=ci_lower,ymax=ci_upper))+
#   geom_point()+geom_errorbar()+
#   geom_hline(yintercept = 0,linetype=2)+
#   coord_flip()+
#   theme_minimal()

# ran the comparison with the complete set, no real insights
```


# Misense constraint 

```{r missense_constraint}
time_var_set<-'at_or_after_2023'
af_var_sets<-c(grep('mismatch_min_oe_ratio:',names(var_sets),value = T),
                       'regeneron_constrained','regeneron_not_constrained')
#af_var_sets<-grep('regeneron',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[time_var_set],var_sets[mismatchoe_var_sets])
mismatchoe_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_to_test,complete = F,save_rocs = F)

performance_by_mismatchoe_table<-
  mismatchoe_analysis_res$roc_metrics_table%>%
  rename(mismatch_oe=filter2)%>%
  filter(tool %in% tools_to_test)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.),
         mismatch_oe=stringr::str_replace(mismatch_oe,'mismatch_min_oe_ratio:','')
         )
write.table(performance_by_moi_table,file=glue('{analysis_folder_name}/performance_by_moi_{time_var_set}_table.{Sys.Date()}.csv'),row.names = F,sep='\t')

performance_by_mismatchoe_table%>%
  filter(!(grepl('regeneron',mismatch_oe)))%>%
  filter(tool %in% top_performing_tools)%>%
  ggplot(aes(x=mismatch_oe,y=AUC50.,group=tool,ymin=AUC2.5,ymax=AUC97.5))+
  geom_line()+
  geom_point()+
  geom_errorbar(width=0)+
  facet_wrap(tool~.)+
  theme_minimal()

performance_by_mismatchoe_plot<-
performance_by_mismatchoe_table%>%
  filter(!(grepl('regeneron',mismatch_oe)))%>%mutate(mismatch_oe=as.numeric(mismatch_oe))%>%
  filter(tool %in% top_performing_tools)%>%
  ggplot(aes(x=mismatch_oe,y=AUC50.,group=tool,ymin=AUC2.5,ymax=AUC97.5,group=tool))+
  geom_line()+
  geom_point()+
  geom_smooth(inherit.aes = F,aes(x=mismatch_oe,y=AUC50.),color='darkred')+
  #facet_wrap(tool~.)+
  theme_minimal()
performance_by_mismatchoe_plot

ggsave(filename = glue('{analysis_folder_name}/performance_by_mismatch_oe_plot.{Sys.Date()}.png'),
       plot=performance_by_mismatchoe_plot,
       device = 'png',
       height = 8,
       width = 8,
       dpi = 300,bg='white')

# analyze results using mixed effects model
library(nlme)
model <- lme(AUC50. ~ mismatch_oe,random=~1 | tool, data = performance_by_mismatchoe_table%>%filter(!(grepl('regeneron',mismatch_oe)))%>%mutate(mismatch_oe=as.numeric(mismatch_oe)))
summary(model)
plot(model)

# find the tools with the highest difference in performance
performance_difference_mismatchoe<-performance_by_mismatchoe_table%>%
  filter(tool %in% top_performing_tools)%>%
  group_by(tool)%>%
  summarize(mean_AUC50=mean(AUC50.),
            min_auc50=min(AUC50.),
            max_auc50=max(AUC50.),
            worst_perf=min_auc50-mean_AUC50,
            worst_perf_constraint=mismatch_oe[which(AUC50.==min_auc50)])


```

# Allele frequency
seems there is no clear trend not sure need to add this to the analysis
```{r allele frequency}
time_var_set<-'at_or_after_2023'
af_var_sets<-c(grep('af_cat',names(var_sets),value = T))
#af_var_sets<-grep('regeneron',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[time_var_set],var_sets[af_var_sets])
af_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_to_test,complete = F,save_rocs = F)

performance_by_af_table<-
  af_analysis_res$roc_metrics_table%>%
  rename(af=filter2)%>%
  mutate(af=forcats::fct_relevel(af,'af_cat_unknown','af_cat_(0,1e-06]','af_cat_(1e-06,1e-05]','af_cat_(1e-05,0.0001]','af_cat_(0.0001,0.001]','af_cat_(0.001,1]'))%>%
  filter(tool %in% tools_to_test)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.),
         mismatch_oe=stringr::str_replace(af,'af_cat_','')
         )
write.table(performance_by_moi_table,file=glue('{analysis_folder_name}/performance_by_af_{time_var_set}_table.{Sys.Date()}.csv'),row.names = F,sep='\t')

performance_by_af_table%>%
  filter(!af%in%c('af_cat_(0.001,1]','af_cat_unknown'))%>%
  filter(tool %in% top_performing_tools)%>%
  ggplot(aes(x=af,y=AUC50.,group=tool,ymin=AUC2.5,ymax=AUC97.5))+
  geom_line()+
  geom_point()+
 # geom_errorbar(width=0)+
  facet_wrap(tool~.,scales='free')+
  theme_minimal()

```

# PanelApp

```{r panelapp}
panelapp_cols<-grep('panelapp:',colnames(proc_data),value = T)
min_count_thresh<-200
panelapp_counts<-data.frame()
for (pa_col in panelapp_cols){
  print(pa_col)
  for (tool in tools_to_test){
    min_count<-proc_data%>%filter(var_sets[[year_to_analyze]],!is.na(!!sym(tool)))%>%count(!!sym(pa_col),clinvar_class)%>%filter(!is.na(!!sym(pa_col)))%>%pull(n)%>%min()
    panelapp_counts<-panelapp_counts%>%bind_rows(data.frame(pa_col,tool,min_count=min_count))
  }
}
panelapp_to_test<-panelapp_counts%>%
  group_by(pa_col)%>%
  summarize(all_tools_above_thresh=all(min_count>min_count_thresh))%>%
  filter(all_tools_above_thresh)%>%pull(pa_col)
# add var_sets
for (pa_to_test in panelapp_to_test){
  var_sets[[pa_to_test]]<-proc_data%>%pull(pa_to_test)
}

#mismatchoe_var_sets<-grep('regeneron',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[year_to_analyze],var_sets[panelapp_to_test])
panelapp_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_to_test,complete = F,save_rocs=F)
# calculate performance for all the variants that have a panelapp 
var_sets[['all_panelapp']]<-apply(proc_data[, panelapp_to_test], 1, function(row) any(row == TRUE))
all_panelapp_analysis_res<-calculate_roc_metrics(proc_data,data.frame(filter1=year_to_analyze,filter2='all_panelapp'),tools_to_test,complete = F,save_rocs=F)

performance_by_panelapp_table<-
  panelapp_analysis_res$roc_metrics_table%>%
  rename(panelapp=filter2)%>%
  filter(tool %in% tools_to_test)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.),
         panelapp=stringr::str_replace(panelapp,'panelapp:','')
         )

# consider instead of running nle on all the data, splitting the analysis into multiple tests each time just the overall versus the panel

library(nlme)
pa_all_comps<-NULL
for (pa in performance_by_panelapp_table%>%pull(panelapp)%>%unique()){
  pa_comp_table<-performance_by_panelapp_table%>%filter(panelapp==pa)%>%
    bind_rows(
      all_panelapp_analysis_res$roc_metrics_table%>%
        mutate(panelapp='overall_performance',tool=stringr::str_replace(tool,'_score.+','')%>%toupper())
    )%>%
  mutate(panelapp=relevel(factor(panelapp), ref = "overall_performance"))
  model <- lme(AUC50. ~ panelapp,random=~1 | tool, data = pa_comp_table)
  model_res<-summary(model)
  class_balance<-proc_data%>%filter(!!sym(glue('panelapp:{pa}')))%>%count(clinvar_class)%>%pivot_wider(names_from = clinvar_class,values_from = n)
  pa_all_comps<-pa_all_comps%>%bind_rows(
    data.frame(panelapp=pa,data.frame(t(model_res$tTable[2,])),class_balance)
  )
}

pa_all_comps<-pa_all_comps%>%mutate(class_balance=`P.LP`/`B.LB`)
pa_all_comps$p.adjust<-p.adjust(pa_all_comps$p.value,method = 'bonferroni')

# check that the reason for the difference is not gene composition (its not)
setdiff(proc_data%>%filter(`panelapp:Structural.eye.disease`)%>%pull(gene.names.clinvar.2024.04.04..ncbi)%>%unique(),
        proc_data%>%filter(`panelapp:Adult.onset.neurodegenerative.disorder`)%>%pull(gene.names.clinvar.2024.04.04..ncbi)%>%unique())

library(ggrepel)
pa_all_comps%>%
  ggplot(aes(x=Value,y=-log(p.value),label=panelapp))+
  geom_point()+
 # geom_point(data=pa_all_comps%>%filter(p.value<0.01),color='blue')+
  geom_point(data=pa_all_comps%>%filter(p.adjust<0.05),color='red')+
  geom_label_repel(data=pa_all_comps%>%filter(p.adjust<0.05),nudge_y = 0.5)+
  theme_minimal()

# now for each tool, compare each panel against all the rest
pa_per_tool_comp<-NULL
for (pa in performance_by_panelapp_table%>%pull(panelapp)%>%unique()){
  print(pa)
  for (selected_tool in performance_by_panelapp_table%>%pull(tool)%>%unique()){
    tool_pa_comp_table<-performance_by_panelapp_table%>%filter(tool==selected_tool)%>%
      mutate(panelapp=ifelse(panelapp==pa,pa,'Other'),
             panelapp=forcats::fct_relevel(panelapp,'Other'))
    model <- tidy(lm(AUC50. ~ panelapp,data = tool_pa_comp_table))
    pa_per_tool_comp<-pa_per_tool_comp%>%bind_rows(
      data.frame(panelapp=pa,tool=selected_tool,model%>%filter(term==glue('panelapp{pa}'))))
  }
}

pa_per_tool_comp<-NULL
for (selected_tool in performance_by_panelapp_table%>%pull(tool)%>%unique()){
  print(selected_tool)
  for (pa in performance_by_panelapp_table%>%pull(panelapp)%>%unique()){
    mean_auc_tool<-performance_by_panelapp_table%>%filter(tool==selected_tool,panelapp!=pa)%>%pull(AUC50.)%>%mean()
    sd_auc_tool<-performance_by_panelapp_table%>%filter(tool==selected_tool,panelapp!=pa)%>%pull(AUC50.)%>%sd()
    tool_pa_auc<-performance_by_panelapp_table%>%filter(tool==selected_tool,panelapp==pa)%>%pull(AUC50.)
    # Calculate the cumulative probability up to X
    cum_prob_pa <- pnorm(tool_pa_auc, mean = mean_auc_tool, sd = sd_auc_tool)
    #print(glue("{tool}: Cumulative probability of the AUC ({tool_pa_auc}) given average AUC of {mean_auc_tool}: {cum_prob_pa} | {prob_density_pa}"))
    pa_per_tool_comp<-pa_per_tool_comp%>%bind_rows(
      data.frame(tool=selected_tool,panelapp=pa,tool_pa_auc,mean_auc_tool,p1=cum_prob_pa,p2=1-cum_prob_pa)
    )
    }
}

pa_per_tool_comp<-pa_per_tool_comp%>%
  mutate(diff=tool_pa_auc-mean_auc_tool)%>%
  rowwise()%>%mutate(p.value=min(p1,p2))%>%ungroup()


to_plot<-pa_per_tool_comp%>%
  filter(tool %in% top_performing_tools)%>%
  filter(panelapp %in% (pa_all_comps%>%filter(p.value<0.05)%>%pull(panelapp)))
to_plot$p.adjust<-p.adjust(to_plot$p.value,method='fdr')
to_plot%>%
  ggplot(aes(x=diff,y=-log(p.value),label=panelapp))+
  geom_point()+
  geom_point(data=to_plot%>%filter(p.adjust<0.05,diff>0.025),color='lightgreen')+
  geom_point(data=to_plot%>%filter(p.adjust<0.05,diff<(-0.025)),color='lightpink')+
  geom_label_repel(data=to_plot%>%filter(p.adjust<0.05,diff>0.025),fill='lightgreen')+
  geom_label_repel(data=to_plot%>%filter(p.adjust<0.05,diff<(-0.025)),fill='lightpink')+
  facet_wrap(tool~.)+
  theme_minimal()
```


# GO functional / bio process

```{r go_process_analysis}
go_cols<-grep('go_process:',colnames(proc_data),value = T)
min_count_thresh<-200
go_process_counts<-data.frame()
for (go_col in go_cols){
  print(go_col)
  for (tool in tools_to_test){
    min_count<-proc_data%>%filter(var_sets[[year_to_analyze]],!is.na(!!sym(tool)))%>%count(!!sym(go_col),clinvar_class)%>%filter(!is.na(!!sym(go_col)))%>%pull(n)%>%min()
    go_process_counts<-go_process_counts%>%bind_rows(data.frame(go_col,tool,min_count=min_count))
  }
}
go_process_to_test<-go_process_counts%>%group_by(go_col)%>%summarize(all_tools_above_thresh=all(min_count>min_count_thresh))%>%filter(all_tools_above_thresh)%>%pull(go_col)
# add var_sets
for (go_to_test in go_process_to_test){
  var_sets[[go_to_test]]<-proc_data%>%pull(go_to_test)
}

#mismatchoe_var_sets<-grep('regeneron',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[year_to_analyze],var_sets[go_process_to_test])
go_process_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_to_test,complete = F,save_rocs = F)

performance_by_go_process_table<-
  go_process_analysis_res$roc_metrics_table%>%
  rename(go_process=filter2)%>%
  filter(tool %in% tools_to_test)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.),
         go_process=stringr::str_replace(go_process,'go_process:','')
         )

performance_by_go_process_table<-
performance_by_go_process_table%>%left_join(
  overall_performance_table%>%filter(complete=='full',filter1==year_to_analyze)%>%
    select(tool,overall_performance=AUC50.)%>%mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())
)%>%
  mutate(tool=forcats::fct_reorder(tool,overall_performance),)
performance_by_go_process_table%>%
  filter(tool%in%(top_tools%>%filter(complete=='complete')%>%pull(tool)))%>%
  mutate(color=case_when(
    AUC50.<overall_performance & AUC97.5<overall_performance~'decrease',
    AUC50.>overall_performance & AUC2.5>overall_performance~'increase',
    .default='no_change'))%>%
  ggplot(aes(x=tool,y=AUC50.,ymin=AUC2.5,ymax=AUC97.5,color=color))+
  geom_point()+
  geom_errorbar(width=0.5)+
  geom_point(aes(x=tool,y=overall_performance),inherit.aes = F,shape=3)+
  coord_flip()+
  scale_color_manual(values=c('increase'='darkblue','no_change'='black','decrease'='darkred'))+
  facet_wrap(go_process~.,scales='free_x')+
  guides(color='none')+
  theme_minimal()

library(nlme)
go_me_analysis_table<-
performance_by_go_process_table%>%bind_rows(
  overall_performance_table%>%filter(complete=='full',filter1==year_to_analyze)%>%
    select(tool,AUC50.)%>%mutate(go_process='overall_performance',tool=stringr::str_replace(tool,'_score.+','')%>%toupper())
  )%>%
  mutate(go_process=relevel(factor(go_process), ref = "overall_performance"))

model <- lme(AUC50. ~ go_process,random=~1 | tool, data = go_me_analysis_table)
summary(model)
plot(model)

# which go process has the best overall performance?
performance_by_go_process_table%>%
  ggplot(aes(x=forcats::fct_reorder(go_process,AUC50.),y=AUC50.))+
  geom_boxplot()+
  geom_point(size=1)+
  coord_flip()+
  theme_minimal()

```

```{r go_function_analysis}
go_func_cols<-grep('go_function:',colnames(proc_data),value = T)
min_count_thresh<-200
go_func_counts<-data.frame()
for (go_col in go_func_cols){
  print(go_col)
  for (tool in tools_to_test){
    min_count<-proc_data%>%filter(var_sets[[year_to_analyze]],!is.na(!!sym(tool)))%>%count(!!sym(go_col),clinvar_class)%>%filter(!is.na(!!sym(go_col)))%>%pull(n)%>%min()
    go_func_counts<-go_func_counts%>%bind_rows(data.frame(go_col,tool,min_count=min_count))
  }
}
go_func_to_test<-go_func_counts%>%group_by(go_col)%>%summarize(all_tools_above_thresh=all(min_count>min_count_thresh))%>%filter(all_tools_above_thresh)%>%pull(go_col)
# add var_sets
for (go_to_test in go_func_to_test){
  var_sets[[go_to_test]]<-proc_data%>%pull(go_to_test)
}

#mismatchoe_var_sets<-grep('regeneron',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[year_to_analyze],var_sets[go_func_to_test])
go_func_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_to_test,complete = F)

performance_by_go_func_table<-
  go_func_analysis_res$roc_metrics_table%>%
  rename(go_function=filter2)%>%
  filter(tool %in% tools_to_test)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.),
         go_function=stringr::str_replace(go_function,'go_function:','')
         )

performance_by_go_func_table<-
performance_by_go_func_table%>%left_join(
  overall_performance_table%>%filter(complete=='full',filter1==year_to_analyze)%>%
    select(tool,overall_performance=AUC50.)%>%mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())
)%>%
  mutate(tool=forcats::fct_reorder(tool,overall_performance),)
performance_by_go_func_table%>%
  filter(tool%in%(top_tools%>%filter(complete=='complete')%>%pull(tool)))%>%
  mutate(color=case_when(
    AUC50.<overall_performance & AUC97.5<overall_performance~'decrease',
    AUC50.>overall_performance & AUC2.5>overall_performance~'increase',
    .default='no_change'))%>%
  ggplot(aes(x=tool,y=AUC50.,ymin=AUC2.5,ymax=AUC97.5,color=color))+
  geom_point()+
  geom_errorbar(width=0.5)+
  geom_point(aes(x=tool,y=overall_performance),inherit.aes = F,shape=3)+
  coord_flip()+
  scale_color_manual(values=c('increase'='darkblue','no_change'='black','decrease'='darkred'))+
  facet_wrap(go_function~.)+
  guides(color='none')+
  theme_minimal()

library(nlme)
go_func_me_analysis_table<-
performance_by_go_func_table%>%bind_rows(
  overall_performance_table%>%filter(complete=='full',filter1==year_to_analyze)%>%
    select(tool,AUC50.)%>%mutate(go_function='overall_performance',tool=stringr::str_replace(tool,'_score.+','')%>%toupper())
  )%>%
  mutate(go_function=relevel(factor(go_function), ref = "overall_performance"))
model <- lme(AUC50. ~ go_function,random=~1 | tool, data = go_func_me_analysis_table)
summary(model)

# which go function has the best overall performance?
performance_by_go_func_table%>%
  ggplot(aes(x=forcats::fct_reorder(go_function,AUC50.),y=AUC50.))+
  geom_boxplot()+
  coord_flip()+
  theme_minimal()


```




# Calibration

```{r calibration}
# calibration plots
text_size<-10
scale_min_max <- function(x) {
  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}

# data for balance
balanced_sample_size_per_tool<-
  proc_data %>%
  select(tools_to_test, clinvar_class, cons_cat) %>%
  pivot_longer(-c(clinvar_class,cons_cat))%>%
  filter(!(is.na(value)|is.na(cons_cat))) %>%
  group_by(name,cons_cat, clinvar_class)%>%
  count()%>%ungroup()%>%group_by(name,cons_cat)%>%summarize(balanced_n=min(n))

proc_data_conservation <- proc_data%>%
    mutate(across(all_of(intersect(converted_scores,tools_to_test)), ~ .x * -1))%>%
    select(tools_to_test,clinvar_class,cons_cat)%>%
    mutate(across(all_of(tools_to_test), scale_min_max))%>%
    filter(!is.na(cons_cat))%>%
    pivot_longer(cols=-c(clinvar_class,cons_cat))

balanced_data<-NULL
for (tool in tools_to_test){
  print(tool)
  for (conservation in unique(proc_data_conservation$cons_cat)){
    balanced_data<-balanced_data%>%bind_rows(
      proc_data_conservation%>%filter(name==tool)%>%
      filter(cons_cat==conservation)%>%
      slice_sample(n=balanced_sample_size_per_tool%>%filter(name==tool,cons_cat==conservation)%>%pull(balanced_n)))
  }
}

# Calibration by conservation, it looks like mos tools overestimate the pathogenicity of low conservation variants - or putting it differently, for a given score, the probability that a variant in a low conservation region is pathogenic is lower compared to higher conservation levels

calibration_by_conservation_plot<-
proc_data_conservation%>%
    #mutate(value_cat=cut(value,breaks=c(seq(0,1,0.05)),labels = seq(0.025,0.975,0.05)))%>%
    mutate(value_cat=cut(value,breaks=c(seq(0,1,0.1)),labels = seq(0.05,0.95,0.1)),
           name=stringr::str_replace(name,'_score.+','')%>%toupper())%>%
    group_by(name,value_cat,cons_cat)%>%
    count(clinvar_class)%>%
    mutate(value_cat=as.numeric(as.character(value_cat)),
           total=sum(n),
           rate=n/sum(n))%>%
    filter(clinvar_class=='P/LP')%>%
    mutate(broom::tidy(binom.test(n,total)))%>%
    ggplot(aes(x=value_cat,y=estimate,color=cons_cat,ymin=conf.low,ymax=conf.high))+
    geom_line()+
    geom_point()+
    facet_wrap(name~.,nrow = 3)+
    geom_errorbar(width=0.01)+
    ggsci::scale_color_cosmic()+
    ylim(0,1)+xlim(0,1)+
    labs(color=NULL,x=NULL,y=NULL)+
    geom_abline(intercept = 0,slope = 1,linetype=2,alpha=0.5)+
    theme_minimal()+
    theme(legend.position='top',text = element_text(size=text_size))
calibration_by_conservation_plot
ggsave(filename = glue('{analysis_folder_name}/calibration_by_conservation_plot.{Sys.Date()}.png'),
       plot=calibration_by_conservation_plot,
       device = 'png',
       height = 7,
       width = 14,
       dpi = 300,bg='white')


```

```{r }
z<-
  roc_metrics_table%>%filter(sub_var_set=='all',missing_option=='no_missing')%>%
  select(tool,main_var_set,bal_accuracy)%>%pivot_wider(names_from = main_var_set,values_from = bal_accuracy)%>%
  rowwise()%>%
  mutate(sd=sd(c(confident_all,confident_2020,confident_2022)),
         diff=confident_all-confident_2020)

proc_data%>%filter(main_var_sets[['confident_2020']],sub_var_sets[['all']])%>%
  select(clinvar_class,contains('_score'))%>%
  pivot_longer(-clinvar_class)%>%
  ggplot(aes(x=value,fill=clinvar_class))+
  geom_histogram(alpha=0.5,position='dodge')+
  facet_wrap(name~.,scales = 'free')+
  scale_fill_manual(values=c('darkcyan','darkred'))+
  theme_minimal()+theme(legend.position = 'top')



# GOF vs LOF
roc_metrics_table%>%
  filter(n>200)%>%
  filter(grepl('AD|AR',sub_var_set))%>%filter(class_balance=='balanced_class')%>%filter(main_var_set=='confident_new')%>%
  ggplot(aes(x=tool,y=`X50.`,color=sub_var_set))+
  geom_point()+
  coord_flip()+
  theme_minimal()


# AD vs AR
roc_metrics_table%>%
  filter(n>200)%>%
  filter(grepl('AD|AR',sub_var_set))%>%filter(class_balance=='balanced_class')%>%filter(main_var_set=='confident_new')%>%
  ggplot(aes(x=tool,y=`X50.`,color=sub_var_set))+
  geom_point()+
  coord_flip()+
  theme_minimal()

# allele freq
roc_metrics_table%>%
  filter(n>50)%>%
  filter(grepl('af_cat',sub_var_set))%>%filter(class_balance=='balanced_class')%>%filter(main_var_set=='confident_new')%>%
  mutate(sub_var_set=forcats::fct_relevel(sub_var_set,"af_cat_NA","af_cat_[0.00e+00,8.24e-06)","af_cat_[8.24e-06,4.91e-05)","af_cat_[4.91e-05,2.60e-04)","[2.60e-04,3.18e-03)","af_cat_[3.18e-03,1.00e+00]"))%>%
  ggplot(aes(x=sub_var_set,y=`X50.`,group=tool))+
  geom_point()+
  geom_line()+
  #coord_flip()+
  theme_minimal()

roc_metrics_table%>%
  filter(n>50)%>%
  filter(grepl('af_cat',sub_var_set))%>%filter(class_balance=='balanced_class')%>%filter(main_var_set=='confident_new')%>%
  ggplot(aes(x=tool,y=`X50.`,color=sub_var_set))+
  geom_point()+
  coord_flip()+
  theme_minimal()


# Grab top performing tools for each category
top_3<-
  roc_metrics_table%>%
  filter(n>1000)%>%
  group_by(main_var_set,sub_var_set,class_balance)%>%
  slice_max(n=3,order_by = `X50.`,with_ties = F)%>%
  mutate(rank=rank(-`X50.`))%>%
  pivot_wider(id_cols = c(main_var_set,sub_var_set,class_balance),names_from = rank,values_from = tool)
  

rocs_table%>%
  ggplot(aes(x=1-specificities,y=sensitivities,color=tool))+
  geom_line(size=1,alpha=0.55)+
  geom_abline(linetype=2,alpha=0.5)+
  #ggsci::scale_color_d3()+
  theme_minimal()+
  labs(color=NULL,x='Sensitivity',y='Specificity',linetype=NULL)+
  theme(legend.position = 'top')
roc_plot

```


