---
title: "Prediction tools analysis"
author: "Ofer Isakov"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: united
    toc: true
    toc_float: false
params:
  all_founder_vars_annotated_file: ''
  clinvar_am_annotated_file: ''
---

# Comments:
- went over the missing values for the different tools - they are really missing.. 

# TODO : 
- for each fairness analysis, add an inter-tool comparison to find the best performing tool - should be on the complete set
- add panelapp performance - what panels do the tools underperform in?
- add PPV for 90% sens and sensitivity for 90% PPV

## optional test sets
https://genomeinterpretation.org/cagi6-invitae.html - registered waiting for confirmation - checked 06/05/2024 - still no confirmation

## reviews
golden helix review - https://www.goldenhelix.com/blog/evaluating-deepminds-alphamissense-classifier/
review - https://www.frontiersin.org/articles/10.3389/fgene.2022.1010327/full
check the performance of each tool on benign vs pathogenic (maybe benign variants were used in training ans should be replaced)
benchmarking using deep mutational scans - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10407742/
becnchmarking analysis - phenotype based - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9313608/#humu24362-sec-0020title

# possible comparisons
- completeness (how many variants have a score)
- conservation (high vs low)
- in domain (interpro, yes vs no)
- allele frequencies (common vs rare)
- mode of inheritance (AD vs AR vs X linked)
- haploinsufficient / triploinsufficient genes
- pLI  , pRec , under missense constraint, LOF constraint
- known cancer pathogenic? (downloaded data from oncovar. consider uploading as a new project and annotating it)
- disease panels from panelapp

```{r markdown_setup,include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd('/media/SSD/Bioinformatics/Projects/prediction_tool_annotation_analysis/')
library(ProjectTemplate)
load.project()

data_folder_name<-'./data/preprocessed_data/2024-06-10'
analysis_folder_name<-glue('./output/prediction_tools_analysis.{Sys.Date()}')
if (!dir.exists(analysis_folder_name)){
  dir.create(analysis_folder_name)
  dir.create(glue('{analysis_folder_name}/data'))
  }

# Definitions ####
# the year that will be used for all subsequent analysis
year_to_analyze<-'at_or_after_2023'
# the varity_r score is the best out of the varities, eve and mutpred have a high rate of missingness
excluded_tools<-c('fathmm.xf_coding','esm1b','varity_r_loo','varity_er','varity_er_loo','eve','mutpred','m.cap','mpc','mutationassessor','mvp','lrt') 
```

```{r save_load_data}
save.image(file = glue('{analysis_folder_name}/data/{basename(analysis_folder_name)}.RData'))
```

```{r load_preprocessed_data}
# The annotated table
proc_data_file<-grep('processed_data.prediction_tool_annotation',list.files(data_folder_name,full.names = T),value=T)
proc_data<-readr::read_delim(proc_data_file,delim='\t')
# The var sets to analyze
load(glue('{data_folder_name}/var_sets.RData'))
#excluded_tools<-c('esm1b')
tools_list<-grep('_score',colnames(proc_data),value = T)
tools_without_excluded<-setdiff(tools_list,paste0(excluded_tools,'_score.dbnsfp4.5a'))

# converted scores (in which lower score means higher prob of P)
converted_scores<-stringr::str_match(colnames(proc_data),'(.+)_converted')[,2]%>%unique()
converted_scores<-glue('{converted_scores[!is.na(converted_scores)]}_score.dbnsfp4.5a')
```

# Missingness
```{r missingness}

excluded_tools<-c('fathmm.xf_coding','esm1b','varity_r_loo','varity_er','varity_er_loo','eve','mutpred','m.cap','mpc','mutationassessor','lrt','mvp','list.s2','h1.hesc_fitcons','huvec_fitcons','gm12878_fitcons','integrated_fitcons') 
tools_without_excluded<-setdiff(tools_list,paste0(excluded_tools,'_score.dbnsfp4.5a'))

proc_data%>%filter(!if_any(all_of(tools_without_excluded), is.na))%>%nrow()

library(naniar)
naniar::gg_miss_upset(proc_data%>%select(tools_without_excluded),nsets=20) 

missing_count<-sapply(tools_list,function(tool) {proc_data%>%filter(is.na(!!sym(tool)))%>%nrow()})
missing_count<-missing_count%>%as.data.frame()%>%rename(n_missing='.')%>%mutate(missing_rate=n_missing/nrow(proc_data))
missing_count

missing_rate_plot<-
missing_count%>%
  mutate(tool=stringr::str_replace(rownames(missing_count),'_score.+','')%>%toupper())%>%
  ggplot(aes(x=forcats::fct_reorder(tool,missing_rate),y=missing_rate,label=glue('{round(missing_rate,2)*100}%')))+
  geom_col(alpha=0.5)+
  geom_text(nudge_y = 0.03,size=3)+
  coord_flip()+
  scale_y_continuous(labels = scales::percent)+
  theme_minimal()+
  labs(x=NULL,y='Missing rate')
missing_rate_plot
ggsave(filename = glue('{analysis_folder_name}/01_missing_rate_plot.{Sys.Date()}.png'),
       plot=missing_rate_plot,
       device = 'png',
       height = 10,
       width = 8,
       dpi = 300,bg='white')

# missing per year
missing_count_per_year<-NULL
#for (year in unique(year(proc_data$date_created))){
for (year in c('at_or_after_2021','at_or_after_2022','at_or_after_2023','at_or_after_2024')){
  #year_data<-proc_data%>%filter(year(date_created)==year)
  year_data<-proc_data%>%filter(var_sets[[year]])
  year_missing_count<-sapply(tools_list,function(tool) {year_data%>%filter(is.na(!!sym(tool)))%>%nrow()})
  missing_count_per_year<-missing_count_per_year%>%
    bind_rows(year_missing_count %>%
                as.data.frame() %>%
                rename(n_missing = '.') %>%
                mutate(tool=tools_list,
                       year = year, 
                       total_vars = nrow(year_data),
                       missing_rate = n_missing / nrow(year_data)))
}

missing_count_per_year%>%
  ggplot(aes(x=year,y=missing_rate))+
  geom_col()+
  facet_wrap(tool~.)

skimr::skim(missing_count$missing_rate)
```

Initial review of the data revealed varying rates of missing values across different scores in the dbNSFP4.5a database (median 0.377 IQR[0.13,0.43]). EVE (0.715) and MutPred (0.634) had the highest missingness rate. In contrast, MutationTaster, FATHMM, DANN, Genocanyon, BayesDel and fitCons all had a missing rate lower than 0.1. 

# Overall performance models 
collect overall performance for each model

```{r rocs}
# Definitions
tools_to_test<-tools_without_excluded
#time_var_set<-c('at_or_after_2024')
time_var_set<-c('at_or_after_2021','at_or_after_2022','at_or_after_2023','at_or_after_2024')
top_num<-10

analysis_var_sets<-generate_var_set_combinations(var_sets[time_var_set])
# overall performance for the complete var set - no missing data for any of the tools
overall_performance_complete_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_without_excluded,complete = T)
# overall perforamnce for the full var set - the performance of each tool on all of it's annotated variants
overall_performance_full_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_without_excluded,complete = F)

rocs_data<-c(overall_performance_complete_analysis_res$procs,overall_performance_full_analysis_res$procs)
overall_performance_table<-overall_performance_complete_analysis_res$roc_metrics_table%>%
  bind_rows(overall_performance_full_analysis_res$roc_metrics_table)%>%
  filter(tool %in% tools_to_test)%>%
  mutate(complete=ifelse(complete,'complete','full'))
write.table(overall_performance_table,file=glue('{analysis_folder_name}/overall_performance.{Sys.Date()}.csv'),row.names = F,sep='\t')

# Collect the top tools rank
top_tools_rank<-overall_performance_table%>%
  group_by(filter1,complete)%>%
  slice_max(order_by = AUC50.,n=top_num)%>%
  mutate(rank=rank(-AUC50.))%>%
  select(filter1,complete,tool,rank)%>%
  pivot_wider(id_cols = c(tool,complete),names_from = filter1,values_from = rank)

# Collect the top tools auc
top_tools_auc<-overall_performance_table%>%
  group_by(filter1,complete)%>%
  slice_max(order_by = AUC50.,n=top_num)%>%
  select(filter1,complete,tool,AUC50.)%>%
  pivot_wider(id_cols = c(tool,complete),names_from = filter1,values_from = AUC50.)

top_tools<-overall_performance_table%>%group_by(complete)%>%filter(filter1==year_to_analyze)%>%
  slice_max(order_by = AUC50.,n=top_num)%>%select(tool)%>%mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())

# generate the rocs table
raw_roc_table<-NULL
for (var_set in names(rocs_data)){
  split_var_set<-stringr::str_split(var_set,'\\|')%>%unlist()
  tool=split_var_set[2]
  if (!tool %in% tools_to_test){next}
  raw_roc_table<-raw_roc_table%>%bind_rows(
    data.frame(var_set=var_set,
               year=split_var_set[1],
               tool=split_var_set[2],
               complete=split_var_set[3],
               sensitivity=rocs_data[[var_set]]$sensitivities,
               specificity=rocs_data[[var_set]]$specificities)
  )
}

# ROCs for the complete (no-missing) dataset
top_tools_complete_roc_plot<-
plot_roc(raw_roc_table%>%filter(complete=='complete'),
         overall_performance_table%>%filter(complete=='complete'),
         top_tools%>%filter(complete=='complete')%>%ungroup(),
         year_to_analyze)
# ROCs for the full dataset
top_tools_full_roc_plot<-
plot_roc(raw_roc_table%>%filter(complete=='full'),
         overall_performance_table%>%filter(complete=='full'),
         top_tools%>%filter(complete=='full')%>%ungroup(),
         year_to_analyze)

library(patchwork)
top_models_roc_plot<-(top_tools_complete_roc_plot+labs(title='A'))+(top_tools_full_roc_plot+labs(title='B'))
top_models_roc_plot

ggsave(filename = glue('{analysis_folder_name}/top_models_roc_plot.{year_to_analyze}.{Sys.Date()}.png'),
       plot=top_models_roc_plot,
       device = 'png',
       height = 7,
       width = 15,
       dpi = 300,bg='white')

roc_table<-raw_roc_table%>%
    #filter(tool %in% tools_to_test)%>%
    filter(year==year_to_analyze)%>%
    left_join(overall_performance_table%>%
                rename(year=filter1))%>%
    mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  left_join(top_tools%>%mutate(is_top=T))%>%
  mutate(tool_name=ifelse(is.na(is_top),'Other',tool),
         tool_name=forcats::fct_relevel(tool_name,'Other'),
         complete=ifelse(complete=='complete',glue('Complete (Non-missing) set'),glue('Full set')))
colors <- setNames(colorRampPalette(c("darkred", "darkcyan", "darkorange",'darkmagenta'))(length(unique(roc_table$tool_name))), unique(roc_table$tool_name))
colors["Other"] <- "lightgray" 
print(colors)
# Plot the ROC for all the tools and emphasize the top ones
roc_plot<-
  roc_table%>%filter(tool_name=='Other')%>%
  ggplot(aes(x=1-specificity,y=sensitivity,color=tool_name,group=tool))+
  facet_wrap(complete~.)+
  geom_line(linewidth=1)+
  geom_line(data=roc_table%>%filter(tool_name!='Other'),linewidth=1)+
  geom_abline(slope = 1,intercept = 0,linetype=2,alpha=0.4)+
  scale_color_manual(values=colors)+
  theme_minimal()+
  theme(legend.position = 'top')+
  labs(color=NULL)
print(roc_plot)

# collect the top tools in BOTH complete and full sets
top_performing_tools<-top_tools_rank%>%
  #filter(complete=='complete')%>%
  #slice_min(!!sym(year_to_analyze),n=10)%>%
  pull(tool)%>%stringr::str_replace('_score.+','')%>%toupper()%>%unique()
```

# Variant assertion by year added to clinvar
metrics to test options  - AUC , Accuracy, Balanced accuracy
since we are intereseted in the intra-tool variance, then we should use the entire dataset for each tool

```{r perfo_by_time}
library(ggtext)
library(viridis)
library(fuzzyjoin)

# the clinvar_training_date is derived from the date the clinvar dataset was downloaded or if not available - the publication date
tools_with_pub_year<-readxl::read_xlsx('./data/accessory_data/tools_list.xlsx')

# Set the metric you wish to plot
analysis_var_sets<-generate_var_set_combinations(var_sets['all'],var_sets[c(as.character(2013:2024))])
rocs_analysis_results<-calculate_roc_metrics(proc_data,
                                             analysis_var_sets,
                                             tools_without_exclusion,
                                             complete=F)

if (length(excluded_tools)>0){
  tools_to_test<-grep(paste0(excluded_tools,'_score',collapse='|'),unique(rocs_analysis_results$roc_metrics_table$tool),value = T,invert = T)
}else{tools_to_test<-tools_list}
perf_by_year_table<-
  rocs_analysis_results$roc_metrics_table%>%
  rename(year=filter2)%>%
  filter(tool %in% tools_to_test)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())
# add publication year
perf_by_year_table<-perf_by_year_table%>%
  left_join(
    tools_with_pub_year%>%
      rename(pub_year="Publication Year"))%>%
  mutate(pub_year=as.character(pub_year),
         training_year=as.character(clinvar_training_date))
write.table(perf_by_year_table,file=glue('{analysis_folder_name}/02_perf_by_year_table.{Sys.Date()}.csv'),row.names = F,sep='\t')

# plot for all tools
perf_metric<-'AUC50.'
perf_by_year_all_plot<-
  perf_by_year_table%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  ggplot(aes_string(x='year',y=perf_metric,group='tool'))+
  geom_point(size=1)+geom_line()+
  facet_wrap(tool~.,scales='free')+
  labs(x='ClinVar Year',y=perf_metric)+
  #scale_color_viridis_d(option = 'turbo')+
  geom_vline(aes(xintercept=training_year),data=perf_by_year_table,linetype=2,color='darkred')+
  theme_minimal()+
  theme(legend.position = 'top')+
  labs(color=NULL)+theme(axis.text.x = element_text(angle = 90))
perf_by_year_all_plot

ggsave(filename = glue('{analysis_folder_name}/02_all_tools_{perf_metric}_by_year_plot.{Sys.Date()}.png'),
       plot=perf_by_year_all_plot,
       device = 'png',
       height = 8,
       width = 12,
       dpi = 300,bg='white')

# plot for top tools
perf_by_year_top_plot<-
  perf_by_year_table%>%
  filter(tool %in% top_performing_tools)%>%
  ggplot(aes_string(x='year',y=perf_metric,group='tool'))+
  geom_point(size=1)+geom_line()+
  facet_wrap(tool~.,nrow=2)+
  labs(x='ClinVar Year',y=perf_metric)+
  #scale_color_viridis_d(option = 'turbo')+
  geom_vline(aes(xintercept=training_year),data=perf_by_year_table%>%
               filter(tool %in% top_performing_tools),linetype=2,color='darkred')+
  theme_minimal()+
  theme(legend.position = 'top')+
  labs(color=NULL)+theme(axis.text.x = element_text(angle = 90))
perf_by_year_top_plot

ggsave(filename = glue('{analysis_folder_name}/02_top_tools_{perf_metric}_by_year_plot.{Sys.Date()}.png'),
       plot=perf_by_year_top_plot,
       device = 'png',
       height = 6,
       width = 12,
       dpi = 300,bg='white')


# analyze results per tool to identify tools with decreasing values
year_analysis_table <- perf_by_year_table %>%mutate(year=as.numeric(year),
                                      before_training=ifelse(year<=as.numeric(training_year),
                                                             'before_training','after_training'))%>%
  mutate(ifelse(year<=2013,NA,year))%>%
  mutate(before_training=forcats::fct_relevel(before_training,'before_training'))
  
no_clinvar_training_tools<-year_analysis_table%>%group_by(tool)%>%
  summarize(before_and_after=length(unique(before_training)))%>%
  filter(before_and_after<2)%>%pull(tool)

# analyzing effect over time - all tools together
year_effect_model <- lme(AUC50. ~ year,random=~1 | tool, data = year_analysis_table%>%filter(tool%in%top_performing_tools))
summary(year_effect_model)
# analyzing effect over time - for every tool 
year_effect_model_by_tool<-NULL
for (tool2analyze in unique(year_analysis_table$tool)){
  print(tool2analyze)
  tool_analysis<-tidy(lm(AUC50.~year,data=year_analysis_table%>%filter(tool==tool2analyze)))
  year_effect_model_by_tool<-year_effect_model_by_tool%>%
    bind_rows(data.frame(tool=tool2analyze,
                         tool_analysis%>%filter(term=='year')))
}
year_effect_model_by_tool
# analyze effect before and after training
lm_by_training_year<-
  year_analysis_table%>%
  filter(!tool %in% no_clinvar_training_tools)%>%
  group_by(tool) %>%
  do(tidy(lm(AUC50. ~ before_training, data = .)))%>%
  filter(term=='before_trainingafter_training')


```

example: The results of the analysis demonstrate that the majority of the predictive tools evaluated do not exhibit a significant change in performance over the years indicating a stable predictive accuracy over the evaluated period. A few tools did show significant variations. Specifically, fitCons scores demosntrated an improvement throughout the years while VEST4's performance decreased (-0.005 per year; p=0.0039). Analyzing tool performance before and after model training also demonstrated stable performance for most tools. Tools that exhibited noteworthy decrease in AUC included CLINPRED (-0.015; p=0.0401) and METARNN (0.016; p=0.0082). 

# Affect of conservation

```{r conservation}
conservation_var_sets<-grep('conserv',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[year_to_analyze],var_sets[conservation_var_sets])

conservation_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_to_test,complete = F)

performance_by_conservation_table<-
  conservation_analysis_res$roc_metrics_table%>%
  filter(tool %in% tools_to_test)%>%
  rename(conservation_level=filter2)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.),
         conservation_level=stringr::str_replace(conservation_level,'_conservation','')%>%stringr::str_to_title(),
         conservation_level=factor(conservation_level,levels = c('Low','Intermediate','High')))%>%
  select(filter1,conservation_level,tool,total,AUC50.,sens,spec)
# add overall performance
performance_by_conservation_table<-performance_by_conservation_table%>%left_join(
  overall_performance_table%>%filter(filter1==year_to_analyze,complete=='full')%>%
    mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
    select(filter1,tool,overall_total=total,overall_AUC50=AUC50.,overall_sens=sens,overall_spec=spec)
)

write.table(performance_by_conservation_table,file=glue('{analysis_folder_name}/performance_by_conservation_table.{Sys.Date()}.csv'),row.names = F,sep='\t')

performance_by_conservation_table<-performance_by_conservation_table%>%
  left_join(performance_by_conservation_table%>%group_by(tool)%>%mutate(min_auc=min(AUC50.),max_auc=max(AUC50.),min_max_diff=max_auc-min_auc)%>%select(tool,conservation_level,min_max_diff))

performance_by_conservation_table<-performance_by_conservation_table%>%mutate(tool=forcats::fct_reorder(tool,min_max_diff))

performance_by_conservation_table%>%
  select(conservation_level,tool,total,AUC50.)%>%
  pivot_wider(id_cols = tool,names_from = conservation_level,values_from = AUC50.)

# analyze results using mixed effects model 
library(nlme)
# discriminatory performance using all the tools
cons_auc_model <- lme(AUC50. ~ conservation_level,random=~1 | tool, data = performance_by_conservation_table)
summary(cons_auc_model)
# sensitivity and specificity using tools that have predictions
cons_sens_model<-lme(sens ~ conservation_level,random=~1 | tool, data = performance_by_conservation_table%>%filter(!is.na(sens)))
summary(cons_sens_model)
cons_spec_model<-lme(spec ~ conservation_level,random=~1 | tool, data = performance_by_conservation_table%>%filter(!is.na(spec)))
summary(cons_spec_model)

# plot the results
conservation_to_plot<-
performance_by_conservation_table%>%
  filter(!is.na(sens))%>%
  pivot_longer(-c(tool,filter1,conservation_level,total))%>%
  mutate(name=forcats::fct_recode(name,AUC='AUC50.',Sensitivity='sens',Specificity='spec'))%>%
  filter(name%in%c('AUC','Sensitivity','Specificity'))%>%
  #filter(name%in%c('AUC'))%>%
  left_join(
    performance_by_conservation_table%>%
    filter(!is.na(sens))%>%
    pivot_longer(-c(tool,filter1,conservation_level,total))%>%
    mutate(name=forcats::fct_recode(name,AUC='AUC50.',Sensitivity='sens',Specificity='spec'))%>%
    filter(name%in%c('AUC','Sensitivity','Specificity'))%>%
    #filter(name%in%c('AUC'))%>%
    group_by(tool,name)%>%
      summarize(min_val=min(value),max_val=max(value))%>%ungroup()
  )

cons_overall_performance<-
  performance_by_conservation_table%>%
  filter(!is.na(sens))%>%
  pivot_longer(-c(tool,filter1,conservation_level,total))%>%
  mutate(name=forcats::fct_recode(name,AUC='overall_AUC50',Sensitivity='overall_sens',Specificity='overall_spec'))%>%
  filter(name%in%c('AUC','Sensitivity','Specificity'))%>%select(-c(total,conservation_level))%>%distinct()

performance_by_conservation_plot<-
  conservation_to_plot%>%
  ggplot(aes(color=conservation_level,y=value,x=tool,ymin=min_val,ymax=max_val))+
  geom_errorbar(color='gray',width=0)+
  geom_point(size=2)+
  geom_point(data = cons_overall_performance, aes(y = value, x = tool, shape = "Overall performance"), fill = "black", size = 2, inherit.aes = FALSE) +
  facet_grid(.~name)+
  coord_flip()+
  theme_minimal()+
  scale_color_manual(values=c('darkcyan','darkorange','darkred'))+
  scale_shape_manual(values = c("Overall performance" = 3)) +
  labs(color='Conservation',x=NULL,shape=NULL)+
  theme(legend.position = 'top')
performance_by_conservation_plot

ggsave(filename = glue('{analysis_folder_name}/performance_by_conservation_plot.{year_to_analyze}.{Sys.Date()}.png'),
       plot=performance_by_conservation_plot,
       device = 'png',
       height = 6,
       width = 9,
       dpi = 300,bg='white')


# a second plot option.. 
g<-conservation_to_plot%>%
  ggplot(aes(color=conservation_level,y=value,x=name,ymin=min_val,ymax=max_val))+
  geom_errorbar(color='gray',width=0)+
  geom_point(size=2)+
  coord_flip()+
  facet_grid(tool~.,space='free',scales='free',switch = 'both',labeller = label_wrap_gen(width = 6))+
  theme_minimal()+
  scale_color_manual(values=c('darkcyan','darkorange','darkred'))+
  theme(legend.position = 'bottom',strip.placement = 'outside',strip.text.y.left = element_text(angle=0,face = 'bold'))+
  geom_vline(xintercept = 0.4,alpha=0.75)+theme(panel.spacing = unit(0, "lines"))
g

```

The analysis of the mixed models for AUC, sensitivity, and specificity revealed distinct patterns associated with conservation levels. For AUC, the results indicated a significant decrease as conservation levels increased, with the AUC decreasing by 0.0495 (p = 0.0047) in high conservation levels compared to low levels. Sensitivity, on the other hand, showed a significant increase with higher conservation levels, with an increase of 0.1036 (p < 0.0001) at intermediate and 0.1984 (p < 0.0001) at high conservation levels. Specificity exhibited a significant decline as conservation levels rose, with a decrease of 0.2177 (p < 0.0001) at intermediate and 0.4707 (p < 0.0001) at high conservation levels. These findings suggest that while tools become more sensitive in highly conserved regions, their specificity and overall performance (AUC) tend to decrease, highlighting the challenges in balancing sensitivity and specificity across different conservation contexts.

Stable Performance Across Conservation Levels:

Tools such as BAYESDEL_ADDAF and BAYESDEL_NOAF exhibited remarkably stable performance across all three conservation levels (High, Intermediate, and Low). These tools maintained high AUC values consistently, with BAYESDEL_ADDAF achieving AUCs of 0.9907, 0.9874, and 0.9728 for Low, Intermediate, and High conservation levels, respectively. Sensitivity and specificity were also high and consistent, indicating robust performance regardless of the conservation context.
Tools Highly Affected by Conservation Levels:

In contrast, DANN showed significant variability across conservation levels. Its AUC dropped drastically from 0.8929 in Low conservation to 0.5568 in High conservation. This tool's performance was highly sensitive to the conservation level, suggesting it is less reliable in more conserved regions.

Significantly Worse Performance in Certain Conservation Levels:

LRT also demonstrated poorer performance, particularly in the High conservation level, with an AUC of 0.6965 compared to its overall AUC of 0.7477. Sensitivity was particularly affected, dropping to 0.4069, indicating its limited utility in highly conserved regions.
Association Between Conservation Levels and AUC, Sensitivity, and Specificity:

The analysis revealed a general trend where tools tend to perform better in less conserved regions. This is reflected in higher AUCs and sensitivity values in Low conservation levels for most tools. Specificity, however, varied more significantly, with some tools maintaining high specificity across all levels, such as BAYESDEL_ADDAF, while others like ALPHAMISSENSE showed a notable drop in specificity in High conservation levels (0.4963).

# MOI analysis

```{r moi}
#tools_to_test<-top_tools_2023
moi_var_sets<-grep('AD|AR|prec|pli',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[year_to_analyze],var_sets[moi_var_sets])
moi_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_to_test,complete=F)

performance_by_moi_table<-
  moi_analysis_res$roc_metrics_table%>%
  rename(moi=filter2)%>%
  #filter(tool %in% tools_to_test)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.),
         complete=ifelse(complete,'complete','full'))
write.table(performance_by_moi_table,file=glue('{analysis_folder_name}/performance_by_moi_{year_to_analyze}_table.{Sys.Date()}.csv'),row.names = F,sep='\t')

# plot the difference between AD and AR
moi_to_plot<-performance_by_moi_table%>%
  filter(moi%in%c('AD','AR'))%>%select(moi,tool,AUC50.)%>%
  pivot_wider(id_cols = tool,names_from = moi,values_from = AUC50.)%>%
  left_join(
    performance_by_moi_table%>%
      filter(moi%in%c('prec_high','pli_high'))%>%select(moi,tool,AUC50.)%>%
      pivot_wider(id_cols = tool,names_from = moi,values_from = AUC50.)
  )%>%
  mutate('AD vs AR'=AD-AR,
         'pLI vs pRec'=pli_high-prec_high,
         tool=forcats::fct_reorder(tool,desc(`AD vs AR`)))%>%
  select(tool,`AD vs AR`,`pLI vs pRec`)%>%
  pivot_longer(-tool)
performance_by_moi_plot<-
moi_to_plot%>%
  ggplot(aes(y=value,x=tool,color=value>0,fill=value>0))+
  geom_point(size=3)+
  geom_col(linewidth=0.1,width = 0.1)+
  geom_hline(yintercept = 0,linetype=2,color='gray')+
  facet_grid(.~name)+
  scale_fill_manual(values=c('darkred','darkgreen'),labels=c('Better perofrmance in AR/pRec','Better performance in AD/pLI'))+
  scale_color_manual(values=c('darkred','darkgreen'))+
  coord_flip()+
  labs(y='AUC difference',fill=NULL,x=NULL)+guides(color='none')+
  theme_minimal()+theme(strip.text = element_text(face='bold'),legend.position = 'bottom',
                        axis.text.y = element_text(face=ifelse(levels(moi_to_plot$tool)%in%top_performing_tools,"bold","italic")))
performance_by_moi_plot

ggsave(filename = glue('{analysis_folder_name}/performance_by_moi_plot.{Sys.Date()}.png'),
       plot=performance_by_moi_plot,
       device = 'png',
       height = 8,
       width = 9,
       dpi = 300,bg='white')

library(nlme)
# discriminatory performance using all the tools
moi_auc_model <- lme(AUC50. ~ moi,random=~1 | tool, data = performance_by_moi_table%>%filter(moi%in%c('AD','AR')))
summary(moi_auc_model)
# sensitivity and specificity using tools that have predictions
moi_sens_model<-lme(sens ~ moi,random=~1 | tool, data = performance_by_moi_table%>%filter(moi%in%c('AD','AR'))%>%filter(!is.na(sens)))
summary(moi_sens_model)
moi_spec_model<-lme(spec ~ moi,random=~1 | tool, data = performance_by_moi_table%>%filter(moi%in%c('AD','AR'))%>%filter(!is.na(spec)))
summary(moi_spec_model)

# plot score distribution
proc_data%>%filter(var_sets[['at_or_after_2023']])%>%
  select(clinvar_class,moi_gencc,tools_to_test)%>%
  filter(moi_gencc%in%c('Autosomal dominant','Autosomal recessive'))%>%
  pivot_longer(-c(clinvar_class,moi_gencc))%>%
  ggplot(aes(x=value,fill=clinvar_class))+
  geom_histogram(position='dodge')+
  facet_wrap(moi_gencc~name,scales='free',ncol = length(tools_to_test))+
  theme_minimal()
  #filter(if_any(all_of(tools_to_test), is.na))

# plot with CI
performance_by_moi_table%>%
  filter(moi%in%c('AD','AR'))%>%
  pivot_wider(id_cols = tool,names_from = moi,values_from = c(AUC50.,AUC2.5,AUC97.5))%>%
  mutate(SE_AD = (AUC97.5_AD - AUC2.5_AD) / (2 * 1.96),
         SE_AR = (AUC97.5_AR - AUC2.5_AR) / (2 * 1.96)) %>%
  summarize(
    mean_diff = AUC50._AD - AUC50._AR,
    se_diff = sqrt(SE_AD^2 + SE_AR^2),
    ci_lower = mean_diff - 1.96 * se_diff,
    ci_upper = mean_diff + 1.96 * se_diff,
    tool=forcats::fct_reorder(tool,mean_diff)
  )%>%
  ggplot(aes(x=tool,y=mean_diff,ymin=ci_lower,ymax=ci_upper))+
  geom_point()+geom_errorbar()+
  geom_hline(yintercept = 0,linetype=2)+
  coord_flip()+
  theme_minimal()
```

The analysis comparing the performance of tools in detecting variants affecting autosomal dominant (AD) versus autosomal recessive (AR) genes yielded significant insights. For AUC, the linear mixed-effects model indicated that AR variants had a significantly higher AUC compared to AD variants, with an increase of 0.0263 (p = 0.0001). This suggests that tools are slightly more effective at identifying variants in AR genes. For sensitivity, the model showed no significant difference between AD and AR variants. This implies that the sensitivity of the tools is consistent across both types of inheritance. In terms of specificity, the model revealed a significant increase for AR variants. The specificity for AR variants was 0.1048 higher than for AD variants (p < 0.0001), indicating that tools are more specific in detecting variants in AR genes, resulting in fewer false positives.

# Misense constraint 
consider using the sHet value from Regeneron million exomes (already downloaded in the accessory data)

```{r missense_constraint}
mismatchoe_var_sets<-c(grep('mismatch_min_oe_ratio_manual:',names(var_sets),value = T),
                       'regeneron_constrained','regeneron_not_constrained')
#mismatchoe_var_sets<-grep('regeneron',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[time_var_set],var_sets[mismatchoe_var_sets])
mismatchoe_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_to_test,complete = F)

performance_by_mismatchoe_table<-
  mismatchoe_analysis_res$roc_metrics_table%>%
  rename(mismatch_oe=filter2)%>%
  filter(tool %in% tools_to_test)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.),
         mismatch_oe=stringr::str_replace(mismatch_oe,'mismatch_min_oe_ratio_manual:','')
         )
write.table(performance_by_moi_table,file=glue('{analysis_folder_name}/performance_by_moi_{time_var_set}_table.{Sys.Date()}.csv'),row.names = F,sep='\t')

performance_by_mismatchoe_table%>%
  filter(!(grepl('regeneron',mismatch_oe)))%>%
  ggplot(aes(x=mismatch_oe,y=AUC50.,group=tool,ymin=AUC2.5,ymax=AUC97.5))+
  geom_line()+
  geom_point()+
  geom_errorbar(width=0)+
  facet_wrap(tool~.,scales='free')+
  theme_minimal()

# analyze results using mixed effects model
library(nlme)
model <- lme(AUC50. ~ mismatch_oe,random=~1 | tool, data = performance_by_mismatchoe_table%>%filter(!(grepl('regeneron',mismatch_oe))))
summary(model)
plot(model)
```

# Allele frequency

```{r allele frequency}
```

# GO functional / bio process

```{r go_process_analysis}
go_cols<-grep('go_process:',colnames(proc_data),value = T)
min_count_thresh<-200
go_process_counts<-data.frame()
for (go_col in go_cols){
  print(go_col)
  for (tool in tools_to_test){
    min_count<-proc_data%>%filter(var_sets[[year_to_analyze]],!is.na(!!sym(tool)))%>%count(!!sym(go_col),clinvar_class)%>%filter(!is.na(!!sym(go_col)))%>%pull(n)%>%min()
    go_process_counts<-go_process_counts%>%bind_rows(data.frame(go_col,tool,min_count=min_count))
  }
}
go_process_to_test<-go_process_counts%>%group_by(go_col)%>%summarize(all_tools_above_thresh=all(min_count>min_count_thresh))%>%filter(all_tools_above_thresh)%>%pull(go_col)
# add var_sets
for (go_to_test in go_process_to_test){
  var_sets[[go_to_test]]<-proc_data%>%pull(go_to_test)
}

#mismatchoe_var_sets<-grep('regeneron',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[year_to_analyze],var_sets[go_process_to_test])
go_process_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_to_test,complete = F)

performance_by_go_process_table<-
  go_process_analysis_res$roc_metrics_table%>%
  rename(go_process=filter2)%>%
  filter(tool %in% tools_to_test)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.),
         go_process=stringr::str_replace(go_process,'go_process:','')
         )

performance_by_go_process_table<-
performance_by_go_process_table%>%left_join(
  overall_performance_table%>%filter(complete=='full',filter1==year_to_analyze)%>%
    select(tool,overall_performance=AUC50.)%>%mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())
)%>%
  mutate(tool=forcats::fct_reorder(tool,overall_performance),)
performance_by_go_process_table%>%
  filter(tool%in%(top_tools%>%filter(complete=='complete')%>%pull(tool)))%>%
  mutate(color=case_when(
    AUC50.<overall_performance & AUC97.5<overall_performance~'decrease',
    AUC50.>overall_performance & AUC2.5>overall_performance~'increase',
    .default='no_change'))%>%
  ggplot(aes(x=tool,y=AUC50.,ymin=AUC2.5,ymax=AUC97.5,color=color))+
  geom_point()+
  geom_errorbar(width=0.5)+
  geom_point(aes(x=tool,y=overall_performance),inherit.aes = F,shape=3)+
  coord_flip()+
  scale_color_manual(values=c('increase'='darkblue','no_change'='black','decrease'='darkred'))+
  facet_wrap(go_process~.,scales='free_x')+
  guides(color='none')+
  theme_minimal()

library(nlme)
go_me_analysis_table<-
performance_by_go_process_table%>%bind_rows(
  overall_performance_table%>%filter(complete=='full',filter1==year_to_analyze)%>%
    select(tool,AUC50.)%>%mutate(go_process='overall_performance',tool=stringr::str_replace(tool,'_score.+','')%>%toupper())
  )%>%
  mutate(go_process=relevel(factor(go_process), ref = "overall_performance"))

model <- lme(AUC50. ~ go_process,random=~1 | tool, data = go_me_analysis_table)
summary(model)
plot(model)

# which go process has the best overall performance?
performance_by_go_process_table%>%
  ggplot(aes(x=forcats::fct_reorder(go_process,AUC50.),y=AUC50.))+
  geom_boxplot()+
  geom_point(size=1)+
  coord_flip()+
  theme_minimal()

```

```{r go_function_analysis}
go_func_cols<-grep('go_function:',colnames(proc_data),value = T)
min_count_thresh<-200
go_func_counts<-data.frame()
for (go_col in go_func_cols){
  print(go_col)
  for (tool in tools_to_test){
    min_count<-proc_data%>%filter(var_sets[[year_to_analyze]],!is.na(!!sym(tool)))%>%count(!!sym(go_col),clinvar_class)%>%filter(!is.na(!!sym(go_col)))%>%pull(n)%>%min()
    go_func_counts<-go_func_counts%>%bind_rows(data.frame(go_col,tool,min_count=min_count))
  }
}
go_func_to_test<-go_func_counts%>%group_by(go_col)%>%summarize(all_tools_above_thresh=all(min_count>min_count_thresh))%>%filter(all_tools_above_thresh)%>%pull(go_col)
# add var_sets
for (go_to_test in go_func_to_test){
  var_sets[[go_to_test]]<-proc_data%>%pull(go_to_test)
}

#mismatchoe_var_sets<-grep('regeneron',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[year_to_analyze],var_sets[go_func_to_test])
go_func_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_to_test,complete = F)

performance_by_go_func_table<-
  go_func_analysis_res$roc_metrics_table%>%
  rename(go_function=filter2)%>%
  filter(tool %in% tools_to_test)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.),
         go_function=stringr::str_replace(go_function,'go_function:','')
         )

performance_by_go_func_table<-
performance_by_go_func_table%>%left_join(
  overall_performance_table%>%filter(complete=='full',filter1==year_to_analyze)%>%
    select(tool,overall_performance=AUC50.)%>%mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())
)%>%
  mutate(tool=forcats::fct_reorder(tool,overall_performance),)
performance_by_go_func_table%>%
  filter(tool%in%(top_tools%>%filter(complete=='complete')%>%pull(tool)))%>%
  mutate(color=case_when(
    AUC50.<overall_performance & AUC97.5<overall_performance~'decrease',
    AUC50.>overall_performance & AUC2.5>overall_performance~'increase',
    .default='no_change'))%>%
  ggplot(aes(x=tool,y=AUC50.,ymin=AUC2.5,ymax=AUC97.5,color=color))+
  geom_point()+
  geom_errorbar(width=0.5)+
  geom_point(aes(x=tool,y=overall_performance),inherit.aes = F,shape=3)+
  coord_flip()+
  scale_color_manual(values=c('increase'='darkblue','no_change'='black','decrease'='darkred'))+
  facet_wrap(go_function~.)+
  guides(color='none')+
  theme_minimal()

library(nlme)
go_func_me_analysis_table<-
performance_by_go_func_table%>%bind_rows(
  overall_performance_table%>%filter(complete=='full',filter1==year_to_analyze)%>%
    select(tool,AUC50.)%>%mutate(go_function='overall_performance',tool=stringr::str_replace(tool,'_score.+','')%>%toupper())
  )%>%
  mutate(go_function=relevel(factor(go_function), ref = "overall_performance"))
model <- lme(AUC50. ~ go_function,random=~1 | tool, data = go_func_me_analysis_table)
summary(model)

# which go function has the best overall performance?
performance_by_go_func_table%>%
  ggplot(aes(x=forcats::fct_reorder(go_function,AUC50.),y=AUC50.))+
  geom_boxplot()+
  coord_flip()+
  theme_minimal()


```

# PanelApp

```{r panelapp}
panelapp_cols<-grep('panelapp:',colnames(proc_data),value = T)
min_count_thresh<-200
panelapp_counts<-data.frame()
for (pa_col in panelapp_cols){
  print(pa_col)
  for (tool in tools_to_test){
    min_count<-proc_data%>%filter(var_sets[[year_to_analyze]],!is.na(!!sym(tool)))%>%count(!!sym(pa_col),clinvar_class)%>%filter(!is.na(!!sym(pa_col)))%>%pull(n)%>%min()
    panelapp_counts<-panelapp_counts%>%bind_rows(data.frame(pa_col,tool,min_count=min_count))
  }
}
panelapp_to_test<-panelapp_counts%>%group_by(pa_col)%>%summarize(all_tools_above_thresh=all(min_count>min_count_thresh))%>%filter(all_tools_above_thresh)%>%pull(pa_col)
# add var_sets
for (pa_to_test in panelapp_to_test){
  var_sets[[pa_to_test]]<-proc_data%>%pull(pa_to_test)
}

#mismatchoe_var_sets<-grep('regeneron',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[year_to_analyze],var_sets[panelapp_to_test])
panelapp_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_to_test,complete = F)

performance_by_panelapp_table<-
  panelapp_analysis_res$roc_metrics_table%>%
  rename(panelapp=filter2)%>%
  filter(tool %in% tools_to_test)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.),
         panelapp=stringr::str_replace(panelapp,'panelapp:','')
         )

# consider instead of running nle on all the data, splitting the analysis into multiple tests each time just the overall versus the panel

library(nlme)
pa_all_comps<-NULL
for (pa in performance_by_panelapp_table%>%pull(panelapp)%>%unique()){
  pa_comp_table<-performance_by_panelapp_table%>%filter(panelapp==pa)%>%
    bind_rows(
      overall_performance_table%>%filter(complete=='full',filter1==year_to_analyze)%>%
      select(tool,AUC50.)%>%mutate(panelapp='overall_performance',tool=stringr::str_replace(tool,'_score.+','')%>%toupper())
    )%>%
  mutate(panelapp=relevel(factor(panelapp), ref = "overall_performance"))
  model <- lme(AUC50. ~ panelapp,random=~1 | tool, data = pa_comp_table)
  model_res<-summary(model)
  pa_all_comps<-pa_all_comps%>%bind_rows(
    data.frame(panelapp=pa,data.frame(t(model_res$tTable[2,])))
  )
}


```



# Calibration

```{r calibration}
# calibration plots
text_size<-10
scale_min_max <- function(x) {
  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}

# data for balance
balanced_sample_size_per_tool<-
  proc_data %>%
  select(tools_to_test, clinvar_class, cons_cat) %>%
  pivot_longer(-c(clinvar_class,cons_cat))%>%
  filter(!(is.na(value)|is.na(cons_cat))) %>%
  group_by(name,cons_cat, clinvar_class)%>%
  count()%>%ungroup()%>%group_by(name,cons_cat)%>%summarize(balanced_n=min(n))

proc_data_conservation <- proc_data%>%
    mutate(across(all_of(intersect(converted_scores,tools_to_test)), ~ .x * -1))%>%
    select(tools_to_test,clinvar_class,cons_cat)%>%
    mutate(across(all_of(tools_to_test), scale_min_max))%>%
    filter(!is.na(cons_cat))%>%
    pivot_longer(cols=-c(clinvar_class,cons_cat))

balanced_data<-NULL
for (tool in tools_to_test){
  print(tool)
  for (conservation in unique(proc_data_conservation$cons_cat)){
    balanced_data<-balanced_data%>%bind_rows(
      proc_data_conservation%>%filter(name==tool)%>%
      filter(cons_cat==conservation)%>%
      slice_sample(n=balanced_sample_size_per_tool%>%filter(name==tool,cons_cat==conservation)%>%pull(balanced_n)))
  }
}

# Calibration by conservation, it looks like mos tools overestimate the pathogenicity of low conservation variants - or putting it differently, for a given score, the probability that a variant in a low conservation region is pathogenic is lower compared to higher conservation levels

calibration_by_conservation_plot<-
proc_data_conservation%>%
    #mutate(value_cat=cut(value,breaks=c(seq(0,1,0.05)),labels = seq(0.025,0.975,0.05)))%>%
    mutate(value_cat=cut(value,breaks=c(seq(0,1,0.1)),labels = seq(0.05,0.95,0.1)),
           name=stringr::str_replace(name,'_score.+','')%>%toupper())%>%
    group_by(name,value_cat,cons_cat)%>%
    count(clinvar_class)%>%
    mutate(value_cat=as.numeric(as.character(value_cat)),
           total=sum(n),
           rate=n/sum(n))%>%
    filter(clinvar_class=='P/LP')%>%
    mutate(broom::tidy(binom.test(n,total)))%>%
    ggplot(aes(x=value_cat,y=estimate,color=cons_cat,ymin=conf.low,ymax=conf.high))+
    geom_line()+
    geom_point()+
    facet_wrap(name~.,nrow = 3)+
    geom_errorbar(width=0.01)+
    ggsci::scale_color_cosmic()+
    ylim(0,1)+xlim(0,1)+
    labs(color=NULL,x=NULL,y=NULL)+
    geom_abline(intercept = 0,slope = 1,linetype=2,alpha=0.5)+
    theme_minimal()+
    theme(legend.position='top',text = element_text(size=text_size))
calibration_by_conservation_plot
ggsave(filename = glue('{analysis_folder_name}/calibration_by_conservation_plot.{Sys.Date()}.png'),
       plot=calibration_by_conservation_plot,
       device = 'png',
       height = 7,
       width = 14,
       dpi = 300,bg='white')


```

```{r }
z<-
  roc_metrics_table%>%filter(sub_var_set=='all',missing_option=='no_missing')%>%
  select(tool,main_var_set,bal_accuracy)%>%pivot_wider(names_from = main_var_set,values_from = bal_accuracy)%>%
  rowwise()%>%
  mutate(sd=sd(c(confident_all,confident_2020,confident_2022)),
         diff=confident_all-confident_2020)

proc_data%>%filter(main_var_sets[['confident_2020']],sub_var_sets[['all']])%>%
  select(clinvar_class,contains('_score'))%>%
  pivot_longer(-clinvar_class)%>%
  ggplot(aes(x=value,fill=clinvar_class))+
  geom_histogram(alpha=0.5,position='dodge')+
  facet_wrap(name~.,scales = 'free')+
  scale_fill_manual(values=c('darkcyan','darkred'))+
  theme_minimal()+theme(legend.position = 'top')



# GOF vs LOF
roc_metrics_table%>%
  filter(n>200)%>%
  filter(grepl('AD|AR',sub_var_set))%>%filter(class_balance=='balanced_class')%>%filter(main_var_set=='confident_new')%>%
  ggplot(aes(x=tool,y=`X50.`,color=sub_var_set))+
  geom_point()+
  coord_flip()+
  theme_minimal()


# AD vs AR
roc_metrics_table%>%
  filter(n>200)%>%
  filter(grepl('AD|AR',sub_var_set))%>%filter(class_balance=='balanced_class')%>%filter(main_var_set=='confident_new')%>%
  ggplot(aes(x=tool,y=`X50.`,color=sub_var_set))+
  geom_point()+
  coord_flip()+
  theme_minimal()

# allele freq
roc_metrics_table%>%
  filter(n>50)%>%
  filter(grepl('af_cat',sub_var_set))%>%filter(class_balance=='balanced_class')%>%filter(main_var_set=='confident_new')%>%
  mutate(sub_var_set=forcats::fct_relevel(sub_var_set,"af_cat_NA","af_cat_[0.00e+00,8.24e-06)","af_cat_[8.24e-06,4.91e-05)","af_cat_[4.91e-05,2.60e-04)","[2.60e-04,3.18e-03)","af_cat_[3.18e-03,1.00e+00]"))%>%
  ggplot(aes(x=sub_var_set,y=`X50.`,group=tool))+
  geom_point()+
  geom_line()+
  #coord_flip()+
  theme_minimal()

roc_metrics_table%>%
  filter(n>50)%>%
  filter(grepl('af_cat',sub_var_set))%>%filter(class_balance=='balanced_class')%>%filter(main_var_set=='confident_new')%>%
  ggplot(aes(x=tool,y=`X50.`,color=sub_var_set))+
  geom_point()+
  coord_flip()+
  theme_minimal()


# Grab top performing tools for each category
top_3<-
  roc_metrics_table%>%
  filter(n>1000)%>%
  group_by(main_var_set,sub_var_set,class_balance)%>%
  slice_max(n=3,order_by = `X50.`,with_ties = F)%>%
  mutate(rank=rank(-`X50.`))%>%
  pivot_wider(id_cols = c(main_var_set,sub_var_set,class_balance),names_from = rank,values_from = tool)
  

rocs_table%>%
  ggplot(aes(x=1-specificities,y=sensitivities,color=tool))+
  geom_line(size=1,alpha=0.55)+
  geom_abline(linetype=2,alpha=0.5)+
  #ggsci::scale_color_d3()+
  theme_minimal()+
  labs(color=NULL,x='Sensitivity',y='Specificity',linetype=NULL)+
  theme(legend.position = 'top')
roc_plot

```


