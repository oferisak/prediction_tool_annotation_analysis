---
title: "Prediction tools analysis"
author: "Ofer Isakov"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: united
    toc: true
    toc_float: false
params:
  all_founder_vars_annotated_file: ''
  clinvar_am_annotated_file: ''
---

# TODO : 
went over the missing values - they are really missing.. 
asked GPT to gather the tools with their corresponding publication years. probably will need to go over one by one and see if they used clinvar or not and if so what year.. 
go function and biological process

## optional test sets
https://genomeinterpretation.org/cagi6-invitae.html - registered waiting for confirmation - checked 06/05/2024 - still no confirmation

## reviews
golden helix review - https://www.goldenhelix.com/blog/evaluating-deepminds-alphamissense-classifier/
review - https://www.frontiersin.org/articles/10.3389/fgene.2022.1010327/full
check the performance of each tool on benign vs pathogenic (maybe benign variants were used in training ans should be replaced)
benchmarking using deep mutational scans - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10407742/
becnchmarking analysis - phenotype based - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9313608/#humu24362-sec-0020title

# possible comparisons
- completeness (how many variants have a score)
- conservation (high vs low)
- in domain (interpro, yes vs no)
- allele frequencies (common vs rare)
- mode of inheritance (AD vs AR vs X linked)
- haploinsufficient / triploinsufficient genes
- pLI  , pRec , under missense constraint, LOF constraint
- known cancer pathogenic? (downloaded data from oncovar. consider uploading as a new project and annotating it)

```{r markdown_setup,include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd('/media/SSD/Bioinformatics/Projects/prediction_tool_annotation_analysis/')
library(ProjectTemplate)
load.project()

data_folder_name<-'./data/preprocessed_data/2024-05-14'
analysis_folder_name<-glue('./output/prediction_tools_analysis.{Sys.Date()}')
if (!dir.exists(analysis_folder_name)){dir.create(analysis_folder_name)}

# Definitions ####
# the year that will be used for all subsequent analysis
year_to_analyze<-'at_or_after_2023'
# the varity_r score is the best out of the varities, eve and mutpred have a high rate of missingness
excluded_tools<-c('fathmm.xf_coding','esm1b','varity_r_loo','varity_er','varity_er_loo','eve','mutpred') 
```

```{r load_preprocessed_data}
# The annotated table
proc_data_file<-grep('processed_data.prediction_tool_annotation',list.files(data_folder_name,full.names = T),value=T)
proc_data<-readr::read_delim(proc_data_file,delim='\t')
# The var sets to analyze
load(glue('{data_folder_name}/var_sets.RData'))
#excluded_tools<-c('esm1b')
tools_list<-grep('_score',colnames(proc_data),value = T)
tools_without_excluded<-setdiff(tools_list,paste0(excluded_tools,'_score.dbnsfp4.5a'))

# converted scores (in which lower score means higher prob of P)
converted_scores<-stringr::str_match(colnames(proc_data),'(.+)_converted')[,2]%>%unique()
converted_scores<-glue('{converted_scores[!is.na(converted_scores)]}_score.dbnsfp4.5a')
```

# Missingness
```{r missingness}
missing_count<-sapply(tools_list,function(tool) {proc_data%>%filter(is.na(!!sym(tool)))%>%nrow()})
missing_count<-missing_count%>%as.data.frame()%>%rename(n_missing='.')%>%mutate(missing_rate=n_missing/nrow(proc_data))
missing_count

missing_rate_plot<-
missing_count%>%
  mutate(tool=stringr::str_replace(rownames(missing_count),'_score.+','')%>%toupper())%>%
  ggplot(aes(x=forcats::fct_reorder(tool,missing_rate),y=missing_rate,label=glue('{round(missing_rate,2)*100}%')))+
  geom_col(alpha=0.5)+
  geom_text(nudge_y = 0.03,size=3)+
  coord_flip()+
  scale_y_continuous(labels = scales::percent)+
  theme_minimal()+
  labs(x=NULL,y='Missing rate')
missing_rate_plot
ggsave(filename = glue('{analysis_folder_name}/missing_rate_plot.{Sys.Date()}.png'),
       plot=missing_rate_plot,
       device = 'png',
       height = 10,
       width = 8,
       dpi = 300,bg='white')

skimr::skim(missing_count$missing_rate)

```

Initial review of the data revealed varying rates of missing values across different scores in the dbNSFP4.5a database (median 0.377 IQR[0.13,0.43]). EVE (0.715) and MutPred (0.634) had the highest missingness rate. In contrast, MutationTaster, FATHMM, DANN, Genocanyon, BayesDel and fitCons all had a missing rate lower than 0.1. 

# Variant assertion by year added to clinvar
metrics to test options  - AUC , Accuracy, Balanced accuracy
since we are intereseted in the intra-tool variance, then we should use the entire dataset for each tool

```{r perfo_by_time}
library(ggtext)
library(viridis)
library(fuzzyjoin)

# the clinvar_training_date is derived from the date the clinvar dataset was downloaded or if not available - the publication date
tools_with_pub_year<-readxl::read_xlsx('./data/accessory_data/tools_list.xlsx')

# Set the metric you wish to plot
analysis_var_sets<-generate_var_set_combinations(var_sets['all'],var_sets[c(as.character(2013:2024))])
rocs_analysis_results<-calculate_roc_metrics(proc_data,
                                             analysis_var_sets,
                                             tools_without_exclusion,
                                             complete=F)

if (length(excluded_tools)>0){
  tools_to_test<-grep(paste0(excluded_tools,collapse='|'),unique(rocs_analysis_results$roc_metrics_table$tool),value = T,invert = T)
}else{tools_to_test<-tools_list}
perf_by_year_table<-
  rocs_analysis_results$roc_metrics_table%>%
  rename(year=filter2)%>%
  filter(tool %in% tools_to_test)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())
# add publication year
perf_by_year_table<-perf_by_year_table%>%
  left_join(
    tools_with_pub_year%>%
      rename(pub_year="Publication Year"))%>%
  mutate(pub_year=as.character(pub_year),
         training_year=as.character(clinvar_training_date))
write.table(perf_by_year_table,file=glue('{analysis_folder_name}/perf_by_year_table.{Sys.Date()}.csv'),row.names = F,sep='\t')

perf_metric<-'AUC50.'
perf_by_year_plot<-
  perf_by_year_table%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  ggplot(aes_string(x='year',y=perf_metric,group='tool'))+
  geom_point(size=1)+geom_line()+
  facet_wrap(tool~.,scales='free')+
  labs(x='ClinVar Year',y=perf_metric)+
  scale_color_viridis_d(option = 'turbo')+
  geom_vline(aes(xintercept=training_year),data=perf_by_year_table,linetype=2,color='darkred')+
  theme_minimal()+
  theme(legend.position = 'top')+
  labs(color=NULL)+theme(axis.text.x = element_text(angle = 90))
perf_by_year_plot

ggsave(filename = glue('{analysis_folder_name}/{perf_metric}_by_year_plot.{Sys.Date()}.png'),
       plot=perf_by_year_plot,
       device = 'png',
       height = 8,
       width = 12,
       dpi = 300,bg='white')

# analyze results per tool to identify tools with decreasing values
analysis_table <- perf_by_year_table %>%mutate(year=as.numeric(year),
                                      before_training=ifelse(year<=as.numeric(training_year),
                                                             'before_training','after_training'))%>%
  mutate(ifelse(year<=2013,NA,year))%>%
  mutate(before_training=forcats::fct_relevel(before_training,'before_training'))
  
no_clinvar_training_tools<-analysis_table%>%group_by(tool)%>%summarize(before_and_after=length(unique(before_training)))%>%filter(before_and_after<2)%>%pull(tool)

# analyzing effect over time
lm_by_year<-
  perf_by_year_table%>%
  mutate(year=as.numeric(year))%>%
  group_by(tool) %>%
  do(tidy(lm(AUC50. ~ year, data = .)))%>%
  filter(term=='year')

# analyze before and after training
lm_by_training_year<-
  analysis_table%>%
  filter(!tool %in% no_clinvar_training_tools)%>%
  group_by(tool) %>%
  do(tidy(lm(AUC50. ~ before_training, data = .)))%>%
  filter(term=='before_trainingafter_training')

```

The results of the analysis demonstrate that the majority of the predictive tools evaluated do not exhibit a significant change in performance over the years indicating a stable predictive accuracy over the evaluated period. A few tools did show significant variations. Specifically, fitCons scores demosntrated an improvement throughout the years while VEST4's performance decreased (-0.005 per year; p=0.0039). Analyzing tool performance before and after model training also demonstrated stable performance for most tools. Tools that exhibited noteworthy decrease in AUC included CLINPRED (-0.015; p=0.0401) and METARNN (0.016; p=0.0082). 

# Overall performance models 
collect overall perforamance for each model

```{r rocs}
# Definitions
tools_to_test<-tools_without_excluded
time_var_set<-c('at_or_after_2021','at_or_after_2022','at_or_after_2023','at_or_after_2024')
top_num<-10

analysis_var_sets<-generate_var_set_combinations(var_sets[time_var_set])
# overall performance for the complete var set - no missing data for any of the tools
overall_performance_complete_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_without_excluded,complete = T)
# overall perforamnce for the full var set - the performance of each tool on all of it's annotated variants
overall_performance_full_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_without_excluded,complete = F)

rocs_data<-c(overall_performance_complete_analysis_res$procs,overall_performance_full_analysis_res$procs)
overall_performance_table<-overall_performance_complete_analysis_res$roc_metrics_table%>%
  bind_rows(overall_performance_full_analysis_res$roc_metrics_table)%>%
  filter(tool %in% tools_to_test)%>%
  mutate(complete=ifelse(complete,'complete','full'))
write.table(overall_performance_table,file=glue('{analysis_folder_name}/overall_performance.{Sys.Date()}.csv'),row.names = F,sep='\t')

# Collect the top tools rank
top_tools_rank<-overall_performance_table%>%
  group_by(filter1,complete)%>%
  slice_max(order_by = AUC50.,n=top_num)%>%
  mutate(rank=rank(-AUC50.))%>%
  select(filter1,complete,tool,rank)%>%
  pivot_wider(id_cols = c(tool,complete),names_from = filter1,values_from = rank)

# Collect the top tools auc
top_tools_auc<-overall_performance_table%>%
  group_by(filter1,complete)%>%
  slice_max(order_by = AUC50.,n=top_num)%>%
  select(filter1,complete,tool,AUC50.)%>%
  pivot_wider(id_cols = c(tool,complete),names_from = filter1,values_from = AUC50.)

top_tools<-overall_performance_table%>%group_by(complete)%>%filter(filter1==year_to_analyze)%>%
  slice_max(order_by = AUC50.,n=top_num)%>%select(tool)%>%mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())

# generate the rocs table
raw_roc_table<-NULL
for (var_set in names(rocs_data)){
  split_var_set<-stringr::str_split(var_set,'\\|')%>%unlist()
  tool=split_var_set[2]
  if (!tool %in% tools_to_test){next}
  raw_roc_table<-raw_roc_table%>%bind_rows(
    data.frame(var_set=var_set,
               year=split_var_set[1],
               tool=split_var_set[2],
               complete=split_var_set[3],
               sensitivity=rocs_data[[var_set]]$sensitivities,
               specificity=rocs_data[[var_set]]$specificities)
  )
}

# ROCs for the complete (no-missing) dataset
top_tools_complete_roc_plot<-
plot_roc(raw_roc_table%>%filter(complete=='complete'),
         overall_performance_table%>%filter(complete=='complete'),
         top_tools%>%filter(complete)%>%ungroup(),
         year_to_analyze)
# ROCs for the full dataset
top_tools_full_roc_plot<-
plot_roc(raw_roc_table%>%filter(complete=='full'),
         overall_performance_table%>%filter(complete=='full'),
         top_tools%>%filter(!complete)%>%ungroup(),
         year_to_analyze)

library(patchwork)
top_models_roc_plot<-(top_tools_complete_roc_plot+labs(title='A'))+(top_tools_full_roc_plot+labs(title='B'))
top_models_roc_plot

ggsave(filename = glue('{analysis_folder_name}/top_models_roc_plot.{year_to_plot}.{Sys.Date()}.png'),
       plot=top_models_roc_plot,
       device = 'png',
       height = 7,
       width = 15,
       dpi = 300,bg='white')

# bump chart (less informative)  
library(ggbump)
top_tools_rank%>%
  select(tool,as.character(2020:2024))%>%
  pivot_longer(-tool)%>%
  #filter(!is.na(value))%>%
  ggplot(aes(x=as.numeric(name),y=-value,color=tool))+
  geom_bump()+
  geom_point()+
  theme_minimal()

top_tools_2023<-top_tools_rank%>%filter(!is.na(at_or_after_2023))%>%pull(tool)
```

# Affect of conservation

```{r conservation}

year_to_plot<-'at_or_after_2023'
conservation_var_sets<-grep('conserv',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[year_to_plot],var_sets[conservation_var_sets])

conservation_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_to_test,complete = F)

performance_by_conservation_table<-
  conservation_analysis_res$roc_metrics_table%>%
  filter(tool %in% tools_to_test)%>%
  rename(conservation_level=filter2)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.),
         conservation_level=stringr::str_replace(conservation_level,'_conservation','')%>%stringr::str_to_title(),
         conservation_level=factor(conservation_level,levels = c('Low','Intermediate','High')))%>%
  select(filter1,conservation_level,tool,total,AUC50.,sens,spec)
write.table(performance_by_conservation_table,file=glue('{analysis_folder_name}/performance_by_conservation_table.{Sys.Date()}.csv'),row.names = F,sep='\t')

performance_by_conservation_table<-performance_by_conservation_table%>%
  left_join(performance_by_conservation_table%>%group_by(tool)%>%mutate(min_auc=min(AUC50.),max_auc=max(AUC50.),min_max_diff=max_auc-min_auc)%>%select(tool,conservation_level,min_max_diff))

performance_by_conservation_table<-performance_by_conservation_table%>%mutate(tool=forcats::fct_reorder(tool,min_max_diff))

performance_by_conservation_table%>%
  select(conservation_level,tool,total,AUC50.)%>%
  pivot_wider(id_cols = tool,names_from = conservation_level,values_from = AUC50.)

# analyze results using mixed effects model
library(nlme)
model <- lme(AUC50. ~ conservation_level,random=~1 | tool, data = performance_by_conservation_table)
summary(model)
plot(model)

performance_by_conservation_plot<-
performance_by_conservation_table%>%
  left_join(
    performance_by_conservation_table%>%group_by(tool)%>%summarize(min_auc=min(AUC50.),max_auc=max(AUC50.))
  )%>%
  filter(!is.na(sens))%>%
  pivot_longer(-c(tool,filter1,conservation_level,total,min_auc,max_auc))%>%
  mutate(name=forcats::fct_recode(name,AUC='AUC50.',Sensitivity='sens',Specificity='spec'))%>%
  filter(name=='AUC')%>%
  ggplot(aes(color=conservation_level,y=value,x=tool,ymin=min_auc,ymax=max_auc))+
  geom_errorbar(color='gray',width=0)+
  geom_point(size=2)+
  coord_flip()+
  theme_minimal()+
  scale_color_manual(values=c('darkcyan','darkorange','darkred'))+
  labs(color='Conservation',y='AUC',x=NULL)+
  theme(legend.position = 'top')
performance_by_conservation_plot

ggsave(filename = glue('{analysis_folder_name}/performance_by_conservation_plot.{year_to_plot}.{Sys.Date()}.png'),
       plot=performance_by_conservation_plot,
       device = 'png',
       height = 6,
       width = 9,
       dpi = 300,bg='white')

# conservation - performance per cons level
performance_by_conservation_table%>%
  filter(!is.na(sens))%>%
  pivot_longer(-c(tool,filter1,conservation_level,total))%>%
  mutate(name=forcats::fct_recode(name,AUC='AUC50.',Sensitivity='sens',Specificity='spec'))%>%
  ggplot(aes(x=tool,y=value,color=conservation_level))+
  geom_point(position=position_dodge(width=0.75),size=2)+
  geom_linerange( aes(x=tool, xmin=tool, ymin=0, ymax=value),position=position_dodge(width=c(0.75)))+
  facet_grid(.~name)+
  coord_flip()+
  scale_color_cosmic()+
  labs(color='Conservation',x=NULL)+
  theme_minimal()+
  theme(legend.position = 'top')

```

# MOI analysis

```{r moi}
tools_to_test<-top_tools_2023
time_var_set<-'at_or_after_2023'
moi_var_sets<-grep('AD|AR|prec|pli',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[time_var_set],var_sets[moi_var_sets])
moi_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,top_tools_2023,complete=F)

performance_by_moi_table<-
  moi_analysis_res$roc_metrics_table%>%
  rename(moi=filter2)%>%
  #filter(tool %in% tools_to_test)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.))
write.table(performance_by_moi_table,file=glue('{analysis_folder_name}/performance_by_moi_{time_var_set}_table.{Sys.Date()}.csv'),row.names = F,sep='\t')

# plot the difference between AD and AR
performance_by_moi_plot<-
performance_by_moi_table%>%
  filter(moi%in%c('AD','AR'))%>%select(moi,tool,AUC50.)%>%
  pivot_wider(id_cols = tool,names_from = moi,values_from = AUC50.)%>%
  left_join(
    performance_by_moi_table%>%
      filter(moi%in%c('prec_high','pli_high'))%>%select(moi,tool,AUC50.)%>%
      pivot_wider(id_cols = tool,names_from = moi,values_from = AUC50.)
  )%>%
  mutate('AD vs AR'=AD-AR,
         'pLI vs pRec'=pli_high-prec_high,
         tool=forcats::fct_reorder(tool,desc(`AD vs AR`)))%>%
  select(tool,`AD vs AR`,`pLI vs pRec`)%>%
  pivot_longer(-tool)%>%
  ggplot(aes(y=value,x=tool,color=value>0,fill=value>0))+
  geom_point(size=3)+
  geom_col(linewidth=0.1,width = 0.1)+
  geom_hline(yintercept = 0,linetype=2,color='gray')+
  facet_grid(.~name)+
  scale_fill_manual(values=c('darkred','darkgreen'),labels=c('Better perofrmance in AR/pRec','Better performance in AD/pLI'))+
  scale_color_manual(values=c('darkred','darkgreen'))+
  coord_flip()+
  labs(y='AUC difference',fill=NULL,x=NULL)+guides(color='none')+
  theme_minimal()+theme(strip.text = element_text(face='bold'),legend.position = 'bottom')
performance_by_moi_plot

ggsave(filename = glue('{analysis_folder_name}/performance_by_moi_plot.{Sys.Date()}.png'),
       plot=performance_by_moi_plot,
       device = 'png',
       height = 8,
       width = 9,
       dpi = 300,bg='white')

# plot score distribution
proc_data%>%filter(var_sets[['at_or_after_2023']])%>%
  select(clinvar_class,moi_gencc,tools_to_test)%>%
  filter(moi_gencc%in%c('Autosomal dominant','Autosomal recessive'))%>%
  pivot_longer(-c(clinvar_class,moi_gencc))%>%
  ggplot(aes(x=value,fill=clinvar_class))+
  geom_histogram(position='dodge')+
  facet_wrap(moi_gencc~name,scales='free',ncol = length(tools_to_test))+
  theme_minimal()
  #filter(if_any(all_of(tools_to_test), is.na))

# plot with CI
performance_by_moi_table%>%
  filter(moi%in%c('AD','AR'))%>%
  pivot_wider(id_cols = tool,names_from = moi,values_from = c(AUC50.,AUC2.5,AUC97.5))%>%
  mutate(SE_AD = (AUC97.5_AD - AUC2.5_AD) / (2 * 1.96),
         SE_AR = (AUC97.5_AR - AUC2.5_AR) / (2 * 1.96)) %>%
  summarize(
    mean_diff = AUC50._AD - AUC50._AR,
    se_diff = sqrt(SE_AD^2 + SE_AR^2),
    ci_lower = mean_diff - 1.96 * se_diff,
    ci_upper = mean_diff + 1.96 * se_diff,
    tool=forcats::fct_reorder(tool,mean_diff)
  )%>%
  ggplot(aes(x=tool,y=mean_diff,ymin=ci_lower,ymax=ci_upper))+
  geom_point()+geom_errorbar()+
  geom_hline(yintercept = 0,linetype=2)+
  coord_flip()+
  theme_minimal()
```

# Misense constraint 
consider using the sHet value from Regeneron million exomes (already downloaded in the accessory data)

```{r missense_constraint}
time_var_set<-'at_or_after_2023'
mismatchoe_var_sets<-c(grep('mismatch_min_oe_ratio_manual:',names(var_sets),value = T),
                       'regeneron_constrained','regeneron_not_constrained')
#mismatchoe_var_sets<-grep('regeneron',names(var_sets),value = T)
analysis_var_sets<-generate_var_set_combinations(var_sets[time_var_set],var_sets[mismatchoe_var_sets])
mismatchoe_analysis_res<-calculate_roc_metrics(proc_data,analysis_var_sets,tools_to_test,complete = F)

performance_by_mismatchoe_table<-
  mismatchoe_analysis_res$roc_metrics_table%>%
  rename(mismatch_oe=filter2)%>%
  filter(tool %in% tools_to_test)%>%
  mutate(tool=stringr::str_replace(tool,'_score.+','')%>%toupper())%>%
  mutate(tool=forcats::fct_reorder(tool,AUC50.),
         mismatch_oe=stringr::str_replace(mismatch_oe,'mismatch_min_oe_ratio_manual:','')
         )
write.table(performance_by_moi_table,file=glue('{analysis_folder_name}/performance_by_moi_{time_var_set}_table.{Sys.Date()}.csv'),row.names = F,sep='\t')

performance_by_mismatchoe_table%>%
  filter(!(grepl('regeneron',mismatch_oe)))%>%
  ggplot(aes(x=mismatch_oe,y=AUC50.,group=tool,ymin=AUC2.5,ymax=AUC97.5))+
  geom_line()+
  geom_point()+
  geom_errorbar(width=0)+
  facet_wrap(tool~.,scales='free')+
  theme_minimal()

# analyze results using mixed effects model
library(nlme)
model <- lme(AUC50. ~ mismatch_oe,random=~1 | tool, data = performance_by_mismatchoe_table%>%filter(!(grepl('regeneron',mismatch_oe))))
summary(model)
plot(model)
```

# Allele frequency

```{r allele frequency}
```


# Calibration

```{r calibration}
# calibration plots
text_size<-10
scale_min_max <- function(x) {
  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}

# data for balance
balanced_sample_size_per_tool<-
  proc_data %>%
  select(tools_to_test, clinvar_class, cons_cat) %>%
  pivot_longer(-c(clinvar_class,cons_cat))%>%
  filter(!(is.na(value)|is.na(cons_cat))) %>%
  group_by(name,cons_cat, clinvar_class)%>%
  count()%>%ungroup()%>%group_by(name,cons_cat)%>%summarize(balanced_n=min(n))

proc_data_conservation <- proc_data%>%
    mutate(across(all_of(intersect(converted_scores,tools_to_test)), ~ .x * -1))%>%
    select(tools_to_test,clinvar_class,cons_cat)%>%
    mutate(across(all_of(tools_to_test), scale_min_max))%>%
    filter(!is.na(cons_cat))%>%
    pivot_longer(cols=-c(clinvar_class,cons_cat))

balanced_data<-NULL
for (tool in tools_to_test){
  print(tool)
  for (conservation in unique(proc_data_conservation$cons_cat)){
    balanced_data<-balanced_data%>%bind_rows(
      proc_data_conservation%>%filter(name==tool)%>%
      filter(cons_cat==conservation)%>%
      slice_sample(n=balanced_sample_size_per_tool%>%filter(name==tool,cons_cat==conservation)%>%pull(balanced_n)))
  }
}

# Calibration by conservation, it looks like mos tools overestimate the pathogenicity of low conservation variants - or putting it differently, for a given score, the probability that a variant in a low conservation region is pathogenic is lower compared to higher conservation levels

calibration_by_conservation_plot<-
proc_data_conservation%>%
    #mutate(value_cat=cut(value,breaks=c(seq(0,1,0.05)),labels = seq(0.025,0.975,0.05)))%>%
    mutate(value_cat=cut(value,breaks=c(seq(0,1,0.1)),labels = seq(0.05,0.95,0.1)),
           name=stringr::str_replace(name,'_score.+','')%>%toupper())%>%
    group_by(name,value_cat,cons_cat)%>%
    count(clinvar_class)%>%
    mutate(value_cat=as.numeric(as.character(value_cat)),
           total=sum(n),
           rate=n/sum(n))%>%
    filter(clinvar_class=='P/LP')%>%
    mutate(broom::tidy(binom.test(n,total)))%>%
    ggplot(aes(x=value_cat,y=estimate,color=cons_cat,ymin=conf.low,ymax=conf.high))+
    geom_line()+
    geom_point()+
    facet_wrap(name~.,nrow = 3)+
    geom_errorbar(width=0.01)+
    ggsci::scale_color_cosmic()+
    ylim(0,1)+xlim(0,1)+
    labs(color=NULL,x=NULL,y=NULL)+
    geom_abline(intercept = 0,slope = 1,linetype=2,alpha=0.5)+
    theme_minimal()+
    theme(legend.position='top',text = element_text(size=text_size))
calibration_by_conservation_plot
ggsave(filename = glue('{analysis_folder_name}/calibration_by_conservation_plot.{Sys.Date()}.png'),
       plot=calibration_by_conservation_plot,
       device = 'png',
       height = 7,
       width = 14,
       dpi = 300,bg='white')


```

```{r }
z<-
  roc_metrics_table%>%filter(sub_var_set=='all',missing_option=='no_missing')%>%
  select(tool,main_var_set,bal_accuracy)%>%pivot_wider(names_from = main_var_set,values_from = bal_accuracy)%>%
  rowwise()%>%
  mutate(sd=sd(c(confident_all,confident_2020,confident_2022)),
         diff=confident_all-confident_2020)

proc_data%>%filter(main_var_sets[['confident_2020']],sub_var_sets[['all']])%>%
  select(clinvar_class,contains('_score'))%>%
  pivot_longer(-clinvar_class)%>%
  ggplot(aes(x=value,fill=clinvar_class))+
  geom_histogram(alpha=0.5,position='dodge')+
  facet_wrap(name~.,scales = 'free')+
  scale_fill_manual(values=c('darkcyan','darkred'))+
  theme_minimal()+theme(legend.position = 'top')



# GOF vs LOF
roc_metrics_table%>%
  filter(n>200)%>%
  filter(grepl('AD|AR',sub_var_set))%>%filter(class_balance=='balanced_class')%>%filter(main_var_set=='confident_new')%>%
  ggplot(aes(x=tool,y=`X50.`,color=sub_var_set))+
  geom_point()+
  coord_flip()+
  theme_minimal()


# AD vs AR
roc_metrics_table%>%
  filter(n>200)%>%
  filter(grepl('AD|AR',sub_var_set))%>%filter(class_balance=='balanced_class')%>%filter(main_var_set=='confident_new')%>%
  ggplot(aes(x=tool,y=`X50.`,color=sub_var_set))+
  geom_point()+
  coord_flip()+
  theme_minimal()

# allele freq
roc_metrics_table%>%
  filter(n>50)%>%
  filter(grepl('af_cat',sub_var_set))%>%filter(class_balance=='balanced_class')%>%filter(main_var_set=='confident_new')%>%
  mutate(sub_var_set=forcats::fct_relevel(sub_var_set,"af_cat_NA","af_cat_[0.00e+00,8.24e-06)","af_cat_[8.24e-06,4.91e-05)","af_cat_[4.91e-05,2.60e-04)","[2.60e-04,3.18e-03)","af_cat_[3.18e-03,1.00e+00]"))%>%
  ggplot(aes(x=sub_var_set,y=`X50.`,group=tool))+
  geom_point()+
  geom_line()+
  #coord_flip()+
  theme_minimal()

roc_metrics_table%>%
  filter(n>50)%>%
  filter(grepl('af_cat',sub_var_set))%>%filter(class_balance=='balanced_class')%>%filter(main_var_set=='confident_new')%>%
  ggplot(aes(x=tool,y=`X50.`,color=sub_var_set))+
  geom_point()+
  coord_flip()+
  theme_minimal()


# Grab top performing tools for each category
top_3<-
  roc_metrics_table%>%
  filter(n>1000)%>%
  group_by(main_var_set,sub_var_set,class_balance)%>%
  slice_max(n=3,order_by = `X50.`,with_ties = F)%>%
  mutate(rank=rank(-`X50.`))%>%
  pivot_wider(id_cols = c(main_var_set,sub_var_set,class_balance),names_from = rank,values_from = tool)
  

rocs_table%>%
  ggplot(aes(x=1-specificities,y=sensitivities,color=tool))+
  geom_line(size=1,alpha=0.55)+
  geom_abline(linetype=2,alpha=0.5)+
  #ggsci::scale_color_d3()+
  theme_minimal()+
  labs(color=NULL,x='Sensitivity',y='Specificity',linetype=NULL)+
  theme(legend.position = 'top')
roc_plot

```


```{r am_revel_comp}
proc_data%>%ggplot(aes(x=revel_score.dbnsfp4.5a,y=alphamissense_score.dbnsfp4.5a,col=clinvar_class))+
  geom_point(alpha=0.2)+
  scale_color_manual(values=c('darkcyan','darkred'))+
  theme_minimal()

```

# Prepare data
this section parses the clinvar XML and retrieves for each clinvar entry the date created and updated

```{r parseXML}
library(xml2)
library(purrr)
library(XML)

#file_name<-'/media/SSD/Bioinformatics/Databases/clinvar/clinvar_assertions.xml'
file_name<-'/media/SSD/Bioinformatics/Databases/clinvar/ClinVarFullRelease_00-latest.xml.gz'

output_file<-glue('./output/clinvar_assertions.{Sys.Date()}.csv')
clinvar_file<-file_conn <- file(output_file, open = "w")
current_dates <- new.env()
processNode <- function(name, attrs, ...) {
  if (name == "ReferenceClinVarAssertion") {
    #print(attrs)
    # The dates might be in a different node; adjust as needed
    current_dates$date_created <- attrs["DateCreated"]
    current_dates$date_updated <- attrs["DateLastUpdated"]
    }
  if (name=='MeasureSet' & length(attrs)>1){
    #print(attrs['ID'])
    #print(current_dates$date_created)
    id<-attrs['ID']
    #print(date_created)
    if (!is.na(id)){
      if (!is.null(id) && !is.null(current_dates$date_created) && !is.null(current_dates$date_updated)) {
        #print(glue('{id}: {current_dates$date_created} : {current_dates$date_updated}'))
        write.table(x = data.frame(id = id, date_created = current_dates$date_created, date_updated = current_dates$date_updated),
                    file = file_conn, sep = ",", row.names = FALSE, col.names = FALSE, quote = FALSE, append = TRUE)
      }
    }
  }
}

xmlEventParse(file_name, handlers = list(startElement = processNode))
close(file_conn)

#clinvar_vars<-bind_rows(clinvar_vars)
```

